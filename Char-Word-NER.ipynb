{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import regex as re\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from allennlp.modules.conditional_random_field import ConditionalRandomField\n",
    "from allennlp.modules.conditional_random_field import allowed_transitions\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchcrf import CRF\n",
    "\n",
    "from RULE import RULEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(dictionary_dir: '(str) directory of dictionary (fasttext format)')\\\n",
    "-> '(dict) dict[word: vector]':\n",
    "        dictionary = {}\n",
    "        with open(dictionary_dir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                        tmp_line = line.strip()\n",
    "                        tmp_list = [word.strip() for word in tmp_line.split()]\n",
    "                        if tmp_line != '' and len(tmp_list) == 301:\n",
    "                                dictionary[tmp_list[0]] = np.array([float(number) for \\\n",
    "                                number in tmp_list[1:]])\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = get_dictionary('fasttext.th.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len_list = []\n",
    "total_oov_list = []\n",
    "cnt_o1 = 0\n",
    "cnt_o2 = 0\n",
    "with open('clean_384.txt', 'r', encoding='utf8') as f:\n",
    "    with open('clean_384_oov_less_10.txt', 'w', encoding='utf8') as o1:\n",
    "        with open('clean_384_oov_more_10.txt', 'w', encoding='utf8') as o2:\n",
    "            for line_ind, line in enumerate(f):\n",
    "                if line_ind % 100==0:\n",
    "                    print(line_ind)\n",
    "                tmp_line = line.strip()\n",
    "                if tmp_line != '':\n",
    "                    tmp_line = (word.strip() for word in tmp_line.split('||'))\n",
    "                    #print(tmp_line)\n",
    "                    cnt_oov = 0\n",
    "                    for ind, word in enumerate(tmp_line):\n",
    "                        if word not in my_dict:\n",
    "                            cnt_oov += 1\n",
    "                    if cnt_oov/(ind+1) < 0.1:\n",
    "                        o1.write(line.strip() + '\\n')\n",
    "                        cnt_o1 += 1\n",
    "                    else:\n",
    "                        o2.write(line.strip() + '\\n')\n",
    "                        cnt_o2 += 1\n",
    "                    total_oov_list.append(cnt_oov)\n",
    "                    total_len_list.append(ind+1)\n",
    "with open('report_oov_test.txt', 'w', encoding = 'utf8') as f:\n",
    "    f.write(f'less10_cnt: {cnt_o1}, upper10_cnt: {cnt_o2}')\n",
    "percent_oov_list = [100*total_oov_list[i]/total_len_list[i] for i in \\\n",
    "range(len(total_len_list))]\n",
    "\n",
    "plt.hist(percent_oov_list,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([i for i in percent_oov_list if i >= 10]))\n",
    "print(len([i for i in percent_oov_list if i < 10]))\n",
    "plt.hist(total_oov_list,100)\n",
    "plt.show()\n",
    "plt.hist(total_len_list,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len(text_dir: 'path to text dir', delimeter: 'delimeter used for split()'):\n",
    "    Max_len = 0\n",
    "    Min_len = 1000000\n",
    "    with open(text_dir, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            tmp_len = len(line.split(delimeter))\n",
    "            Max_len = max(tmp_len, Max_len)\n",
    "            Min_len = min(tmp_len, Min_len)\n",
    "    return Max_len, Min_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_len('label_384.txt', '||')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_char_dicitonary(char_dict_dir: 'all unique chars', out_dic_vec_dir: 'dir of dictionary vectors',\\\n",
    " num_unique_char):\n",
    "    with open(char_dict_dir, 'r', encoding='utf8') as f:\n",
    "        with open(out_dic_vec_dir, 'w', encoding='utf8') as o1:\n",
    "            for line in f:\n",
    "                for ind, Char in enumerate(line.strip()):\n",
    "                    o1.write(Char + ' ')\n",
    "                    for i in range(num_unique_char + 1):\n",
    "                        if i != ind:\n",
    "                            o1.write('0 ')\n",
    "                        else:\n",
    "                            o1.write('1 ')\n",
    "                    o1.write('\\n')\n",
    "                    if 'a' <= Char <= 'z':\n",
    "                        o1.write(Char.upper() + ' ')\n",
    "                        for i in range(num_unique_char):\n",
    "                            if i != ind:\n",
    "                                o1.write('0 ')\n",
    "                            else:\n",
    "                                o1.write('1 ')\n",
    "                        o1.write('1 ')\n",
    "                        o1.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_char_dicitonary('./char-word-level-LSTM-CRF/char_dictionary.txt', './char-word-level-LSTM-CRF/char_vec_dictionary.txt', 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./char-word-level-LSTM-CRF/char_dictionary.txt', 'r', encoding='utf8') as f:\n",
    "    cnt = 0\n",
    "    for i in f:\n",
    "        for char in i.strip():\n",
    "            cnt = cnt + 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataloader(Dataset):\n",
    "    def __init__(self, TextDir: '.txt extension of samples', LabelDir: '.txt extension of labels',rules:\\\n",
    "    'the rules to be replaced => see in RULE.py', Len_word_vec: 'size of word vector') -> None:\n",
    "        super().__init__()\n",
    "        self.DF = pd.read_csv(TextDir, names=['text'])\n",
    "        self.Label_DF = pd.read_csv(LabelDir, names=['text'])\n",
    "        self.rules = rules\n",
    "        self.Len_word_vec = Len_word_vec\n",
    "    def __len__(self):\n",
    "        return len(self.DF)\n",
    "    def __getitem__(self, Index) -> '(sample: (torch.tensor), label: (torch.tensor))':\n",
    "        all_words = [word.strip() for word in self.DF['text'][Index].strip().split('||')]\n",
    "        for i in range(len(all_words)):\n",
    "            for rule in self.rules:\n",
    "                all_words[i] = re.sub(*rule, all_words[i])\n",
    "        Label = [float(word.strip()) for word in self.Label_DF['text'][Index].strip().split('||')]\n",
    "        mask = [1.0]*len(all_words)\n",
    "        if len(all_words) < self.Len_word_vec:\n",
    "            Label = Label + [2.0]*(self.Len_word_vec - len(all_words))\n",
    "            mask = mask + [0.0]*(self.Len_word_vec - len(all_words))\n",
    "            all_words = all_words + ['<pad>']*(self.Len_word_vec - len(all_words))\n",
    "        # print(len(all_words))\n",
    "        # print(len(Label))\n",
    "        # print(len(mask))\n",
    "        # print('----------')\n",
    "        return (all_words, torch.tensor(Label), torch.tensor(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "with open('../clean_384.txt', 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        if line.strip() != '':\n",
    "            cnt = cnt + 1\n",
    "print(cnt)\n",
    "\n",
    "cnt = 0\n",
    "with open('../label_384.txt', 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        if line.strip() != '':\n",
    "            cnt = cnt + 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {0:'I', 1:'B', 2:'O', 3:'<PAD>'}\n",
    "all=allowed_transitions('IOB1', tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 4\n",
    "mt_crf = ConditionalRandomField(num_tags=num_tags, constraints =all, include_start_end_transitions= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=4\n",
    "batch_size=3\n",
    "\n",
    "data = torch.randn(seq_length, batch_size, num_tags)#shape(seq_length, batch_size, num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([[0,0,0],[0,0,0],[2,2,2],[0,3,3]])#shape = (seq_length, batch_size)\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[1,1,1],[1,1,1], [1,1,1],[0,1,0]])\n",
    "#(seq_length, batch_size)\n",
    "#mask = torch.tensor([[1,0,0],[0,1,0],[0,0,1]])\n",
    "print(mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_crf(data, target,mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(len_row, len_col):\n",
    "    for i in range(len_row):\n",
    "        for j in range(len_col):\n",
    "            yield(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([[1,0,0,0],[1,1,1,0]])\n",
    "print(x[0,0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = x.size()\n",
    "max_len = 0\n",
    "prev_col = 1\n",
    "for row, col in get_index(r,c):\n",
    "    if prev_col == 1 and x[row,col].item() == 0:\n",
    "        max_len = max(max_len, col)\n",
    "    prev_col = x[row,col].item()\n",
    "#print(max_len)\n",
    "x = x[:,:max_len]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeDistributed(nn.Module):\n",
    "#     def __init__(self, layer: '(nn.Module) layer to be processed', time_steps: '(int)'):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([layer for i in range(time_steps)])\n",
    "\n",
    "#     def forward(self, x) -> '(torch.tensor) shape=(1, embedding_size)':\n",
    "#         batch_size, time_steps, C, H, W = x.size()\n",
    "#         output = torch.tensor([])\n",
    "#         for i in range(time_steps):\n",
    "#           output_t = self.layers[i](x[:, i, :, :, :])\n",
    "#           output_t  = torch.flatten(output_t)\n",
    "#           output = torch.cat((output, output_t ), 1)\n",
    "#         return output\n",
    "\n",
    "# class Convs(nn.Module):\n",
    "#     def __init__(self, List_of_kernel_sizes: 'example: [(3,100),(5,100),(7,100)]', List_num_filter: 'example: \\\n",
    "#     [64,64,128] ***len(List_num_filter) must equal to len(List_of_kernel_sizes)***',\\\n",
    "#     use_BN: 'see My2DConv', activation_func: 'see My2DConv', input_channel: 'see My2DConv', \\\n",
    "#     same_padding: 'see My2DConv', time_steps: 'see TimeDistributed'):\n",
    "#         tmp_List_layers = []\n",
    "#         for ind, kernel_size in enumerate(List_of_kernel_sizes):\n",
    "#             tmp_List_layers.append(TimeDistributed(My2DConv(List_num_filter[ind], use_BN, \\\n",
    "#             activation_func, input_channel, kernel_size, same_padding), time_steps))\n",
    "#         self.Layer_list = nn.ModuleList(tmp_List_layers)\n",
    "\n",
    "def get_index(len_row, len_col)->'(iterator of all ((int)row, (int)col))':\n",
    "    for i in range(len_row):\n",
    "        for j in range(len_col):\n",
    "            yield(i,j)\n",
    "\n",
    "def get_longest_seq_len(MASK: '(torch.tensor: shape=(batch_size, num_words)) \\\n",
    "    of mask 1 for non padding, 0 for otherwise')->'(int) col index of first zero in\\\n",
    "    of the longest sequence example: x=torch.tensor([[1,1,0],[1,0,0]]) -> return 2':\n",
    "    tmp_mask = np.sum(MASK.numpy(),0)\n",
    "    col = 0\n",
    "    for i in range(tmp_mask.shape[0]):\n",
    "        if tmp_mask[i]==0:\n",
    "            col = i\n",
    "            break\n",
    "    if col == 0:\n",
    "        col = tmp_mask.shape[0]\n",
    "    return col\n",
    "\n",
    "\n",
    "\n",
    "class overall_char_embedding(nn.Module):\n",
    "    def __init__(self, output_size: '(tuple of ints): (batch_size, \\\n",
    "    embedding_size_per_word)',\n",
    "    dir_char_dictionary: 'see in CharEmbedding',\n",
    "    max_len_char: 'see in CharEmbedding',\n",
    "    nums_filter: '(list) list of number of filters according to each \\\n",
    "    kernel_sizes (respectively)',\n",
    "    use_BN: 'see in My2DConv',\n",
    "    activation_func: 'see in My2DConv',\n",
    "    input_channel: 'see in My2DConv',\n",
    "    kernel_sizes: '(list) list of size of kernels used, and they will be \\\n",
    "    computed concurrently',\n",
    "    same_padding: 'see in My2DConv',\n",
    "    num_words: 'number of words used in 1 sample',\n",
    "    num_char_encoding_size: 'size of encoding for each char'):\n",
    "        super().__init__()\n",
    "        self.batch_size, self.embedding_size_per_word = output_size\n",
    "        self.Char_embedder = CharEmbedding(dir_char_dictionary,\\\n",
    "        max_len_char,  self.batch_size)\n",
    "        tmp_cnn_models = []\n",
    "        for ind_cnn, kernel_size in enumerate(kernel_sizes):\n",
    "            tmp_cnn_models.append(\\\n",
    "            My2DConv(nums_filter[ind_cnn], use_BN, activation_func, input_channel,\\\n",
    "            kernel_size, same_padding)\n",
    "            )\n",
    "        self.num_words = num_words\n",
    "        self.CNNs = nn.ModuleList(tmp_cnn_models)\n",
    "        self.MyMaxPool = nn.MaxPool2d((1, num_char_encoding_size), stride= (1,1))\n",
    "        self.MyFCN = nn.Linear(sum(nums_filter)*max_len_char, output_size[1])\n",
    "    def forward(self, x):\n",
    "        tmp_compute = self.Char_embedder(x)\n",
    "        batch_size, num_word, num_char, embedding_size = tmp_compute.size()\n",
    "        tmp_compute = tmp_compute.view(batch_size, num_word, 1, num_char, \\\n",
    "        embedding_size)\n",
    "        all_output_list = []\n",
    "        for num_word in range(self.num_words):\n",
    "            tmp_output_cnn = []\n",
    "            for tmp_cnn in self.CNNs:\n",
    "                tmp_output_cnn.append(self.MyMaxPool(tmp_cnn(tmp_compute[:,\\\n",
    "                num_word,:,:,:])).view((self.batch_size, -1)))\n",
    "            all_output_list.append(nn.ReLU()(self.MyFCN(torch.cat(tmp_output_cnn, 1))))\n",
    "        #print(all_output_list[0].size())\n",
    "        #print(len(all_output_list))\n",
    "        all_output_list = torch.stack(all_output_list, dim=1)\n",
    "        return all_output_list\n",
    "                \n",
    "class gru_crf(nn.Module):\n",
    "    def __init__(self, num_input_features: '(int) number of input features', hidden_size: '(int) number of\\\n",
    "    hidden features the outputs will also have hidden_size features', num_layers: '(int) number of \\\n",
    "    recursion', dropout_gru, bidirectional: '(bool) if True, use bidirectional GRU',\\\n",
    "    tags: \"(dict[int: str])example: {0:'I', 1:'B', 2:'O', 3:'<PAD>'}\"):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=num_input_features, hidden_size=hidden_size, num_layers=num_layers,\\\n",
    "        batch_first = True, dropout=dropout_gru, bidirectional=bidirectional)\n",
    "        all_transition=allowed_transitions('IOB1', tags)\n",
    "        #self.crf = CRF(num_tags=len(tags), batch_first= True)\n",
    "        self.crf = ConditionalRandomField(4, all_transition, include_start_end_transitions= False)\n",
    "    def forward(self, samples, target: '(torch.tensor) shape=(...............,)the target tags to be used',\\\n",
    "                mask: 'True for non-pad elements'):\n",
    "        batch_size, words, _ = samples.size()\n",
    "        tmp_t = time()\n",
    "        tmp_compute = self.gru(samples)[0].view(batch_size, words, -1)\n",
    "        print(f'total GRU time: {time() - tmp_t}')\n",
    "        index_to_cut = get_longest_seq_len(mask)\n",
    "        ##############################################\n",
    "        ###cut padding some parts out#################\n",
    "        tmp_compute = tmp_compute[:, :index_to_cut,:]\n",
    "        target = target[:, :index_to_cut]\n",
    "        mask = mask[:, :index_to_cut]\n",
    "        tmp_t = time()\n",
    "        nll_loss = self.crf(tmp_compute,target.long(),mask)\n",
    "        print(f'total CRF time: {time() - tmp_t}')\n",
    "        return nll_loss\n",
    "    def predict(self, samples, mask):\n",
    "        batch_size, words, _ = samples.size()\n",
    "        tmp_t = time()\n",
    "        tmp_compute = self.gru(samples)[0].view(batch_size, words, -1)\n",
    "        print(f'total GRU time: {time() - tmp_t}')\n",
    "        index_to_cut = get_longest_seq_len(mask)\n",
    "        ##############################################\n",
    "        ###cut padding some parts out#################\n",
    "        tmp_compute = tmp_compute[:, :index_to_cut,:]\n",
    "        mask = mask[:, :index_to_cut]\n",
    "        tmp_t = time()\n",
    "        tmp_tags = self.crf.viterbi_tags(tmp_compute,mask)\n",
    "        print(f'total CRF prediction time: {time() - tmp_t}')\n",
    "        return tmp_tags\n",
    "    \n",
    "class My2DConv(nn.Module):\n",
    "    def __init__(self, num_filter: '(int) number of filters', use_BN: '(bool) if True, use 2d-batchnorm after linear conv',\\\n",
    "    activation_func: '(bool) if True, use RELU after BN', input_channel: '(int) number of input channels', \\\n",
    "    kernel_size: '(tuple): (width, height) size of the kernels', same_padding: '(bool) if True, input_w,input_h=output_w,output_h'):\n",
    "        super().__init__()\n",
    "        if same_padding:\n",
    "            #assume that dialation = 1 and stride = 1\n",
    "            self.padding = (math.floor((kernel_size[0] - 1)/2), math.floor((kernel_size[1] -1)/2))\n",
    "        else:\n",
    "            self.padding = 0\n",
    "        self.Conv = nn.Conv2d(input_channel, num_filter, kernel_size, padding= self.padding)\n",
    "        self.use_BN = use_BN\n",
    "        self.activation_func = activation_func\n",
    "        if self.use_BN:\n",
    "            self.BN = nn.BatchNorm2d(num_filter)\n",
    "\n",
    "    def forward(self, input_data: '(torch.tensor) dimension= (batch_size, num_channel_in, in_height, in_width)') \\\n",
    "    -> '(torch.tensor) shape= (batch_size, num_filter, in_height, in_width)':\n",
    "        tmp_compute = self.Conv(input_data.float())\n",
    "        if self.use_BN:\n",
    "            tmp_compute = self.BN(tmp_compute)\n",
    "        if self.activation_func:\n",
    "            tmp_compute = nn.ReLU()(tmp_compute)\n",
    "        return tmp_compute\n",
    "        \n",
    "\n",
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self,\\\n",
    "    dir_char_dictionary: '(str) .txt',\\\n",
    "    max_len_char: '(int) max size of char representation, for example: given max_len_char=3 and word= \"abcde\" => only \"abc\" is used', batch_size):\n",
    "    #Example: given embed_capital=True and 'a' is embedded as array([1.,0.,0.,0.,0]). 'A' is then embedded as array([1.,0.,0.,0.,1.])\n",
    "        super().__init__()\n",
    "        self.dictionary = {}\n",
    "        self.max_len_char = max_len_char\n",
    "        self.batch_size = batch_size\n",
    "        with open(dir_char_dictionary, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                tmp_data = line.strip().split()\n",
    "                self.dictionary[tmp_data[0]] = np.array([float(Char) for Char in tmp_data[1:]])\n",
    "    def forward(self, list_of_tuples: '(List) for \\\n",
    "    example: [(\"w1_article1\",\"w1_article2\",...,\"w1_articlen\"),\\\n",
    "            (\"w2_article1\",\"w2_article2\",...,\"w2_articlen\"),\\\n",
    "            ....\\\n",
    "            (\"wm_article1\",\"wm_article2\",...,\"wm_articlen\"),\\\n",
    "            ]') -> '(torch.tensor) \\\n",
    "    shape:(max_len_char, len(dictionary)(+1))':\n",
    "        #Note: 1 outer list is for 1 word.\n",
    "        output = []\n",
    "        for tmp_tuple in list_of_tuples:\n",
    "            for word in tmp_tuple:\n",
    "                embedded_word = []\n",
    "                tmp_word = word\n",
    "                if len(word) > self.max_len_char:\n",
    "                    tmp_word = tmp_word[:self.max_len_char]\n",
    "                for Char in tmp_word:\n",
    "                    if Char in self.dictionary:\n",
    "                        tmp_vector = self.dictionary[Char]\n",
    "                    else:\n",
    "                        tmp_vector = np.zeros(self.dictionary['a'].shape)\n",
    "                    embedded_word.append(tmp_vector)\n",
    "                if len(embedded_word) < self.max_len_char:\n",
    "                    for i in range(self.max_len_char - len(embedded_word)):\n",
    "                        embedded_word.append(np.zeros(self.dictionary['a'].shape))\n",
    "                output.append(torch.tensor(embedded_word))\n",
    "        tensor_out = []\n",
    "        for i in range(self.batch_size):\n",
    "            tensor_out.append([])\n",
    "        for word_ind, word in enumerate(output):\n",
    "            tensor_out[word_ind%self.batch_size].append(word)\n",
    "        #print(len(tensor_out))\n",
    "        #print(tensor_out)\n",
    "        for ind in range(len(tensor_out)):\n",
    "            # for j in tensor_out[ind]:\n",
    "            #     print(j.size())\n",
    "            # print('-------------')\n",
    "            tensor_out[ind] = torch.stack(tensor_out[ind])\n",
    "        return torch.stack(tensor_out)\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    #use fasttext embedding ==> read from a file\n",
    "    def __init__(self, fasttext_dictionary_dir: '(str) .vec extension of words and embedded_vectors',\\\n",
    "     Len_embedded_vector: '(int) size of embedded each vector (300 for fasttext) **Count only numbers not words'\\\n",
    "     , batch_size) -> None:\n",
    "        #example of format in fasttext_dictionary_dir\n",
    "        #กิน 1.0 -2.666 -3 22.5 .... \\n\n",
    "        #นอน 1.5 -5.666 3 9.5 .... \\n\n",
    "        #...\n",
    "        #...\n",
    "        super().__init__()\n",
    "        self.dictionary = {}\n",
    "        self.Len_embedded_vector = Len_embedded_vector\n",
    "        self.batch_size = batch_size\n",
    "        with open(fasttext_dictionary_dir, 'r', encoding = 'utf8') as f:\n",
    "            for line in f:\n",
    "                tmp_line = line.strip()\n",
    "                tmp_words = tmp_line.split()\n",
    "                if tmp_line != '' and len(tmp_words) == self.Len_embedded_vector + 1:\n",
    "                    self.dictionary[tmp_words[0]] = np.array([float(element) for element in tmp_words[1:]])\n",
    "                else:\n",
    "                    continue\n",
    "    def forward(self, list_of_tuples: '(List) for \\\n",
    "    example: [(\"w1_article1\",\"w1_article2\",...,\"w1_articlen\"),\\\n",
    "            (\"w2_article1\",\"w2_article2\",...,\"w2_articlen\"),\\\n",
    "            ....\\\n",
    "            (\"wm_article1\",\"wm_article2\",...,\"wm_articlen\"),\\\n",
    "            ]') -> '(torch.tensor) \\\n",
    "    shape:(max_len_char, len(dictionary)(+1))':\n",
    "        tmp_list = []\n",
    "        for tmp_tuple in list_of_tuples:\n",
    "            for word in tmp_tuple:\n",
    "                if word in self.dictionary:\n",
    "                    tmp_list.append(self.dictionary[word])\n",
    "                else:\n",
    "                    #in case of OOV: Zero-vector is used.\n",
    "                    tmp_list.append(np.zeros(self.Len_embedded_vector))\n",
    "        tensor_out = []\n",
    "        for i in range(self.batch_size):\n",
    "            tensor_out.append([])\n",
    "        for i in range(len(tmp_list)):\n",
    "            tensor_out[i%self.batch_size].append(tmp_list[i])\n",
    "        for i in range(self.batch_size):\n",
    "            # print(len(tensor_out[i]))\n",
    "            # print(tensor_out[i][0])\n",
    "            tensor_out[i] = torch.tensor(tensor_out[i])\n",
    "        #print(torch.stack(tensor_out))\n",
    "        return torch.stack(tensor_out)\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class AttentionBetweenWordsAndChars(nn.Module):\n",
    "    def __init__(self, hidden_size: '(int) size of key, query and value vectors',\\\n",
    "    input_vec_size: '(int) incase of fasttext input_vec_size=300'):\n",
    "        super().__init__()\n",
    "        self.K_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.Q_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.V_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.AttLayer = ScaledDotProductAttention(math.sqrt(hidden_size), 0.1)\n",
    "    def forward(self, char_vectors, word_vectors):\n",
    "        batch_size, word_size, _ = word_vectors.size()\n",
    "        word_vectors = word_vectors.float()\n",
    "        char_vectors = char_vectors.float()\n",
    "#         print(word_vectors.size())\n",
    "#         print(char_vectors.size())\n",
    "        K = torch.stack([self.K_FCN(word_vectors),self.K_FCN(char_vectors)],dim = 2)\n",
    "        Q = torch.stack([self.Q_FCN(word_vectors),self.Q_FCN(char_vectors)],dim = 2)\n",
    "        V = torch.stack([self.V_FCN(word_vectors),self.V_FCN(char_vectors)],dim = 2)\n",
    "        all_output_list = []\n",
    "        for word_ind in range(word_size):\n",
    "            all_output_list.append(self.AttLayer(Q[:,word_ind,:,:], \\\n",
    "            K[:,word_ind,:,:], V[:,word_ind,:,:])[0].view(batch_size,-1))\n",
    "\n",
    "        return torch.stack(all_output_list,dim = 1)\n",
    "\n",
    "class over_all_NER(nn.Module):\n",
    "    def __init__(self, word_embedding_dir: '(str) see in WordEmbedding', \\\n",
    "                 Batch_size: '(int)',\\\n",
    "                 size_of_embedding: '(int) for fasttext is *300 this number will be used for char\\\n",
    "                 embedding as well', \\\n",
    "                 char_embedding_dir: '(str) see in CharEmbedding', \\\n",
    "                 max_len_char: '(int) see in overall_char_embedding', \\\n",
    "                 num_conv_filters: '(list[int]) see in overall_char_embedding', \\\n",
    "                 use_BN: '(bool) see in overall_char_embedding', \\\n",
    "                 use_activation: '(bool) see in overall_char_embedding', \\\n",
    "                 num_conv_input_channel: '(int) see in overall_char_embedding', \\\n",
    "                 kernel_sizes: '(list[tuple[int, int]]) see in overall_char_embedding', \\\n",
    "                 use_same_padding: '(bool) see in overall_char_embedding', \\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 num_char_encoding_size: '(int) see in overall_char_embedding', \\\n",
    "                 att_hidden_size: '(int) see in AttentionBetweenWordsAndChars', \\\n",
    "                 num_input_features: '(int) see in gru_crf', gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf'):\n",
    "        super().__init__()\n",
    "        self.word_embed = WordEmbedding(word_embedding_dir, size_of_embedding, Batch_size)\n",
    "        self.char_embed = overall_char_embedding((Batch_size,size_of_embedding),char_embedding_dir,\\\n",
    "                                         max_len_char, num_conv_filters, use_BN, use_activation, \\\n",
    "                                         num_conv_input_channel, kernel_sizes, use_same_padding,\\\n",
    "                                         num_words, num_char_encoding_size)\n",
    "        self.my_attention = AttentionBetweenWordsAndChars(att_hidden_size, size_of_embedding)\n",
    "        self.gru_crf_layer = gru_crf(num_input_features, gru_hidden_size, num_words, dropout_gru, \\\n",
    "                                bidirectional, tags)\n",
    "        self.Batch_size = Batch_size\n",
    "    def forward(self, x):\n",
    "        tmp_char_en = self.char_embed(x[0])\n",
    "        tmp_word_en = self.word_embed(x[0])\n",
    "        tmp_att = self.my_attention(tmp_char_en, tmp_word_en)\n",
    "        tmp_gru_crf = self.gru_crf_layer(tmp_att, x[1], x[2].long())\n",
    "        return tmp_gru_crf/self.Batch_size\n",
    "    def predict(self, x):\n",
    "        tmp_char_en = self.char_embed(x[0])\n",
    "        tmp_word_en = self.word_embed(x[0])\n",
    "        tmp_att = self.my_attention(tmp_char_en, tmp_word_en)\n",
    "        tmp_tags = self.gru_crf_layer.predict(tmp_att, x[2].long())\n",
    "        return tmp_tags\n",
    "###\n",
    "# self, num_input_features: '(int) number of input features', hidden_size: '(int) number of\\\n",
    "#     hidden features the outputs will also have hidden_size features', num_layers: '(int) number of \\\n",
    "#     recursion', dropout_gru, bidirectional: '(bool) if True, use bidirectional GRU',\\\n",
    "#     tags: \"(dict[int: str])example: {0:'I', 1:'B', 2:'O', 3:'<PAD>'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c65fa996da3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "torch.utils.data.random_split(dataset, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 4\n",
    "dataloader = DataLoader(MyDataloader('../clean_384.txt', '../label_384.txt', RULEs, 544), batch_size=BS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embed: 22.487746238708496\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "word_embed = WordEmbedding('../fasttext.th.vec', 300, BS)\n",
    "print(f'word_embed: {time() - t1}')\n",
    "t1=time()\n",
    "tmp_all = overall_char_embedding((BS,300),'../LSTM-CRF-NER/char_vec_dictionary.txt',5,[1,1],True,True,1,[(3,135),(5,135)],True,544,135)\n",
    "my_attention = AttentionBetweenWordsAndChars(50,300)\n",
    "gru_crf_layer = gru_crf(100, 2, 544, 0.1, True, {0:'I', 1:'B', 2:'O', 3:'<PAD>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER = over_all_NER('../fasttext.th.vec', BS, 300, '../LSTM-CRF-NER/char_vec_dictionary.txt', 6, [2,2], True, True, \\\n",
    "                  1, [(3,135),(5,135)], True, 544, 135, 50, 100, 2, 0.1, True, {0:'I', 1:'B', 2:'O', 3:'<PAD>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 544, 300])\n",
      "torch.Size([4, 544, 300])\n",
      "total GRU time: 94.23186492919922\n",
      "total CRF time: 0.1108851432800293\n",
      "total processing time: 103.13286876678467\n",
      "--------------------\n",
      "torch.Size([4, 544, 300])\n",
      "torch.Size([4, 544, 300])\n",
      "total GRU time: 84.87694692611694\n",
      "total CRF time: 0.1100778579711914\n",
      "total processing time: 100.75126600265503\n",
      "--------------------\n",
      "torch.Size([4, 544, 300])\n",
      "torch.Size([4, 544, 300])\n",
      "total GRU time: 87.88698506355286\n",
      "total CRF time: 0.16074395179748535\n",
      "total processing time: 101.92423391342163\n",
      "--------------------\n",
      "torch.Size([4, 544, 300])\n",
      "torch.Size([4, 544, 300])\n",
      "total GRU time: 108.75852012634277\n",
      "total CRF time: 0.45869016647338867\n",
      "total processing time: 125.23746371269226\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for ind, i in enumerate(dataloader):\n",
    "#     print(len(i[0]))\n",
    "#     print(len(i[0][0]))\n",
    "#     print(i[0])\n",
    "    if ind > 30:\n",
    "        break\n",
    "    tmp_t = time()\n",
    "    data.append(NER(i))\n",
    "    print(f'total processing time: {time() - tmp_t}')\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word time: 0.2858550548553467\n",
      "total char time: 20.298537015914917\n",
      "total att time: 0.21376490592956543\n",
      "total GRU time: 114.46121907234192\n",
      "total CRF time: 0.20536398887634277\n",
      "total GRU_CRF time: 114.67736172676086\n",
      "total processing time: 135.47968697547913\n",
      "tensor(-5076.6465, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 0.44779396057128906\n",
      "total char time: 24.737746238708496\n",
      "total att time: 0.3094217777252197\n",
      "total GRU time: 110.95353722572327\n",
      "total CRF time: 0.12538766860961914\n",
      "total GRU_CRF time: 117.35793399810791\n",
      "total processing time: 213.4978621006012\n",
      "tensor(-5085.3843, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 0.9157102108001709\n",
      "total char time: 30.02030897140503\n",
      "total att time: 0.4075019359588623\n",
      "total GRU time: 121.80503511428833\n",
      "total CRF time: 0.1226041316986084\n",
      "total GRU_CRF time: 127.78598809242249\n",
      "total processing time: 226.17286205291748\n",
      "tensor(-5090.1235, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 0.6805779933929443\n",
      "total char time: 21.14777970314026\n",
      "total att time: 0.7313902378082275\n",
      "total GRU time: 129.44670987129211\n",
      "total CRF time: 0.22479987144470215\n",
      "total GRU_CRF time: 139.75064992904663\n",
      "total processing time: 238.2800190448761\n",
      "tensor(-5088.1816, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 1.0581068992614746\n",
      "total char time: 26.155379056930542\n",
      "total att time: 0.2717466354370117\n",
      "total GRU time: 120.22032284736633\n",
      "total CRF time: 0.17502379417419434\n",
      "total GRU_CRF time: 129.7955961227417\n",
      "total processing time: 224.59886384010315\n",
      "tensor(-5088.1948, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 0.8649961948394775\n",
      "total char time: 25.4025661945343\n",
      "total att time: 0.25536298751831055\n",
      "total GRU time: 141.12883615493774\n",
      "total CRF time: 0.17606401443481445\n",
      "total GRU_CRF time: 153.33755898475647\n",
      "total processing time: 258.79001212120056\n",
      "tensor(-5077.6543, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 1.0448393821716309\n",
      "total char time: 28.626399040222168\n",
      "total att time: 0.18304896354675293\n",
      "total GRU time: 128.08568501472473\n",
      "total CRF time: 0.2001340389251709\n",
      "total GRU_CRF time: 137.26157307624817\n",
      "total processing time: 245.78645133972168\n",
      "tensor(-5088.6685, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 1.1169588565826416\n",
      "total char time: 30.320427894592285\n",
      "total att time: 0.368196964263916\n",
      "total GRU time: 128.8946089744568\n",
      "total CRF time: 0.5492241382598877\n",
      "total GRU_CRF time: 139.97942185401917\n",
      "total processing time: 253.01054191589355\n",
      "tensor(-5084.7778, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 0.9321131706237793\n",
      "total char time: 29.172061920166016\n",
      "total att time: 0.375108003616333\n",
      "total GRU time: 128.13574314117432\n",
      "total CRF time: 0.18321514129638672\n",
      "total GRU_CRF time: 138.03272008895874\n",
      "total processing time: 250.5751931667328\n",
      "tensor(-5082.0249, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 1.001051902770996\n",
      "total char time: 30.397948026657104\n",
      "total att time: 0.44266200065612793\n",
      "total GRU time: 131.50075793266296\n",
      "total CRF time: 0.2217879295349121\n",
      "total GRU_CRF time: 142.18636107444763\n",
      "total processing time: 258.65682196617126\n",
      "tensor(-5077.2173, grad_fn=<SumBackward0>)\n",
      "----------------\n",
      "total word time: 0.9873270988464355\n",
      "total char time: 30.726272106170654\n",
      "total att time: 0.3100600242614746\n",
      "total GRU time: 132.72025990486145\n",
      "total CRF time: 0.1851820945739746\n",
      "total GRU_CRF time: 143.6447310447693\n",
      "total processing time: 257.55911135673523\n",
      "tensor(-5087.3169, grad_fn=<SumBackward0>)\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "for ind, i in enumerate(dataloader):\n",
    "    if ind > 10:\n",
    "        break\n",
    "    tmp_t1 = time()\n",
    "    word_en = word_embed(i[0])\n",
    "    print(f'total word time: {time() - tmp_t1}')\n",
    "    tmp_t = time()\n",
    "    char_en = tmp_all(i[0])\n",
    "    print(f'total char time: {time() - tmp_t}')\n",
    "    tmp_t = time()\n",
    "    att_out = my_attention(char_en, word_en)\n",
    "    del word_en, char_en\n",
    "    print(f'total att time: {time() - tmp_t}')\n",
    "    tmp_t = time()\n",
    "    gru_crf_out = gru_crf_layer(att_out, i[1], i[2])\n",
    "    print(f'total GRU_CRF time: {time() - tmp_t}')\n",
    "    print(f'total processing time: {time() - tmp_t1}')\n",
    "    print(gru_crf_out)\n",
    "    del att_out\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "2\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(tmp_gru_crf))\n",
    "print(len(tmp_gru_crf[0]))\n",
    "print(len(tmp_gru_crf[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_attention = AttentionBetweenWordsAndChars(50,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_out = []\n",
    "for i in range(len(data)):\n",
    "    t1 = time()\n",
    "    att_out.append(my_attention(output_char_en[i], word_en[i]))\n",
    "    print(f'att_layer: {time() - t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_crf_layer = gru_crf(100, 2, 544, 0.1, True, {0:'I', 1:'B', 2:'O', 3:'<PAD>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3932.5808, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_gru_crf/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(att_out)):\n",
    "    t1 = time()\n",
    "    print(gru_crf_layer(att_out[i], data[i][1], data[i][2]))\n",
    "    print(f'gru_crf: {time() - t1}')\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {0:'I', 1:'B', 2:'O', 3:'<PAD>'}\n",
    "all_transition=allowed_transitions('IOB1', tags)\n",
    "CRF = ConditionalRandomField(4, all_transition, include_start_end_transitions= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.transpose(x,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 384, 4])\n"
     ]
    }
   ],
   "source": [
    "print(x2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8493.3369, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRF(x2,y.long(),z)#torch.transpose(z,1,0).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 384, 4])\n",
      "torch.Size([16, 384])\n",
      "torch.Size([16, 384])\n"
     ]
    }
   ],
   "source": [
    "print(x2.size())\n",
    "print(y.size())\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    if z[i,500] == 1:\n",
    "        print('no pad found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_seq_len(MARK: '(torch.tensor: shape=(batch_size, num_words)) \\\n",
    "    of mask 1 for non padding, 0 for otherwise')->'(int) col index of first zero in\\\n",
    "    of the longest sequence example: x=torch.tensor([[1,1,0],[1,0,0]]) -> return 2':\n",
    "    r,c = MARK.size()\n",
    "    max_len = 0\n",
    "    prev_col = 1\n",
    "    for row, col in get_index(r,c):\n",
    "        if prev_col == 1 and MARK[row,col].item() == 2:\n",
    "            max_len = max(max_len, col)\n",
    "        prev_col = MARK[row,col].item()\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_longest_seq_len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "www=z.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543,)\n"
     ]
    }
   ],
   "source": [
    "a = np.sum(www,0)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "for i in range(a.shape[0]):\n",
    "    if a[i] == 0:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_seq_len2(MASK: '(torch.tensor: shape=(batch_size, num_words)) \\\n",
    "    of mask 1 for non padding, 0 for otherwise')->'(int) col index of first zero in\\\n",
    "    of the longest sequence example: x=torch.tensor([[1,1,0],[1,0,0]]) -> return 2':\n",
    "    tmp_mask = np.sum(MASK.numpy(),0)\n",
    "    col = 0\n",
    "    for i in range(tmp_mask.shape[0]):\n",
    "        if tmp_mask[i]==0:\n",
    "            col = i\n",
    "            break\n",
    "    if col == 0:\n",
    "        col = tmp_mask.shape[0]\n",
    "    return col, tmp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "col,v =get_longest_seq_len2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "allennlp2",
   "language": "python",
   "name": "allennlp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
