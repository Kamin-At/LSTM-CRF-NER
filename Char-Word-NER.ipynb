{"cells":[{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport regex as re\nfrom time import time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom allennlp.modules.conditional_random_field import ConditionalRandomField\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom RULE import RULEs"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"('๑', '1')\n('๒', '2')\n('๓', '3')\n('๔', '4')\n('๕', '5')\n('๖', '6')\n('๗', '7')\n('๘', '8')\n('๙', '9')\n('๐', '0')\n"}],"source":"for i in RULEs:\n    print(i)"},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[],"source":"def check_len(text_dir: 'path to text dir', delimeter: 'delimeter used for split()'):\n    Max_len = 0\n    Min_len = 1000000\n    with open(text_dir, 'r', encoding='utf8') as f:\n        for line in f:\n            tmp_len = len(line.split(delimeter))\n            Max_len = max(tmp_len, Max_len)\n            Min_len = min(tmp_len, Min_len)\n    return Max_len, Min_len"},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"data":{"text/plain":"(538, 384)"},"execution_count":122,"metadata":{},"output_type":"execute_result"}],"source":"check_len('label_384.txt', '||')"},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[],"source":"def gen_char_dicitonary(char_dict_dir: 'all unique chars', out_dic_vec_dir: 'dir of dictionary vectors',\\\n num_unique_char):\n    with open(char_dict_dir, 'r', encoding='utf8') as f:\n        with open(out_dic_vec_dir, 'w', encoding='utf8') as o1:\n            for line in f:\n                for ind, Char in enumerate(line.strip()):\n                    o1.write(Char + ' ')\n                    for i in range(num_unique_char + 1):\n                        if i != ind:\n                            o1.write('0 ')\n                        else:\n                            o1.write('1 ')\n                    o1.write('\\n')\n                    if 'a' <= Char <= 'z':\n                        o1.write(Char.upper() + ' ')\n                        for i in range(num_unique_char):\n                            if i != ind:\n                                o1.write('0 ')\n                            else:\n                                o1.write('1 ')\n                        o1.write('1 ')\n                        o1.write('\\n')"},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[],"source":"gen_char_dicitonary('./char-word-level-LSTM-CRF/char_dictionary.txt', './char-word-level-LSTM-CRF/char_vec_dictionary.txt', 134)"},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"134\n"}],"source":"with open('./char-word-level-LSTM-CRF/char_dictionary.txt', 'r', encoding='utf8') as f:\n    cnt = 0\n    for i in f:\n        for char in i.strip():\n            cnt = cnt + 1\nprint(cnt)"},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[],"source":"class MyDataloader(Dataset):\n    def __init__(self, TextDir: '.txt extension of samples', LabelDir: '.txt extension of labels',rules:\\\n    'the rules to be replaced => see in RULE.py', Len_word_vec: 'size of word vector') -> None:\n        super().__init__()\n        self.DF = pd.read_csv(TextDir, names=['text'])\n        self.Label_DF = pd.read_csv(LabelDir, names=['text'])\n        self.rules = rules\n        self.Len_word_vec = Len_word_vec\n    def __len__(self):\n        return len(self.DF)\n    def __getitem__(self, Index) -> '(sample: (torch.tensor), label: (torch.tensor))':\n        all_words = [word.strip() for word in self.DF['text'][Index].strip().split('||')]\n        for i in range(len(all_words)):\n            for rule in self.rules:\n                all_words[i] = re.sub(*rule, all_words[i])\n        Label = [float(word.strip()) for word in self.Label_DF['text'][Index].strip().split('||')]\n        if len(all_words) < self.Len_word_vec - 1:\n            Label = Label + [0.0]*(self.Len_word_vec - len(all_words))\n            all_words = all_words + ['<\\s>'] + ['']*(self.Len_word_vec - 1 - len(all_words))\n        # print(len(Label))\n        # print(len(all_words))\n        # print('-----------')\n        return (all_words, torch.tensor(Label))"},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[],"source":"class NameEntityRecognition(nn.Module):\n    def __init__(self, ):\n        pass\n\n\n\n# class TimeDistributed(nn.Module):\n#     def __init__(self, layer: '(nn.Module) layer to be processed', time_steps: '(int)'):\n#         super().__init__()\n#         self.layers = nn.ModuleList([layer for i in range(time_steps)])\n\n#     def forward(self, x) -> '(torch.tensor) shape=(1, embedding_size)':\n#         batch_size, time_steps, C, H, W = x.size()\n#         output = torch.tensor([])\n#         for i in range(time_steps):\n#           output_t = self.layers[i](x[:, i, :, :, :])\n#           output_t  = torch.flatten(output_t)\n#           output = torch.cat((output, output_t ), 1)\n#         return output\n\n# class Convs(nn.Module):\n#     def __init__(self, List_of_kernel_sizes: 'example: [(3,100),(5,100),(7,100)]', List_num_filter: 'example: \\\n#     [64,64,128] ***len(List_num_filter) must equal to len(List_of_kernel_sizes)***',\\\n#     use_BN: 'see My2DConv', activation_func: 'see My2DConv', input_channel: 'see My2DConv', \\\n#     same_padding: 'see My2DConv', time_steps: 'see TimeDistributed'):\n#         tmp_List_layers = []\n#         for ind, kernel_size in enumerate(List_of_kernel_sizes):\n#             tmp_List_layers.append(TimeDistributed(My2DConv(List_num_filter[ind], use_BN, \\\n#             activation_func, input_channel, kernel_size, same_padding), time_steps))\n#         self.Layer_list = nn.ModuleList(tmp_List_layers)\n\nclass My2DConv(nn.Module):\n    def __init__(self, num_filter: '(int) number of filters', use_BN: '(bool) if True, use 2d-batchnorm after linear conv',\\\n    activation_func: '(bool) if True, use RELU after BN', input_channel: '(int) number of input channels', \\\n    kernel_size: '(int or tuple) size of the kernels', same_padding: '(bool) if True, input_w,input_h=output_w,output_h'):\n        super().__init__()\n        if same_padding:\n            #assume that dialation = 1 and stride = 1\n            self.padding = (math.floor((kernel_size[0] - 1)/2), math.floor((kernel_size[1] -1)/2))\n        else:\n            self.padding = 0\n        self.Conv = nn.Conv2d(input_channel, num_filter, kernel_size, padding= self.padding)\n        self.use_BN = use_BN\n        self.activation_func = activation_func\n        if self.use_BN:\n            self.BN = nn.BatchNorm2d(num_filter)\n\n    def forward(self, input_data: '(torch.tensor) dimension= (batch_size, num_channel_in, in_height, in_width)') \\\n    -> '(torch.tensor) shape= (batch_size, num_filter, in_height, in_width)':\n        tmp_compute = self.Conv(input_data)\n        if self.use_BN:\n            tmp_compute = self.BN(tmp_compute)\n        if self.activation_func:\n            tmp_compute = nn.ReLU()(tmp_compute)\n        return tmp_compute\n        \n\nclass CharEmbedding(nn.Module):\n    def __init__(self,\\\n    dir_char_dictionary: '(str) .txt extension of characters in the same line, for example: abcdefg.......ฬอฮ',\\\n    max_len_char: '(int) max size of char representation, for example: given max_len_char=3 and word= \"abcde\" => only \"abc\" is used', batch_size):\n    #Example: given embed_capital=True and 'a' is embedded as array([1.,0.,0.,0.,0]). 'A' is then embedded as array([1.,0.,0.,0.,1.])\n        super().__init__()\n        self.dictionary = {}\n        self.max_len_char = max_len_char\n        self.batch_size = batch_size\n        with open(dir_char_dictionary, 'r', encoding='utf8') as f:\n            for line in f:\n                tmp_data = line.strip().split()\n                self.dictionary[tmp_data[0]] = np.array([float(Char) for Char in tmp_data[1:]])\n    def forward(self, list_of_tuples: '(List) for \\\n    example: [(\"w1_article1\",\"w1_article2\",...,\"w1_articlen\"),\\\n            (\"w2_article1\",\"w2_article2\",...,\"w2_articlen\"),\\\n            ....\\\n            (\"wm_article1\",\"wm_article2\",...,\"wm_articlen\"),\\\n            ]') -> '(torch.tensor) \\\n    shape:(max_len_char, len(dictionary)(+1))':\n        #Note: 1 outer list is for 1 word.\n        output = []\n        for tmp_tuple in list_of_tuples:\n            for word in tmp_tuple:\n                embedded_word = []\n                tmp_word = word\n                if len(word) > self.max_len_char:\n                    tmp_word = tmp_word[:self.max_len_char]\n                for Char in tmp_word:\n                    if Char in self.dictionary:\n                        tmp_vector = self.dictionary[Char]\n                    else:\n                        tmp_vector = np.zeros(self.dictionary['a'].shape)\n                    embedded_word.append(tmp_vector)\n                if len(embedded_word) < self.max_len_char:\n                    for i in range(self.max_len_char - len(embedded_word)):\n                        embedded_word.append(np.zeros(self.dictionary['a'].shape))\n                output.append(torch.tensor(embedded_word))\n        tensor_out = []\n        for i in range(self.batch_size):\n            tensor_out.append([])\n        for word_ind, word in enumerate(output):\n            tensor_out[word_ind%self.batch_size].append(word)\n        print(len(tensor_out))\n        #print(tensor_out)\n        for ind in range(len(tensor_out)):\n            # for j in tensor_out[ind]:\n            #     print(j.size())\n            # print('-------------')\n            tensor_out[ind] = torch.stack(tensor_out[ind])\n        return torch.stack(tensor_out)\n\nclass WordEmbedding(nn.Module):\n    #use fasttext embedding ==> read from a file\n    def __init__(self, fasttext_dictionary_dir: '(str) .vec extension of words and embedded_vectors',\\\n     Len_embedded_vector: '(int) size of embedded each vector (300 for fasttext) **Count only numbers not words'\\\n     , batch_size) -> None:\n        #example of format in fasttext_dictionary_dir\n        #กิน 1.0 -2.666 -3 22.5 .... \\n\n        #นอน 1.5 -5.666 3 9.5 .... \\n\n        #...\n        #...\n        super().__init__()\n        self.dictionary = {}\n        self.Len_embedded_vector = Len_embedded_vector\n        self.batch_size = batch_size\n        with open(fasttext_dictionary_dir, 'r', encoding = 'utf8') as f:\n            for line in f:\n                tmp_line = line.strip()\n                tmp_words = tmp_line.split()\n                if tmp_line != '' and len(tmp_words) == self.Len_embedded_vector + 1:\n                    self.dictionary[tmp_words[0]] = np.array([float(element) for element in tmp_words[1:]])\n                else:\n                    continue\n    def forward(self, list_of_tuples: '(List) for \\\n    example: [(\"w1_article1\",\"w1_article2\",...,\"w1_articlen\"),\\\n            (\"w2_article1\",\"w2_article2\",...,\"w2_articlen\"),\\\n            ....\\\n            (\"wm_article1\",\"wm_article2\",...,\"wm_articlen\"),\\\n            ]') -> '(torch.tensor) \\\n    shape:(max_len_char, len(dictionary)(+1))':\n        tmp_list = []\n        for tmp_tuple in list_of_tuples:\n            for word in tmp_tuple:\n                if word in self.dictionary:\n                    tmp_list.append(self.dictionary[word])\n                else:\n                    #in case of OOV: Zero-vector is used.\n                    tmp_list.append(np.zeros(self.Len_embedded_vector))\n        tensor_out = []\n        for i in range(self.batch_size):\n            tensor_out.append([])\n        for i in range(len(tmp_list)):\n            tensor_out[i%self.batch_size].append(tmp_list[i])\n        for i in range(self.batch_size):\n            # print(len(tensor_out[i]))\n            # print(tensor_out[i][0])\n            tensor_out[i] = torch.tensor(tensor_out[i])\n        #print(torch.stack(tensor_out))\n        return torch.stack(tensor_out)\n\nclass ScaledDotProductAttention(nn.Module):\n    ''' Scaled Dot-Product Attention '''\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.bmm(q, k.transpose(1, 2))\n        attn = attn / self.temperature\n\n        if mask is not None:\n            attn = attn.masked_fill(mask, -np.inf)\n\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        output = torch.bmm(attn, v)\n\n        return output, attn"},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":"x = torch.tensor([[1.0,2,3],[2,3,4]])\ny = torch.tensor([[2.0,3,4],[1,2,3]])\nz = torch.tensor([[1.0,1,1],[1,1,1]])\nw = torch.stack([x,y,z])\nAtt = ScaledDotProductAttention(math.sqrt(3), 0.)"},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[[1.9696, 2.9696, 3.9696],\n         [1.9945, 2.9945, 3.9945]],\n\n        [[1.9945, 2.9945, 3.9945],\n         [1.9696, 2.9696, 3.9696]],\n\n        [[1.0000, 1.0000, 1.0000],\n         [1.0000, 1.0000, 1.0000]]])\ntensor([[[0.0304, 0.9696],\n         [0.0055, 0.9945]],\n\n        [[0.9945, 0.0055],\n         [0.9696, 0.0304]],\n\n        [[0.5000, 0.5000],\n         [0.5000, 0.5000]]])\n"}],"source":"out, att = Att(w,w,w)\nprint(out)\nprint(att)"},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[],"source":"dataloader = DataLoader(MyDataloader('clean_384.txt', 'label_384.txt', RULEs, 544), batch_size=256, shuffle=True)"},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"ok\nchar_embed: 0.011304855346679688\n256\nembedding: 24.250362873077393\nword_embed: 17.82865834236145\ntorch.Size([256, 544, 300])\nembedding: 7.2322680950164795\n"}],"source":"for ind, i in enumerate(dataloader):\n    break\nprint('ok')\nt1 = time()\nchar_embed = CharEmbedding('./char-word-level-LSTM-CRF/char_vec_dictionary.txt',5, 256)\nprint(f'char_embed: {time() - t1}')\nt1 = time()\noutput = char_embed(i[0])\nprint(f'embedding: {time() - t1}')\nt1=time()\nword_embed = WordEmbedding('./char-word-level-LSTM-CRF/fasttext.th.vec', 300, 256)\nprint(f'word_embed: {time() - t1}')\nt1=time()\nprint(word_embed(i[0]).size())\nprint(f'embedding: {time() - t1}')"},{"cell_type":"code","execution_count":211,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([256, 544, 5, 135])\ntorch.Size([256, 5, 135])\n"}],"source":"print(output.size())\nprint(output[:,1,:,:].size())"},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"หำ\n"}],"source":"a = 'ำ'\nprint('ห' + a)"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[('มี', 'สิทธิ', 'มะซัก', 'ดอก'), ('ความ', 'แตกต่าง', 'Sapindus', 'ดก'), ('เห็น', 'กัน', 'rarak', 'ดี'), ('ว่า', 'ผู้', 'ใบ', 'กว่า'), ('เนื่อง', 'ที่', 'เสม็ด', 'ปลูก'), ('จาก', 'ทำ', 'หรือ', 'ใน'), ('ประเทศไทย', 'กรรม', 'เสม็ด', 'ที่'), ('เป็น', 'ดี', 'ขาว', 'ร่ม'), ('ประเทศ', 'มา', 'Melaleuca', 'ขยาย'), ('เกษตรกรรม', 'มาก', 'cajuputi', 'พันธุ์'), ('ประชาชน', 'ที่สุด', 'ต้น', 'โดย'), ('ส่วน', 'ก็', 'ขอบชะนาง', 'ใช้'), ('ใหญ่', 'คือ', 'หรือ', 'กิ่ง'), ('ซึ่ง', 'พระมหากษัตริย์', 'หญ้า', 'ตอน'), ('อยู่', 'เพราะ', 'หนอนตาย', '113'), ('ใน', 'ทรง', 'Pouzolzia', 'สายหยุด'), ('ชนบท', 'เป็น', 'pentandra', 'Desmos'), ('นั้น', 'พระโพธิสัตว์', 'เปลือก', 'chinensis'), ('ประกอบ', 'ทรง', 'ใบ', 'Lour.'), ('อาชีพ', 'มี', 'และ', 'สายหยุด'), ('ทาง', 'ฐานะ', 'ผล', 'เป็น'), ('การ', 'เป็น', 'สะเดา', 'ไม้'), ('เกษตร', 'ธรรมมิกราชาธิราช', 'Azadirachta', 'พุ่ม'), ('และ', 'ซึ่ง', 'indica', 'หรือ'), ('เนื่อง', 'ทำ', 'เปลือก', 'ไม้'), ('จาก', 'หน้าที่', 'กระเจา', 'รอ'), ('พื้นที่', 'จรรโลง', 'หรือ', 'เลื้อย'), ('เกษตร', 'ธรรมะ', 'กระเชา', 'ที่'), ('ใน', 'ด้วย', 'Holoptelea', 'รู้จัก'), ('ชนบท', 'การ', 'integrifolia', 'กัน'), ('ส่วน', 'สอน', 'ใบ', 'ดี'), ('ใหญ่', 'ให้', 'สด', 'อีก'), ('ต้อง', 'คน', 'กว้าว', 'ชนิด'), ('พึ่ง', 'ทั้งหลาย', 'Haldina', 'หนึ่ง'), ('น้ำ', 'ใน', 'cordifolia', 'วงศ์'), ('ฝน', 'รัฐ', '238', 'เดียว'), ('จึง', 'ได้', 'พืช', 'กับ'), ('ทำ', 'เข้าใจ', 'มี', 'นมแมว'), ('ให้', 'ธรรมะ', 'พิษ', 'และ'), ('ผลิตผล', 'จะ', 'มนุษย์', 'ลำดวน'), ('ที่', 'ได้', 'ได้', 'ใน'), ('ได้', 'ปฏิบัติ', 'นำ', 'พรรณ'), ('ไม่', 'ตน', 'พิษ', 'ไม้'), ('แน่นอน', 'อย่าง', 'ของ', 'ใน'), ('ส่ง', 'ถูกต้อง', 'พืช', 'วรรณกรรม'), ('ผล', 'ตาม', 'บาง', 'สุนทรภู่'), ('กระทบ', 'หลัก', 'ชนิด', 'เสวตร'), ('ต่อ', 'ธรรมะ', 'มา', 'เปี่ยมพงศ์สานต์'), ('ชีวิต', 'หาก', 'ใช้', 'บรรยาย'), ('ความ', 'ผู้', 'ประโยชน์', 'ถึง'), ('เป็นอยู่', 'ใด', 'เช่น', 'สายหยุด'), ('ของ', 'ทำ', 'นำ', 'ไว้'), ('ราษฎร', 'ผิด', 'น้ำ', 'ดัง'), ('เป็น', 'หลัก', 'ยาง', 'นี้'), ('อย่าง', 'ธรรมะ', 'ของ', 'สายหยุดพุ่มซุ้มใหญ่ที่ท้ายเขา'), ('มาก', 'พระองค์', 'พืช', 'ในตอนเช้ากลิ่นขจรก่อนจะสาย'), ('ดัง', 'จะ', 'ที่', 'พอสายคล้อยหน่อยก็สิ้นซึ่งกลิ่นอาย'), ('นั้น', 'ทรง', 'เป็น', 'น่าเสียดายที่กลิ่นสิ้นเร็วไป'), ('การ', 'พิจารณา', 'พิษ', 'สายหยุด'), ('อนุรักษ์', 'ลง', 'มา', 'มี'), ('และ', 'ทัณฑ์', 'ใช้', 'กลิ่น'), ('การ', 'อย่าง', 'อาบ', 'หอม'), ('พัฒนา', 'ยุติธรรม', 'ลูกดอก', 'ใน'), ('ทรัพยากร', 'เพื่อ', 'ไว้', 'เวลา'), ('ด้าน', 'ให้', 'ใช้', 'กลาง'), ('แหล่ง', 'บุคคล', 'ใน', 'คืน'), ('น้ำ', 'นั้น', 'การ', 'ไป'), ('จึง', 'รู้', 'ล่า', 'จน'), ('มี', 'ตัว', 'สัตว์', 'ถึง'), ('ความ', 'ว่า', 'หรือ', 'เช้า'), ('จำเป็น', 'ตน', 'ยิง', 'พอ'), ('อย่าง', 'ได้', 'ศัตรู', 'สาย'), ('ยิ่ง', 'กระทำ', 'และ', 'ก็'), ('ใน', 'ผิด', 'นำ', 'หมด'), ('การ', 'จะ', 'พืช', 'กลิ่น'), ('บรรเทา', 'ได้', 'ที่', 'เป็น'), ('ปัญหา', 'ไม่', 'มี', 'ที่มา'), ('การ', 'กระทำ', 'พิษ', 'ของ'), ('ขาดแคลน', 'เช่น', 'หลาย', 'ชื่อ'), ('น้ำ', 'นั้น', 'ชนิด', 'พันธุ์'), ('เพื่อ', 'อีก', 'มา', 'ไม้'), ('การ', 'ใน', 'ใช้', 'ชนิด'), ('อุปโภคบริโภค', 'ภาย', 'เบื่อ', 'นี้'), ('และ', 'ภาค', 'ปลา', 'ซึ่ง'), ('เพื่อ', 'หน้า', 'ผู้คน', 'ยัง'), ('การ', 'ชีวิต', 'พื้นบ้าน', 'มี'), ('เกษตร', 'จะ', 'ได้', 'ชื่อ'), ('ของ', 'ได้', 'สั่งสม', 'อื่น'), ('ประชาชน', 'ไม่', 'ความ', 'ๆ'), ('ส่วน', 'ตก', 'รู้', 'อีก'), ('ใหญ่', 'ไป', 'และ', 'ได้แก่'), ('ของ', 'สู่', 'ประสบการณ์', 'สาวหยุด'), ('ชาติ', 'อบายภูมิ', 'ใน', 'เครือเขาแกลบ'), ('๔', 'ทั้งนี้', 'การ', 'เสลาเพชร'), ('.', 'ไม่', 'เสาะแสวงหา', 'และ'), ('คุณภาพ', 'จำกัด', 'พืช', 'กล้วยเครือ'), ('ชีวิต', 'อยู่', 'อาหาร', 'พันธุ์'), ('พระบาทสมเด็จพระเจ้าอยู่หัว', 'เฉพาะ', 'ใน', 'ไม้'), ('ทรง', 'คน', 'ขณะ', 'ชนิด'), ('พิจารณา', 'ใน', 'เดียว', 'นี้'), ('แนวทาง', 'เมืองไทย', 'กัน', 'พบ'), ('ใน', 'เท่า', 'ได้', 'ขึ้น'), ('การ', 'นั้น', 'รู้จัก', 'ทั่ว'), ('พัฒนา', 'เพราะ', 'พืช', 'ประเทศ'), ('คุณภาพ', 'ธรรมมิกราชาธิราช', 'ที่', 'ถ้า'), ('ชีวิต', 'ทรง', 'มี', 'ขึ้น'), ('ของ', 'ปกครอง', 'พิษ', 'กลาง'), ('ประชากร', 'รัฐ', 'เพื่อ', 'แจ้ง'), ('โดย', 'แบบ', 'หลีกเลี่ยง', 'ไกล'), ('การ', 'จักรวาล', 'พิษ', 'ไม้'), ('ให้', 'จึง', 'ของ', 'ใหญ่'), ('ความ', 'ทรง', 'พืช', 'มี'), ('สำคัญ', 'ทำ', 'ที่', 'ลักษณะ'), ('ต่อ', 'หน้าที่', 'ทำ', 'เป็น'), ('ปัญหา', 'ดัง', 'อันตราย', 'พุ่ม'), ('พื้นฐาน', 'กล่าว', 'ต่อ', 'แต่'), ('คือ', 'ข้าง', 'ร่างกาย', 'ถ้า'), ('เรื่อง', 'ต้น', 'หรือ', 'อยู่'), ('เกี่ยว', 'นี้', 'การ', 'ใกล้'), ('กับ', 'ใน', 'ดำรง', 'เสา'), ('ปัจจัย', 'ประเทศราช', 'ชีพ', 'หรือ'), ('สี่', 'ทั้งหลาย', 'ผู้คน', 'ไม้'), ('คือ', 'เช่น', 'พื้นบ้าน', 'ใหญ่'), ('ต้อง', 'ลาว', 'จึง', 'จะ'), ('มี', 'กัมพูชา', 'ต้อง', 'ทอด'), ('อาหารการกิน', 'ล้านนา', 'ศึกษา', 'กิ่ง'), ('ที่', 'รวม', 'หา', 'พาดพิง'), ('มี', 'ทั้ง', 'วิธี', 'ไป'), ('คุณภาพ', 'ใน', 'ที่', 'กับ'), ('มี', 'พม่า', 'จะ', 'สิ่ง'), ('เครื่อง', 'ด้วย', 'กำจัด', 'ที่'), ('นุ่งห่ม', 'ใน', 'พิษ', 'อยู่'), ('ที่', 'การ', 'ใน', 'ใกล้'), ('อยู่', 'ทำ', 'พืช', 'ใบ'), ('อาศัยที่', 'สงคราม', 'ให้', 'เป็น'), ('ถูกต้อง', 'กับ', 'หมด', 'ใบ'), ('ตาม', 'พม่า', 'ไป', 'เดี่ยว'), ('สุขอนามัย', 'นั้น', 'จน', 'ค่อนข้าง'), ('การ', 'มี', 'สามารถ', 'ดก'), ('ระวัง', 'การ', 'นำ', 'เรียง'), ('รักษา', 'สร้าง', 'มา', 'สลับ'), ('สุขภาพ', 'คำ', 'ใช้', 'รูป'), ('ร่างกาย', 'อธิบาย', 'ประโยชน์', 'ใบ'), ('ของ', 'ขึ้น', 'ได้', 'รี'), ('ประชาชน', 'มา', 'เช่น', 'หรือ'), ('ให้', 'ว่า', 'บอน', 'รูป'), ('พ้น', 'เป็น', 'Colocasia', 'รี'), ('จาก', 'การ', 'antiquarum', 'แกม'), ('โรคภัย', 'ทำ', 'ส่วน', 'ใบ'), ('ที่', 'สงคราม', 'ต่างๆ', 'หอก'), ('คอย', 'เพื่อ', 'มี', 'ปลาย'), ('เบียดเบียน', 'ช่วย', 'ยาง', 'แหลม'), ('ตลอดจน', 'คน', 'ที่', 'ใบ'), ('การ', 'ทั้งหลาย', 'ทำ', 'บาง'), ('ให้', 'ใน', 'ให้', 'เหนียว'), ('การ', 'กอง', 'เกิด', 'ผิว'), ('ศึกษา', 'ทัพ', 'ความ', 'ใบ'), ('และ', 'พม่า', 'ระคาย', 'ด้าน'), ('การ', 'ให้', 'เคือง', 'บน'), ('เสริมสร้าง', 'ยุติ', 'ต่อ', 'เขียว'), ('คุณธรรม', 'การ', 'เยื่อ', 'เป็น'), ('และ', 'กระทำ', 'บุ', 'มัน'), ('จริยธรรม', 'อัน', 'ปาก', 'ด้าน'), ('ให้', 'ผิด', 'และ', 'ล่าง'), ('แก่', 'หลัก', 'ลำ', 'สี'), ('ประชาชน', 'ธรรมะ', 'คอ', 'เขียว'), ('ด้วย', 'เพราะ', 'เมื่อ', 'ปน'), ('ทั้งนี้', 'การ', 'ยาง', 'นวล'), ('ทรง', 'ยก', 'ถูก', 'แผ่น'), ('เห็น', 'ทัพ', 'ผิวหนัง', 'ใบ'), ('ว่า', 'มา', 'ทำ', 'เป็น'), ('การ', 'ตี', 'ให้', 'คลื่น'), ('พัฒนา', 'ไทย', 'คัน', 'เล็กน้อย'), ('ต่างๆ', 'เป็น', 'หรือ', 'ใบ'), ('จะ', 'การ', 'เป็น', 'ยาว'), ('ได้', 'ประกอบ', 'ผื่น', '๗'), ('ผล', 'อกุศลกรรม', 'แต่', '๑๒'), ('นั้น', 'ซึ่ง', 'ชาว', 'ซม.'), ('จะ', 'จะ', 'บ้าน', 'กว้าง'), ('ต้อง', 'ทำ', 'รู้จัก', '๒'), ('เริ่มต้น', 'ให้', 'นำ', '.'), ('ด้วย', 'ชีวิต', 'ก้าน', '๕'), ('การ', 'ของ', 'ใบ', '๓'), ('ที่', 'คน', 'มา', '.'), ('มี', 'ทั้งหลาย', 'ดอง', '๕'), ('กำลัง', 'ใน', 'กับ', 'ซม.'), ('คน', 'กอง', 'เกลือ', 'ดอก'), ('กำลังใจ', 'ทัพ', 'กิน', 'ออก'), ('ที่', 'พม่า', 'ต่าง', 'ตาม'), ('ดี', 'ต้อง', 'ผัก', 'ซอก'), ('เพื่อ', 'ตก', 'ดอง', 'ใบ'), ('ให้', 'ไป', 'หรือ', 'และ'), ('สามารถ', 'สู่', 'นำ', 'ห้อย'), ('ต่อสู้', 'อบายภูมิ', 'มา', 'ลง'), ('ฝ่าฟัน', 'เมื่อ', 'ต้ม', 'มี'), ('กับ', 'ธรรมมิกราชาธิราช', 'เคี่ยว', '๖'), ('ปัญหา', 'ได้', 'ใน', 'กลีบ'), ('อุปสรรค', 'ทรง', 'แกง', 'แต่ละ'), ('ที่', 'ทำ', 'บอน', 'กลีบ'), ('อาจ', 'หน้าที่', 'เผือก', 'เป็น'), ('เกิด', 'จรรโลง', 'Colocasia', 'แถบ'), ('ขึ้น', 'ธรรมะ', 'esculenta', 'โค้ง'), ('และ', 'อัน', 'นิยม', 'เข้า'), ('มี', 'ทำ', 'ใช้', 'ยาว'), ('ความ', 'ให้', 'หัว', '๕'), ('เข้มแข็ง', 'คน', 'เป็น', '๗'), ('ที่', 'ทั้งหลาย', 'อาหาร', 'ซม.'), ('จะ', 'มี', 'แต่', 'ขอบ'), ('พัฒนา', 'โอกาส', 'ต้อง', 'กลีบ'), ('เพื่อ', 'ประพฤติ', 'ต้ม', 'เป็น'), ('ตน', 'ธรรม', 'ให้', 'คลื่น'), ('เอง', 'และ', 'สุก', 'และ'), ('และ', 'สร้าง', 'เสีย', 'บิด'), ('ประเทศชาติ', 'กอง', 'ก่อน', 'เล็กน้อย'), ('ต่อ', 'การ', 'เพื่อ', 'ดอก'), ('ไป', 'กุศล', 'ที่', 'บาน'), ('ดัง', 'เพื่อ', 'จะ', 'ใหม่'), ('ใน', 'บรรลุ', 'ได้', 'สี'), ('กรณี', 'นิพพาน', 'ไม่', 'เหลือง'), ('ตัวอย่าง', 'ซึ่ง', 'คัน', 'อม'), ('เช่น', 'เป็น', 'เพราะ', 'เขียว'), ('โครงการ', 'อุดมคติ', 'ใน', 'วัน'), ('ด้าน', 'ของ', 'เผือก', 'ต่อ'), ('สาธารณสุข', 'ชีวิต', 'มี', 'มา'), ('ที่', 'เช่น', 'ยาง', 'เป็น'), ('พระองค์', 'นี้', 'เช่น', 'สี'), ('ทรง', 'แล้ว', 'เดียว', 'เหลือง'), ('เริ่ม', 'คน', 'กับ', 'ทอง'), ('ดำเนิน', 'ทั้งหลาย', 'บอน', 'จน'), ('การ', 'ก็', 'แต่', 'ถึง'), ('ตั้งแต่', 'จะ', 'ปริมาณ', 'เหลือ'), ('เสด็จ', 'ต้อง', 'น้อย', 'งอม'), ('ฯ', 'มี', 'กว่า', 'น้ำตาล'), ('นิวัติ', 'ความ', 'กลอย', 'ผล'), ('กลับ', 'กตัญญูกตเวที', 'Dioscorea', 'ยาว'), ('พระนคร', 'ต่อ', 'hispida', 'เป็น'), ('ใน', 'พระองค์', 'หัว', 'ปม'), ('พ.ศ.', 'นั่น', 'กลอย', 'ๆ'), ('๒๔๙๔', 'คือ', 'ให้', 'เกิด'), ('คือ', 'ใช้', 'แป้ง', 'เป็น'), ('โครงการ', 'เรี่ยวแรง', 'มาก', 'กระจุก'), ('ก่อสร้าง', 'เป็นแดน', 'แต่', 'ขยาย'), ('อาคาร', 'แทน', 'มี', 'พันธุ์'), ('การ', 'พระเดชพระคุณ', 'สาร', 'โดย'), ('แพทย์', 'จึ่ง', 'ที่', 'การ'), ('ของ', 'จะ', 'เป็น', 'เพาะ'), ('โรง', 'ชอบ', 'พิษ', 'เมล็ด'), ('พยาบาล', 'หรือ', 'ต่อ', 'หรือ'), ('ต่างๆ', 'จึง', 'ระบบ', 'จาก'), ('เช่น', 'จะ', 'ประสาท', 'กิ่ง'), ('โรงพยาบาลภูมิพลอดุลยเดช', 'ยุติธรรม', 'ต้อง', 'ตอน'), ('ตึกวชิราลงกรณ์', '2', 'สกัด', 'ปลูก'), ('สภากาชาดไทย', 'จะ', 'สาร', 'เลี้ยง'), ('กิจกรรม', 'เห็น', 'ที่', 'ค่อนข้าง'), ('ต่อต้าน', 'ได้', 'เป็น', 'ง่าย'), ('โรค', 'ว่า', 'พิษ', 'และ'), ('โปลิโอ', 'ใน', 'ออก', 'มี'), ('ตราบจน', 'สมัย', 'เสีย', 'ดอก'), ('ปัจจุบัน', 'ต้น', 'ก่อน', 'ตลอด'), ('ซึ่ง', 'กรุงเทพฯ', 'โดย', 'ปี'), ('ยัง', 'ยุติธรรม', 'ปอก', 'เป็น'), ('มี', 'หมาย', 'เปลือก', 'ไม้'), ('หน่วย', 'ถึง', 'แล้ว', 'ประดับ'), ('แพทย์', 'ถูกต้อง', 'ฝาน', 'ที่'), ('พระราชทาน', 'ตาม', 'เป็น', 'นิยม'), ('ใน', 'หลัก', 'แผ่น', 'ปลูก'), ('หลาย', 'ธรรมะ', 'บาง', 'กัน'), ('ๆ', 'ของ', 'ๆ', 'ทั่วไป'), ('สาขา', 'พุทธศาสนา', 'นำ', 'นมแมว'), ('เพื่อ', 'พระมหากษัตริย์', 'ไป', 'Rauwenhoffia'), ('คอย', 'ทรง', 'แช่', 'siamensis'), ('ดูแลรักษา', 'เป็น', 'ใน', 'scheff.'), ('ประชาชน', 'แหล่ง', 'น้ำ', 'นมแมว'), ('ทั่วไป', 'ที่มา', 'ไหล', 'เป็น'), ('โดย', 'ของ', '๒', 'ไม้'), ('เฉพาะ', 'ความ', '๓', 'พุ่ม'), ('ชาว', 'ยุติธรรม', 'วัน', 'ขนาด'), ('ชนบท', 'เพราะ', 'หรือ', 'กลาง'), ('ที่', 'ทรง', 'หมัก', 'หรือ'), ('ห่างไกล', 'เป็น', 'เกลือ', 'ไม้'), ('เป็นต้น', 'พระโพธิสัตว์', 'นำ', 'รอ'), ('214', 'และ', 'มา', 'เลี้อย'), ('นอก', 'ธรรมมิกราชาธิราช', 'คั้น', 'พบ'), ('จาก', 'คือ', 'น้ำ', 'ขึ้น'), ('นี้', 'เป็น', 'ทิ้ง', 'ตาม'), ('ใน', 'ราชา', '๒', 'ชาย'), ('ด้าน', 'ที่', '๓', 'ป่า'), ('การ', 'ยิ่งใหญ่', 'วัน', 'ชื้น'), ('ศึกษา', 'เหนือ', 'แล้ว', 'และ'), ('ก็', 'ราชา', 'จึง', 'ป่า'), ('ได้', 'ทั้งหลาย', 'นำ', 'เบญจพรรณ'), ('ทรง', 'เนื่อง', 'มา', 'ใน'), ('เริ่ม', 'จาก', 'หุง', 'ภาค'), ('ช่วยเหลือ', 'พระองค์', 'ต้ม', 'กลาง'), ('ประชาชน', 'ทรง', 'เป็น', 'และ'), ('ใน', 'เป็น', 'อาหาร', 'ภาค'), ('ท้องถิ่น', 'ประธาน', 'ได้', 'ใต้'), ('กันดาร', 'ใน', 'หมามุ่ย', 'มี'), ('ใน', 'การ', 'Mucuna', 'การ'), ('ด้าน', 'จรรโลง', 'pruriens', 'นำ'), ('การ', 'ธรรมะ', 'ไม้', 'มา'), ('ศึกษา', 'พระองค์', 'เถา', 'ปลูก'), ('ใน', 'ทรง', 'ผล', 'เป็น'), ('พ.ศ.', 'รู้', 'เป็น', 'ไม้'), ('๒๕๑๕', 'ดี', 'ฝัก', 'ประดับ'), ('โดย', 'ว่า', 'มี', 'บ้าง'), ('ได้', 'อะไร', 'ขน', 'แต่'), ('จัดตั้ง', 'ผิด', 'สี', 'ไม่'), ('โรง', 'อะไร', 'น้ำตาล', 'แพร่หลาย'), ('เรียน', 'ถูก', 'เมื่อ', 'ปัจจุบัน'), ('ขึ้น', 'อะไร', 'ถูก', 'เป็น'), ('เรียก', 'ดี', 'ผิวหนัง', 'ที่'), ('ว่า', 'อะไร', 'ทำ', 'รู้จัก'), ('โรงเรียนร่มเกล้า', 'เลว', 'ให้', 'กัน'), ('เพื่อ', 'และ', 'คัน', 'น้อย'), ('บรรเทา', 'ทรง', 'เมื่อ', 'และ'), ('ความ', 'เมตตา', 'ฝัก', 'หา'), ('เดือดร้อน', 'สอน', 'แก่', 'ชม'), ('และ', 'ให้', 'ใน', 'ได้'), ('กระจาย', 'คน', 'ฤดู', 'ค่อนข้าง'), ('การ', 'ทั้งหลาย', 'ร้อน', 'ยาก'), ('ศึกษา', 'ได้', 'ขน', 'ทั้งๆ'), ('ให้', 'เข้าใจ', 'จะ', 'ที่'), ('แก่', 'ใน', 'ปลิว', 'เป็น'), ('บุตรธิดา', 'ธรรมะ', 'ตาม', 'พันธุ์'), ('ของ', 'เหล่า', 'ลม', 'ไม้'), ('ประชาชน', 'นี้', 'ได้', 'ที่'), ('ให้', 'ด้วย', 'ง่าย', 'ปลูก'), ('มาก', 'ไพร่', 'ทำ', 'เลี้ยง'), ('ที่สุด', 'ทั้งหลาย', 'ให้', 'ได้'), ('เพื่อ', 'จึง', 'ผู้', 'ง่าย'), ('เป็น', 'ต้อง', 'ที่', 'ดอก'), ('พื้นฐาน', 'ใช้', 'สัมผัส', 'มี'), ('สำคัญ', 'แรง', 'มี', 'กลิ่น'), ('ใน', 'งาน', 'อาการ', 'หอม'), ('การ', 'ทำ', 'คัน', 'เย็น'), ('ดำรง', 'งาน', 'โดย', 'ชื่นใจ'), ('ชีวิต', 'ให้', 'ไม่', 'คน'), ('ที่', 'พระองค์', 'ทราบ', 'ไทย'), ('มี', 'เป็น', 'สาเหตุ', 'จึง'), ('คุณภาพ', 'การ', 'พลับพลึง', 'เรียก'), ('ใน', 'ตอบแทน', 'Crinum', 'น้ำหอม'), ('ระยะ', 'บุญคุณ', 'asiaticum', 'ปรุง'), ('ยาว', 'กล่าว', 'พืช', 'กลิ่น'), ('ต่อ', 'โดย', 'ล้มลุก', 'ขนม'), ('ไป', 'สรุป', 'มี', 'ว่า'), ('การ', 'ก็', 'หัว', 'น้ำนมแมว'), ('สนับสนุน', 'คือ', 'ใต้', 'พันธุ์'), ('การ', 'ความ', 'ดิน', 'ไม้'), ('ศึกษา', 'ยุติธรรม', 'ถ้า', 'ชนิด'), ('นี้', 'ใน', 'กิน', 'นี้'), ('มิ', 'ยุค', 'ทำ', 'มี'), ('ได้', 'นี้', 'ให้', 'ขน'), ('ทรง', 'มา', 'เกิด', 'ละเอียด'), ('เจาะจง', 'จาก', 'อาการ', 'นุ่ม'), ('เฉพาะ', 'พุทธศาสนา', 'คลื่นไส้', 'ตาม'), ('โรง', 'และ', 'อาเจียน', 'กิ่ง'), ('เรียน', 'เป็น', 'อย่าง', 'อ่อน'), ('ระดับ', 'ฐาน', 'รุนแรง', 'ซึ่ง'), ('ประถม', 'ทาง', 'และ', 'มี'), ('เท่า', 'ความ', 'ท้องร่วง', 'สี'), ('นั้น', 'คิด', 'หาก', 'เขียว'), ('แต่', 'ให้', 'กิน', 'ปน'), ('ยัง', 'แก่', 'เกิน', 'น้ำตาล'), ('ทรง', 'ความ', 'ขนาด', 'กิ่ง'), ('คำนึง', 'สัมพันธ์', 'จะ', 'มัก'), ('ถึง', 'เชิง', 'ตาย', 'จะ'), ('โรง', 'อุปถัมภ์', '239', 'หัก'), ('เรียน', 'ระหว่าง', 'สบู่ดำ', 'คด'), ('ระดับ', 'พระมหากษัตริย์', 'Jatropha', 'ไป'), ('มัธยมศึกษา', 'กับ', 'curcas', 'มา'), ('สำหรับ', 'ประชาชน', 'และ', 'เล็กน้อย'), ('แก้', 'วัฒนธรรม', 'สบู่แดง', 'ใบ'), ('ปัญหา', 'ไทย', 'Jatropha', 'เป็น'), ('ให้', 'และ', 'gossypifolia', 'ใบ'), ('นัก', 'ความ', 'เมล็ด', 'เดี่ยว'), ('เรียน', 'ยุติธรรม', 'มี', 'และ'), ('ธรรมดา', 'ใน', 'พิษ', 'ดก'), ('ที่', 'สมัย', 'กิน', 'เรียง'), ('ประสงค์', 'สมบูรณาญาสิทธิราชย์', 'แล้ว', 'สลับ'), ('จะ', '2430', 'ทำ', 'รูป'), ('ศึกษา', '2475', 'ให้', 'ขอบ'), ('ต่อ', 'ระบบ', 'ปวด', 'ขนาน'), ('ได้', 'ความ', 'ท้อง', 'แกม')], tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])]\n"}],"source":"n=0\nfor i in dataloader:\n    if n >= 1:\n        break\n    print(i)\n    n = n + 1"},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"3\n"}],"source":"a= ('a','b','c')\nprint(len(a))"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"384\n4\n"}],"source":"print(len(i[0]))\nprint(len(i[0][2]))"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":"'วาน||นี้||27||พ.ค.||นายโสภณ  ซารัมย์||รัฐมนตรี||ว่าการ||กระทรวงคมนาคม||เปิดเผย||ถึง||โครงการ||เช่า||รถ||เมล์||4||000||คัน||วงเงิน||6||.||79||หมื่น||ล้าน||บาท||โดย||ยอม||รับ||ว่า||จนถึง||ขณะ||นี้||ประชาชน||และ||คน||ส่วน||ใหญ่||ยัง||ไม่||เข้าใจ||โครงการ||นี้||เข้าใจ||ว่า||ใช้||เงิน||งบ||ประมาณ||ซึ่ง||จริง||ๆ||แล้ว||ไม่||ใช่||คาด||ว่า||เป็น||เพราะ||ช่วง||ก่อนหน้า||นี้||ไม่||ได้||มี||การ||อธิบาย||เรื่อง||ดัง||กล่าว||อย่าง||ละเอียด||และ||คิด||ว่า||พูด||เพียง||เล็กน้อย||แล้ว||คน||จะ||เข้าใจ||แต่||พอ||ถึง||เวลา||จริง||คน||ก็||ไม่||เข้าใจ||อย่าง||ไร||ก็ตาม||ตน||จะ||อธิบาย||ให้||ประชาชน||และ||คณะ||รัฐมนตรี||ครม.||รับฟัง||ไป||เรื่อย||ๆ||เชื่อ||ว่า||จะ||สามารถ||ตอบ||คำ||ถาม||ได้||ทุก||คำ||ถาม||และ||เมื่อ||ตอบ||ได้||ทุก||คำ||ถาม||แล้ว||หาก||ยัง||ไม่||ยอม||อนุมัติ||ให้||อีก||ก็||คง||จะ||ต้อง||ถาม||ว่า||จะ||เอา||อย่าง||ไร||และ||คน||กทม.||จะ||ว่า||อย่าง||ไร||ค่า||เช่า||ที่||คิด||ใน||โครงการ||ถือ||ได้||ว่า||ถูก||มาก||หาก||เมื่อ||เทียบ||กับ||การ||เช่า||รถ||ทัวร์||ไป||ต่าง||จังหวัด||ที่||ปัจจุบัน||คิด||วัน||ละ||15||000||บาท||แต่||ของ||เรา||เช่า||จริง||แค่||4||พัน||กว่า||บาท||เท่า||นั้น||เมื่อ||รวม||ค่า||น้ำมัน||และ||บริหารจัดการ||อื่น||ๆ||ไป||ด้วย||ก็||จะ||ทำ||ให้||ต้นทุน||เป็น||9||พัน||บาท||ต่อ||วัน||ซึ่ง||ถูก||กว่า||ที่||มี||การ||เช่า||รถ||ทัวร์||ไป||เที่ยว||อีก||นายโสภณ ||กล่าว||รัฐมนตรี||ว่าการ||กระทรวงคมนาคม||กล่าว||ต่อ||ว่า||ใน||การ||ประชุม||ครม.||วัน||ที่||26||พ.ค.||ที่||ผ่าน||มา||ตน||เป็น||ผู้||ขอ||ให้||นำ||เรื่อง||แผน||ฟื้นฟู||องค์การขนส่งมวลชนกรุงเทพ||ขสมก.||ซึ่ง||มี||โครงการ||เช่า||รถ||เมล์||4||000||คัน||เข้า||สู่||วาระ||การ||พิจารณา||ใน||ที่ประชุม||ครม.||แต่||เมื่อ||ไม่||ทัน||และ||ถูก||บรรจุ||เป็น||วาระจร||ตน||จึง||ขอ||ถอน||ออก||มา||เพื่อ||เสนอ||ใน||ที่||ประชุม||ครม.||สัปดาห์||หน้า||ประเด็น||ที่||ยัง||มี||ข้อ||สงสัย||เรื่อง||ค่า||บำรุงรักษา||ตัว||รถ||ซึ่ง||จะ||ต้อง||จ่าย||ตั้งแต่||ปี||แรก||นั้น||เนื่อง||จาก||ขสมก.||ได้||นำ||ผล||การ||ศึกษา||ของ||สถาบันพระปกเกล้า||มา||พิจารณา||ควบคู่||การ||ใช้จ่าย||จริง||ของ||ขสมก.||ปรากฏ||ว่า||มี||ข้อ||แตกต่าง||เรื่อง||ของ||ค่า||เครื่อง||ยนต์||ค่า||ยาง||ซึ่ง||ขสมก.||จะ||ต้อง||ใช้||อะไหล่||ใหม่||ใน||การ||ให้||บริการ||สาธารณะ||ดัง||นั้น||ค่า||บำรุงรักษา||จึง||มี||ราคา||สูง||เมื่อ||ครบ||กำหนด||ตาม||ระยะ||เวลา||ผู้||ให้||เช่า||หรือ||เอกชน||จะ||ต้อง||เปลี่ยน||อะไหล่||ตาม||ระยะ||ทาง||ที่||วิ่ง||ซึ่ง||ขสมก.||ได้||คำนวณ||ค่า||ใช้จ่าย||โดย||เฉลี่ย||เผื่อ||ไว้||ตั้งแต่||ต้น||และ||ไม่||ต้องการ||ให้||ไป||กระจุก||ตัว||อยู่||ใน||ปี||หลัง||ๆ||ฉะนั้น||ใน||การ||จัดทำ||แผน||ดำเนิน||โครงการ||เช่า||รถ||เมล์||4||000||คัน||จึง||มี||ตัว||เลข||ค่า||บำรุงรักษา||โดย||เฉลี่ย||ไว้||ใน||แต่ละ||ปี||นายโสภณ||กล่าว||ว่า||สำหรับ||วงเงิน||โครงการ||เช่า||รถ||เมล์||6||000||คัน||มี||ตัว||เลข||ประมาณ||1||.||11||แสน||ล้าน||บาท||แต่||เมื่อ||ปรับ||ลด||จำนวน||รถ||เมล์||ลง||เหลือ||4||000||คัน||ทำ||ให้||วง||เงิน||ของ||โครงการ||ลด||ลง||เหลือ||ประมาณ||6||.||25||หมื่น||ล้าน||บาท||ต่อ||มา||ได้||มี||การ||ปรับเปลี่ยน||ตัว||เลข||ใหม่||เพื่อ||ให้||สอดคล้อง||กับ||ผล||การ||ศึกษา||ของ||ฝ่าย||ที่||เกี่ยวข้อง||ตัว||เลข||ของ||โครงการ||จึง||เพิ่ม||ขึ้น||เป็น||6||.||97||หมื่น||ล้าน||บาท||อย่าง||ไร||ก็ตาม||เมื่อ||นายอภิสิทธิ์||  เวชชาชีวะ||นายก||รัฐมนตรี||ได้||ขอ||ให้||มี||การ||พิจารณา||รายละเอียด||เกี่ยว||กับ||การ||เช่า||รถ||เมล์||ทาง||กระทรวงคมนาคม||จึง||ได้||นำ||กลับ||มา||ทบทวน||และ||คำนวณ||กรอบ||วง||เงิน||โครงการ||เช่า||รถ||เป็น||ก||ว่า||6||.||79||หมื่น||ล้าน||บาท||และ||จะ||นำ||เสนอ||ตัว||เลข||ดัง||กล่าว||ต่อ||ที่ประชุม||ครม.||ใน||สัปดาห์||หน้า||นี้'"},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":"a['text'][len(a)-1]"},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"a\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\no\np\nq\nr\ns\nt\nu\nv\nw\nx\ny\nz\nก\nข\nฃ\nค\nต\nฆ\nง\nจ\nฉ\nช\nซ\nฌ\nญ\nฎ\nฏ\nฐ\nฑ\nฒ\nณ\nด\nต\nถ\nท\nธ\nน\nบ\nป\nผ\nฝ\nพ\nฟ\nภ\nม\nย\nร\nล\nว\nศ\nษ\nส\nห\nฬ\nอ\nฮ\nะ\nา\nิ\nี\nึ\nื\nุ\nู\nเ\nโ\nแ\nไ\nใ\nฤ\nๅ\nฦ\nั\n่\n้\n๊\n๋\n็\n์\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n,\n;\n.\n!\n?\n:\n\"\n/\n\\\n|\n_\n@\n#\n%\n&\n*\n+\n-\n=\n<\n>\n(\n)\n[\n]\n{\n}\n'\n"}],"source":"with open('char_dictionary.txt', 'r', encoding= 'utf8') as f:\n    for i in f:\n        for j in i:\n            print(j)"},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"A\na\n"}],"source":"a='A'\nprint(a)\nprint(a.lower())"},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":"a = np.zeros(5)"},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[0. 0. 0. 0. 1.]\n"}],"source":"a[-1]=1\nprint(a)"},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"data":{"text/plain":"True"},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":"a.isupper()"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[array([0., 0., 1., 0., 0.]), array([0., 1., 0., 0., 0.]), array([0., 0., 0., 0., 1.])]\n"}],"source":"a=np.zeros(5)\na[2] = 1\nb=np.zeros(5)\nb[1] = 1\nc=np.zeros(5)\nc[4] = 1\nd=[a,b,c]\nprint(d)"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1.]], dtype=torch.float64)\ntorch.Size([3, 5])\n"}],"source":"e=torch.tensor(d)\nprint(e)\nprint(e.size())"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"only one element tensors can be converted to Python scalars","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-6be22d7c607f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"]}],"source":"f = torch.tensor([e,e,e])\nprint(f)\nprint(f.size())"},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"1 2 3\n"}],"source":"a = [1,2,3]\nprint(*a)"},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":"a=np.array([1,0,0,1])\nb=np.array([1,2,0,1])"},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":"x=torch.tensor([a,a,a])"},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":"y=torch.tensor([b,b,b])"},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[[1, 0, 0, 1],\n         [1, 0, 0, 1],\n         [1, 0, 0, 1]],\n\n        [[1, 2, 0, 1],\n         [1, 2, 0, 1],\n         [1, 2, 0, 1]]])\n"}],"source":"z=torch.stack([x,y])\nprint(z)"},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"}],"source":"tensor_out = []\nfor i in range(int(64/4)):\n    tensor_out.append([])\nprint(tensor_out)"},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":"a = [[]*4]"},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":"[[]]"},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":"a"},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":"w_em = WordEmbedding('./char-word-level-LSTM-CRF/fasttext.th.vec', 300, 1)"},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"data":{"text/plain":"tensor([[[-6.6157e+00, -3.6187e+00, -4.4070e+00,  ..., -5.2342e+00,\n          -5.5859e+00, -7.4312e+00],\n         [-8.6055e-02,  2.0431e-01,  6.4967e-01,  ..., -7.4839e-02,\n           1.9741e-01, -2.8951e-01],\n         [-2.9842e-01, -2.3566e-02,  2.8939e-01,  ..., -5.4377e-01,\n           1.4998e-01, -3.4089e-01],\n         [ 7.5550e-01,  8.5246e-01,  8.0554e-01,  ...,  1.1361e+00,\n          -9.5987e-01,  5.9702e-02]]], dtype=torch.float64)"},"execution_count":125,"metadata":{},"output_type":"execute_result"}],"source":"w_em(('<\\s>'))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}