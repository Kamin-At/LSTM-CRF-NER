{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import regex as re\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from allennlp.modules.conditional_random_field import ConditionalRandomField\n",
    "from allennlp.modules.conditional_random_field import allowed_transitions\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchcrf import CRF\n",
    "\n",
    "from RULE import RULEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(dictionary_dir: '(str) directory of dictionary (fasttext format)')\\\n",
    "-> '(dict) dict[word: vector]':\n",
    "        dictionary = {}\n",
    "        with open(dictionary_dir, 'r', encoding = 'utf8') as f:\n",
    "                for line in f:\n",
    "                        tmp_line = line.strip()\n",
    "                        tmp_list = [word.strip() for word in tmp_line.split()]\n",
    "                        if tmp_line != '' and len(tmp_list) == 301:\n",
    "                                dictionary[tmp_list[0]] = np.array([float(number) for \\\n",
    "                                number in tmp_list[1:]])\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = get_dictionary('fasttext.th.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len_list = []\n",
    "total_oov_list = []\n",
    "cnt_o1 = 0\n",
    "cnt_o2 = 0\n",
    "with open('clean_384.txt', 'r', encoding='utf8') as f:\n",
    "    with open('clean_384_oov_less_10.txt', 'w', encoding='utf8') as o1:\n",
    "        with open('clean_384_oov_more_10.txt', 'w', encoding='utf8') as o2:\n",
    "            for line_ind, line in enumerate(f):\n",
    "                if line_ind % 100==0:\n",
    "                    print(line_ind)\n",
    "                tmp_line = line.strip()\n",
    "                if tmp_line != '':\n",
    "                    tmp_line = (word.strip() for word in tmp_line.split('||'))\n",
    "                    #print(tmp_line)\n",
    "                    cnt_oov = 0\n",
    "                    for ind, word in enumerate(tmp_line):\n",
    "                        if word not in my_dict:\n",
    "                            cnt_oov += 1\n",
    "                    if cnt_oov/(ind+1) < 0.1:\n",
    "                        o1.write(line.strip() + '\\n')\n",
    "                        cnt_o1 += 1\n",
    "                    else:\n",
    "                        o2.write(line.strip() + '\\n')\n",
    "                        cnt_o2 += 1\n",
    "                    total_oov_list.append(cnt_oov)\n",
    "                    total_len_list.append(ind+1)\n",
    "with open('report_oov_test.txt', 'w', encoding = 'utf8') as f:\n",
    "    f.write(f'less10_cnt: {cnt_o1}, upper10_cnt: {cnt_o2}')\n",
    "percent_oov_list = [100*total_oov_list[i]/total_len_list[i] for i in \\\n",
    "range(len(total_len_list))]\n",
    "\n",
    "plt.hist(percent_oov_list,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([i for i in percent_oov_list if i >= 10]))\n",
    "print(len([i for i in percent_oov_list if i < 10]))\n",
    "plt.hist(total_oov_list,100)\n",
    "plt.show()\n",
    "plt.hist(total_len_list,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len(text_dir: 'path to text dir', delimeter: 'delimeter used for split()'):\n",
    "    Max_len = 0\n",
    "    Min_len = 1000000\n",
    "    with open(text_dir, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            tmp_len = len(line.split(delimeter))\n",
    "            Max_len = max(tmp_len, Max_len)\n",
    "            Min_len = min(tmp_len, Min_len)\n",
    "    return Max_len, Min_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_len('label_384.txt', '||')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_char_dicitonary(char_dict_dir: 'all unique chars', out_dic_vec_dir: 'dir of dictionary vectors',\\\n",
    " num_unique_char):\n",
    "    with open(char_dict_dir, 'r', encoding='utf8') as f:\n",
    "        with open(out_dic_vec_dir, 'w', encoding='utf8') as o1:\n",
    "            for line in f:\n",
    "                for ind, Char in enumerate(line.strip()):\n",
    "                    o1.write(Char + ' ')\n",
    "                    for i in range(num_unique_char + 1):\n",
    "                        if i != ind:\n",
    "                            o1.write('0 ')\n",
    "                        else:\n",
    "                            o1.write('1 ')\n",
    "                    o1.write('\\n')\n",
    "                    if 'a' <= Char <= 'z':\n",
    "                        o1.write(Char.upper() + ' ')\n",
    "                        for i in range(num_unique_char):\n",
    "                            if i != ind:\n",
    "                                o1.write('0 ')\n",
    "                            else:\n",
    "                                o1.write('1 ')\n",
    "                        o1.write('1 ')\n",
    "                        o1.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_char_dicitonary('./char-word-level-LSTM-CRF/char_dictionary.txt', './char-word-level-LSTM-CRF/char_vec_dictionary.txt', 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./char-word-level-LSTM-CRF/char_dictionary.txt', 'r', encoding='utf8') as f:\n",
    "    cnt = 0\n",
    "    for i in f:\n",
    "        for char in i.strip():\n",
    "            cnt = cnt + 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataloader(Dataset):\n",
    "    def __init__(self, TextDir: '.txt extension of samples', LabelDir: '.txt extension of labels',rules:\\\n",
    "    'the rules to be replaced => see in RULE.py', Len_word_vec: 'size of word vector') -> None:\n",
    "        super().__init__()\n",
    "        self.DF = pd.read_csv(TextDir, names=['text'])\n",
    "        self.Label_DF = pd.read_csv(LabelDir, names=['text'])\n",
    "        self.rules = rules\n",
    "        self.Len_word_vec = Len_word_vec\n",
    "    def __len__(self):\n",
    "        return len(self.DF)\n",
    "    def __getitem__(self, Index) -> '(sample: (torch.tensor), label: (torch.tensor))':\n",
    "        all_words = [word.strip() for word in self.DF['text'][Index].strip().split('||')]\n",
    "        for i in range(len(all_words)):\n",
    "            for rule in self.rules:\n",
    "                all_words[i] = re.sub(*rule, all_words[i])\n",
    "        Label = [float(word.strip()) for word in self.Label_DF['text'][Index].strip().split('||')]\n",
    "        mask = [1.0]*len(all_words)\n",
    "        if len(all_words) < self.Len_word_vec:\n",
    "            Label = Label + [2.0]*(self.Len_word_vec - len(all_words))\n",
    "            mask = mask + [0.0]*(self.Len_word_vec - len(all_words))\n",
    "            all_words = all_words + ['<pad>']*(self.Len_word_vec - len(all_words))\n",
    "        # print(len(all_words))\n",
    "        # print(len(Label))\n",
    "        # print(len(mask))\n",
    "        # print('----------')\n",
    "        return (all_words, torch.tensor(Label), torch.tensor(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "with open('../clean_384.txt', 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        if line.strip() != '':\n",
    "            cnt = cnt + 1\n",
    "print(cnt)\n",
    "\n",
    "cnt = 0\n",
    "with open('../label_384.txt', 'r', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        if line.strip() != '':\n",
    "            cnt = cnt + 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {0:'I', 1:'B', 2:'O', 3:'<PAD>'}\n",
    "all=allowed_transitions('IOB1', tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = 4\n",
    "mt_crf = ConditionalRandomField(num_tags=num_tags, constraints =all, include_start_end_transitions= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=4\n",
    "batch_size=3\n",
    "\n",
    "data = torch.randn(seq_length, batch_size, num_tags)#shape(seq_length, batch_size, num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([[0,0,0],[0,0,0],[2,2,2],[0,3,3]])#shape = (seq_length, batch_size)\n",
    "print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[1,1,1],[1,1,1], [1,1,1],[0,1,0]])\n",
    "#(seq_length, batch_size)\n",
    "#mask = torch.tensor([[1,0,0],[0,1,0],[0,0,1]])\n",
    "print(mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_crf(data, target,mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(len_row, len_col):\n",
    "    for i in range(len_row):\n",
    "        for j in range(len_col):\n",
    "            yield(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([[1,0,0,0],[1,1,1,0]])\n",
    "print(x[0,0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = x.size()\n",
    "max_len = 0\n",
    "prev_col = 1\n",
    "for row, col in get_index(r,c):\n",
    "    if prev_col == 1 and x[row,col].item() == 0:\n",
    "        max_len = max(max_len, col)\n",
    "    prev_col = x[row,col].item()\n",
    "#print(max_len)\n",
    "x = x[:,:max_len]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeDistributed(nn.Module):\n",
    "#     def __init__(self, layer: '(nn.Module) layer to be processed', time_steps: '(int)'):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([layer for i in range(time_steps)])\n",
    "\n",
    "#     def forward(self, x) -> '(torch.tensor) shape=(1, embedding_size)':\n",
    "#         batch_size, time_steps, C, H, W = x.size()\n",
    "#         output = torch.tensor([])\n",
    "#         for i in range(time_steps):\n",
    "#           output_t = self.layers[i](x[:, i, :, :, :])\n",
    "#           output_t  = torch.flatten(output_t)\n",
    "#           output = torch.cat((output, output_t ), 1)\n",
    "#         return output\n",
    "\n",
    "# class Convs(nn.Module):\n",
    "#     def __init__(self, List_of_kernel_sizes: 'example: [(3,100),(5,100),(7,100)]', List_num_filter: 'example: \\\n",
    "#     [64,64,128] ***len(List_num_filter) must equal to len(List_of_kernel_sizes)***',\\\n",
    "#     use_BN: 'see My2DConv', activation_func: 'see My2DConv', input_channel: 'see My2DConv', \\\n",
    "#     same_padding: 'see My2DConv', time_steps: 'see TimeDistributed'):\n",
    "#         tmp_List_layers = []\n",
    "#         for ind, kernel_size in enumerate(List_of_kernel_sizes):\n",
    "#             tmp_List_layers.append(TimeDistributed(My2DConv(List_num_filter[ind], use_BN, \\\n",
    "#             activation_func, input_channel, kernel_size, same_padding), time_steps))\n",
    "#         self.Layer_list = nn.ModuleList(tmp_List_layers)\n",
    "\n",
    "def get_index(len_row, len_col)->'(iterator of all ((int)row, (int)col))':\n",
    "    for i in range(len_row):\n",
    "        for j in range(len_col):\n",
    "            yield(i,j)\n",
    "\n",
    "def get_longest_seq_len(MASK: '(torch.tensor: shape=(batch_size, num_words)) \\\n",
    "    of mask 1 for non padding, 0 for otherwise')->'(int) col index of first zero in\\\n",
    "    of the longest sequence example: x=torch.tensor([[1,1,0],[1,0,0]]) -> return 2':\n",
    "    tmp_mask = np.sum(MASK.numpy(),0)\n",
    "    col = 0\n",
    "    for i in range(tmp_mask.shape[0]):\n",
    "        if tmp_mask[i]==0:\n",
    "            col = i\n",
    "            break\n",
    "    if col == 0:\n",
    "        col = tmp_mask.shape[0]\n",
    "    return col\n",
    "\n",
    "class overall_char_embedding(nn.Module):\n",
    "    def __init__(self, output_size: '(tuple of ints): (batch_size, \\\n",
    "    embedding_size_per_word)',\n",
    "    dir_char_dictionary: 'see in CharEmbedding',\n",
    "    max_len_char: 'see in CharEmbedding',\n",
    "    nums_filter: '(list) list of number of filters according to each \\\n",
    "    kernel_sizes (respectively)',\n",
    "    use_BN: 'see in My2DConv',\n",
    "    activation_func: 'see in My2DConv',\n",
    "    input_channel: 'see in My2DConv',\n",
    "    kernel_sizes: '(list) list of size of kernels used, and they will be \\\n",
    "    computed concurrently',\n",
    "    same_padding: 'see in My2DConv',\n",
    "    num_words: 'number of words used in 1 sample',\n",
    "    num_char_encoding_size: 'size of encoding for each char'):\n",
    "        super().__init__()\n",
    "        self.batch_size, self.embedding_size_per_word = output_size\n",
    "        self.Char_embedder = CharEmbedding(dir_char_dictionary,\\\n",
    "        max_len_char,  self.batch_size)\n",
    "        tmp_cnn_models = []\n",
    "        for ind_cnn, kernel_size in enumerate(kernel_sizes):\n",
    "            tmp_cnn_models.append(\\\n",
    "            My2DConv(nums_filter[ind_cnn], use_BN, activation_func, input_channel,\\\n",
    "            kernel_size, same_padding)\n",
    "            )\n",
    "        self.num_words = num_words\n",
    "        self.CNNs = nn.ModuleList(tmp_cnn_models)\n",
    "        self.MyMaxPool = nn.MaxPool2d((1, num_char_encoding_size), stride= (1,1))\n",
    "        self.MyFCN = nn.Linear(sum(nums_filter)*max_len_char, output_size[1])\n",
    "    def forward(self, x):\n",
    "        tmp_compute = self.Char_embedder(x)\n",
    "        batch_size, num_word, num_char, embedding_size = tmp_compute.size()\n",
    "        tmp_compute = tmp_compute.view(batch_size, num_word, 1, num_char, \\\n",
    "        embedding_size)\n",
    "        all_output_list = []\n",
    "        for num_word in range(self.num_words):\n",
    "            tmp_output_cnn = []\n",
    "            for tmp_cnn in self.CNNs:\n",
    "                tmp_output_cnn.append(self.MyMaxPool(tmp_cnn(tmp_compute[:,\\\n",
    "                num_word,:,:,:])).view((self.batch_size, -1)))\n",
    "            all_output_list.append(nn.ReLU()(self.MyFCN(torch.cat(tmp_output_cnn, 1))))\n",
    "        print(all_output_list[0].size())\n",
    "        print(len(all_output_list))\n",
    "        all_output_list = torch.stack(all_output_list, dim=1)\n",
    "        return all_output_list\n",
    "                \n",
    "class gru_crf(nn.Module):\n",
    "    def __init__(self, num_input_features: '(int) number of input features', hidden_size: '(int) number of\\\n",
    "    hidden features the outputs will also have hidden_size features', num_layers: '(int) number of \\\n",
    "    recursion', dropout_gru, bidirectional: '(bool) if True, use bidirectional GRU',\\\n",
    "    tags: \"(dict[int: str])example: {0:'I', 1:'B', 2:'O', 3:'<PAD>'}\"):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=num_input_features, hidden_size=hidden_size, num_layers=num_layers,\\\n",
    "        batch_first = True, dropout=dropout_gru, bidirectional=bidirectional)\n",
    "        all_transition=allowed_transitions('IOB1', tags)\n",
    "        #self.crf = CRF(num_tags=len(tags), batch_first= True)\n",
    "        self.crf = ConditionalRandomField(4, all_transition, include_start_end_transitions= False)\n",
    "    def forward(self, samples, target: '(torch.tensor) shape=(...............,)the target tags to be used', mask: 'True for non-pad elements'):\n",
    "        batch_size, words, _ = samples.size()\n",
    "        tmp_compute = self.gru(samples)[0].view(batch_size, words, -1)\n",
    "        index_to_cut = get_longest_seq_len(mask)\n",
    "        print()\n",
    "        ##############################################\n",
    "        ###cut padding some parts out#################\n",
    "        tmp_compute = tmp_compute[:, :index_to_cut,:]\n",
    "        target = target[:, :index_to_cut]\n",
    "        mask = mask[:, :index_to_cut]\n",
    "        \n",
    "        #self.crf(tmp_compute.float(),target.float(),mask=mask.int(),reduction='sum')\n",
    "        print('tmp_compute')\n",
    "        print(tmp_compute.size())\n",
    "        print('target')\n",
    "        print(target.size())\n",
    "        print('mask')\n",
    "        print(mask.size())\n",
    "        return self.crf(tmp_compute,target.long(),mask)\n",
    "\n",
    "class My2DConv(nn.Module):\n",
    "    def __init__(self, num_filter: '(int) number of filters', use_BN: '(bool) if True, use 2d-batchnorm after linear conv',\\\n",
    "    activation_func: '(bool) if True, use RELU after BN', input_channel: '(int) number of input channels', \\\n",
    "    kernel_size: '(tuple): (width, height) size of the kernels', same_padding: '(bool) if True, input_w,input_h=output_w,output_h'):\n",
    "        super().__init__()\n",
    "        if same_padding:\n",
    "            #assume that dialation = 1 and stride = 1\n",
    "            self.padding = (math.floor((kernel_size[0] - 1)/2), math.floor((kernel_size[1] -1)/2))\n",
    "        else:\n",
    "            self.padding = 0\n",
    "        self.Conv = nn.Conv2d(input_channel, num_filter, kernel_size, padding= self.padding)\n",
    "        self.use_BN = use_BN\n",
    "        self.activation_func = activation_func\n",
    "        if self.use_BN:\n",
    "            self.BN = nn.BatchNorm2d(num_filter)\n",
    "\n",
    "    def forward(self, input_data: '(torch.tensor) dimension= (batch_size, num_channel_in, in_height, in_width)') \\\n",
    "    -> '(torch.tensor) shape= (batch_size, num_filter, in_height, in_width)':\n",
    "        tmp_compute = self.Conv(input_data.float())\n",
    "        if self.use_BN:\n",
    "            tmp_compute = self.BN(tmp_compute)\n",
    "        if self.activation_func:\n",
    "            tmp_compute = nn.ReLU()(tmp_compute)\n",
    "        return tmp_compute\n",
    "        \n",
    "\n",
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self,\\\n",
    "    dir_char_dictionary: '(str) .txt',\\\n",
    "    max_len_char: '(int) max size of char representation, for example: given max_len_char=3 and word= \"abcde\" => only \"abc\" is used', batch_size):\n",
    "    #Example: given embed_capital=True and 'a' is embedded as array([1.,0.,0.,0.,0]). 'A' is then embedded as array([1.,0.,0.,0.,1.])\n",
    "        super().__init__()\n",
    "        self.dictionary = {}\n",
    "        self.max_len_char = max_len_char\n",
    "        self.batch_size = batch_size\n",
    "        with open(dir_char_dictionary, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                tmp_data = line.strip().split()\n",
    "                self.dictionary[tmp_data[0]] = np.array([float(Char) for Char in tmp_data[1:]])\n",
    "    def forward(self, list_of_tuples: '(List) for \\\n",
    "    example: [(\"w1_article1\",\"w1_article2\",...,\"w1_articlen\"),\\\n",
    "            (\"w2_article1\",\"w2_article2\",...,\"w2_articlen\"),\\\n",
    "            ....\\\n",
    "            (\"wm_article1\",\"wm_article2\",...,\"wm_articlen\"),\\\n",
    "            ]') -> '(torch.tensor) \\\n",
    "    shape:(max_len_char, len(dictionary)(+1))':\n",
    "        #Note: 1 outer list is for 1 word.\n",
    "        output = []\n",
    "        for tmp_tuple in list_of_tuples:\n",
    "            for word in tmp_tuple:\n",
    "                embedded_word = []\n",
    "                tmp_word = word\n",
    "                if len(word) > self.max_len_char:\n",
    "                    tmp_word = tmp_word[:self.max_len_char]\n",
    "                for Char in tmp_word:\n",
    "                    if Char in self.dictionary:\n",
    "                        tmp_vector = self.dictionary[Char]\n",
    "                    else:\n",
    "                        tmp_vector = np.zeros(self.dictionary['a'].shape)\n",
    "                    embedded_word.append(tmp_vector)\n",
    "                if len(embedded_word) < self.max_len_char:\n",
    "                    for i in range(self.max_len_char - len(embedded_word)):\n",
    "                        embedded_word.append(np.zeros(self.dictionary['a'].shape))\n",
    "                output.append(torch.tensor(embedded_word))\n",
    "        tensor_out = []\n",
    "        for i in range(self.batch_size):\n",
    "            tensor_out.append([])\n",
    "        for word_ind, word in enumerate(output):\n",
    "            tensor_out[word_ind%self.batch_size].append(word)\n",
    "        #print(len(tensor_out))\n",
    "        #print(tensor_out)\n",
    "        for ind in range(len(tensor_out)):\n",
    "            # for j in tensor_out[ind]:\n",
    "            #     print(j.size())\n",
    "            # print('-------------')\n",
    "            tensor_out[ind] = torch.stack(tensor_out[ind])\n",
    "        return torch.stack(tensor_out)\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    #use fasttext embedding ==> read from a file\n",
    "    def __init__(self, fasttext_dictionary_dir: '(str) .vec extension of words and embedded_vectors',\\\n",
    "     Len_embedded_vector: '(int) size of embedded each vector (300 for fasttext) **Count only numbers not words'\\\n",
    "     , batch_size) -> None:\n",
    "        #example of format in fasttext_dictionary_dir\n",
    "        #กิน 1.0 -2.666 -3 22.5 .... \\n\n",
    "        #นอน 1.5 -5.666 3 9.5 .... \\n\n",
    "        #...\n",
    "        #...\n",
    "        super().__init__()\n",
    "        self.dictionary = {}\n",
    "        self.Len_embedded_vector = Len_embedded_vector\n",
    "        self.batch_size = batch_size\n",
    "        with open(fasttext_dictionary_dir, 'r', encoding = 'utf8') as f:\n",
    "            for line in f:\n",
    "                tmp_line = line.strip()\n",
    "                tmp_words = tmp_line.split()\n",
    "                if tmp_line != '' and len(tmp_words) == self.Len_embedded_vector + 1:\n",
    "                    self.dictionary[tmp_words[0]] = np.array([float(element) for element in tmp_words[1:]])\n",
    "                else:\n",
    "                    continue\n",
    "    def forward(self, list_of_tuples: '(List) for \\\n",
    "    example: [(\"w1_article1\",\"w1_article2\",...,\"w1_articlen\"),\\\n",
    "            (\"w2_article1\",\"w2_article2\",...,\"w2_articlen\"),\\\n",
    "            ....\\\n",
    "            (\"wm_article1\",\"wm_article2\",...,\"wm_articlen\"),\\\n",
    "            ]') -> '(torch.tensor) \\\n",
    "    shape:(max_len_char, len(dictionary)(+1))':\n",
    "        tmp_list = []\n",
    "        for tmp_tuple in list_of_tuples:\n",
    "            for word in tmp_tuple:\n",
    "                if word in self.dictionary:\n",
    "                    tmp_list.append(self.dictionary[word])\n",
    "                else:\n",
    "                    #in case of OOV: Zero-vector is used.\n",
    "                    tmp_list.append(np.zeros(self.Len_embedded_vector))\n",
    "        tensor_out = []\n",
    "        for i in range(self.batch_size):\n",
    "            tensor_out.append([])\n",
    "        for i in range(len(tmp_list)):\n",
    "            tensor_out[i%self.batch_size].append(tmp_list[i])\n",
    "        for i in range(self.batch_size):\n",
    "            # print(len(tensor_out[i]))\n",
    "            # print(tensor_out[i][0])\n",
    "            tensor_out[i] = torch.tensor(tensor_out[i])\n",
    "        #print(torch.stack(tensor_out))\n",
    "        return torch.stack(tensor_out)\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class AttentionBetweenWordsAndChars(nn.Module):\n",
    "    def __init__(self, hidden_size: '(int) size of key, query and value vectors',\\\n",
    "    input_vec_size: '(int) incase of fasttext input_vec_size=300'):\n",
    "        super().__init__()\n",
    "        self.K_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.Q_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.V_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.AttLayer = ScaledDotProductAttention(math.sqrt(hidden_size), 0.1)\n",
    "    def forward(self, char_vectors, word_vectors):\n",
    "        batch_size, word_size, _ = word_vectors.size()\n",
    "        word_vectors = word_vectors.float()\n",
    "        char_vectors = char_vectors.float()\n",
    "        K = torch.stack([self.K_FCN(word_vectors),self.K_FCN(char_vectors)],dim = 2)\n",
    "        Q = torch.stack([self.Q_FCN(word_vectors),self.Q_FCN(char_vectors)],dim = 2)\n",
    "        V = torch.stack([self.V_FCN(word_vectors),self.V_FCN(char_vectors)],dim = 2)\n",
    "        all_output_list = []\n",
    "        for word_ind in range(word_size):\n",
    "            all_output_list.append(self.AttLayer(Q[:,word_ind,:,:], \\\n",
    "            K[:,word_ind,:,:], V[:,word_ind,:,:])[0].view(batch_size,-1))\n",
    "\n",
    "        return torch.stack(all_output_list,dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 16\n",
    "dataloader = DataLoader(MyDataloader('../clean_384.txt', '../label_384.txt', RULEs, 544), batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embed: 14.280364036560059\n",
      "char_embed: 0.012252092361450195\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "word_embed = WordEmbedding('../fasttext.th.vec', 300, BS)\n",
    "print(f'word_embed: {time() - t1}')\n",
    "t1=time()\n",
    "char_embed = CharEmbedding('../LSTM-CRF-NER/char_vec_dictionary.txt',5, BS)\n",
    "print(f'char_embed: {time() - t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "embedding: 0.9911289215087891\n",
      "embedding: 0.35086679458618164\n",
      "embedding: 0.8712151050567627\n",
      "embedding: 0.3417980670928955\n",
      "embedding: 0.8621640205383301\n",
      "embedding: 0.340421199798584\n",
      "embedding: 0.8987948894500732\n",
      "embedding: 0.3425710201263428\n",
      "embedding: 0.8711140155792236\n",
      "embedding: 0.3415639400482178\n",
      "embedding: 0.8454639911651611\n",
      "embedding: 0.3358640670776367\n",
      "embedding: 0.9129228591918945\n",
      "embedding: 0.3381960391998291\n",
      "embedding: 0.849724292755127\n",
      "embedding: 0.337630033493042\n",
      "embedding: 0.8453831672668457\n",
      "embedding: 0.34116411209106445\n",
      "embedding: 0.8487789630889893\n",
      "embedding: 0.33668017387390137\n",
      "embedding: 0.8521537780761719\n",
      "embedding: 0.33583903312683105\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "word_en = []\n",
    "for ind, i in enumerate(dataloader):\n",
    "    if ind > 10:\n",
    "        break\n",
    "    data.append(i)\n",
    "print('ok')\n",
    "for i in data:\n",
    "    t1 = time()\n",
    "    char_embed(i[0])\n",
    "    print(f'embedding: {time() - t1}')\n",
    "    t1 = time()\n",
    "    word_en.append(word_embed(i[0]))\n",
    "    print(f'embedding: {time() - t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 300])\n",
      "544\n",
      "embedding: 10.82581877708435\n",
      "torch.Size([16, 300])\n",
      "544\n",
      "embedding: 13.51282787322998\n",
      "torch.Size([16, 300])\n",
      "544\n",
      "embedding: 13.09506106376648\n",
      "torch.Size([16, 300])\n",
      "544\n",
      "embedding: 12.933187246322632\n",
      "torch.Size([16, 300])\n",
      "544\n",
      "embedding: 12.992132186889648\n",
      "torch.Size([16, 300])\n",
      "544\n",
      "embedding: 13.033794641494751\n"
     ]
    }
   ],
   "source": [
    "tmp_all = overall_char_embedding((BS,300),'../LSTM-CRF-NER/char_vec_dictionary.txt',5,[4],True,True,1,[(3,135)],True,544,135)\n",
    "output_char_en = []\n",
    "for i in data:\n",
    "    t1 = time()\n",
    "    output_char_en.append(tmp_all(i[0]))\n",
    "    print(f'embedding: {time() - t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_attention = AttentionBetweenWordsAndChars(50,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_out = []\n",
    "for i in range(len(data)):\n",
    "    t1 = time()\n",
    "    att_out.append(my_attention(output_char_en[i], word_en[i]))\n",
    "    print(f'att_layer: {time() - t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_crf_layer = gru_crf(100, 2, 544, 0.1, True, {0:'I', 1:'B', 2:'O', 3:'<PAD>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(att_out)):\n",
    "    t1 = time()\n",
    "    print(gru_crf_layer(att_out[i], data[i][1], data[i][2]))\n",
    "    print(f'gru_crf: {time() - t1}')\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {0:'I', 1:'B', 2:'O', 3:'<PAD>'}\n",
    "all_transition=allowed_transitions('IOB1', tags)\n",
    "CRF = ConditionalRandomField(4, all_transition, include_start_end_transitions= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.transpose(x,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 384, 4])\n"
     ]
    }
   ],
   "source": [
    "print(x2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8493.3369, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRF(x2,y.long(),z)#torch.transpose(z,1,0).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 384, 4])\n",
      "torch.Size([16, 384])\n",
      "torch.Size([16, 384])\n"
     ]
    }
   ],
   "source": [
    "print(x2.size())\n",
    "print(y.size())\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    if z[i,500] == 1:\n",
    "        print('no pad found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_seq_len(MARK: '(torch.tensor: shape=(batch_size, num_words)) \\\n",
    "    of mask 1 for non padding, 0 for otherwise')->'(int) col index of first zero in\\\n",
    "    of the longest sequence example: x=torch.tensor([[1,1,0],[1,0,0]]) -> return 2':\n",
    "    r,c = MARK.size()\n",
    "    max_len = 0\n",
    "    prev_col = 1\n",
    "    for row, col in get_index(r,c):\n",
    "        if prev_col == 1 and MARK[row,col].item() == 2:\n",
    "            max_len = max(max_len, col)\n",
    "        prev_col = MARK[row,col].item()\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_longest_seq_len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "www=z.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(543,)\n"
     ]
    }
   ],
   "source": [
    "a = np.sum(www,0)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "for i in range(a.shape[0]):\n",
    "    if a[i] == 0:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16.\n",
      " 16. 16. 16. 16. 16. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_longest_seq_len2(MASK: '(torch.tensor: shape=(batch_size, num_words)) \\\n",
    "    of mask 1 for non padding, 0 for otherwise')->'(int) col index of first zero in\\\n",
    "    of the longest sequence example: x=torch.tensor([[1,1,0],[1,0,0]]) -> return 2':\n",
    "    tmp_mask = np.sum(MASK.numpy(),0)\n",
    "    col = 0\n",
    "    for i in range(tmp_mask.shape[0]):\n",
    "        if tmp_mask[i]==0:\n",
    "            col = i\n",
    "            break\n",
    "    if col == 0:\n",
    "        col = tmp_mask.shape[0]\n",
    "    return col, tmp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "col,v =get_longest_seq_len2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "allennlp2",
   "language": "python",
   "name": "allennlp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
