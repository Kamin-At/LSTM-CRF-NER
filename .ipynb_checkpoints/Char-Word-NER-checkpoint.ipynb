{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import regex as re\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from allennlp.modules.conditional_random_field import ConditionalRandomField\n",
    "from allennlp.modules.conditional_random_field import allowed_transitions\n",
    "from allennlp.modules.lstm_cell_with_projection import LstmCellWithProjection\n",
    "from allennlp.modules.input_variational_dropout import InputVariationalDropout\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "#from torchcrf import CRF\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import random\n",
    "from torchnlp.nn import WeightDropLSTM\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from typing import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from sklearn_crfsuite import metrics\n",
    "from torchnlp.nn import WeightDropGRU\n",
    "from RULE import RULEs\n",
    "from POSMap import POSMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataloader(Dataset):\n",
    "    def __init__(self, TextDir: '.txt extension of samples', LabelDir: '.txt extension of labels',rules:\\\n",
    "                 'the rules to be replaced => see in RULE.py', Len_word_vec: 'size of word vector', \\\n",
    "                delimiter: '(str) delimiter used to separate data', dir_char_dictionary: \\\n",
    "                '(str) see in CharEmbedding', max_len_char: '(int) see in CharEmbedding', \\\n",
    "                fasttext_dictionary_dir: '(str) see in WordEmbedding',\\\n",
    "                Len_embedded_vector: '(int) see in WordEmbedding', device, POSDir: '(str) .txt extension of POS',\\\n",
    "                POSMapping: 'see in POSMap.py') -> None:\n",
    "        super().__init__()\n",
    "        self.DF = pd.read_csv(TextDir, names=['text'])\n",
    "        self.Label_DF = pd.read_csv(LabelDir, names=['text'])\n",
    "        self.pos_DF = pd.read_csv(POSDir, names=['text'])\n",
    "        self.rules = rules\n",
    "        self.Len_word_vec = Len_word_vec\n",
    "        self.delimiter = delimiter\n",
    "        self.char_embedder = CharEmbedding(dir_char_dictionary, max_len_char)\n",
    "        self.word_embedder = WordEmbedding(fasttext_dictionary_dir, Len_embedded_vector)\n",
    "        self.device = device\n",
    "        self.pos_embedder = POSEmbedding(POSMapping)\n",
    "    def __len__(self):\n",
    "        return len(self.DF)\n",
    "    def __getitem__(self, Index) -> '(sample: (torch.tensor), label: (torch.tensor))':\n",
    "        all_words = [word.strip() for word in self.DF['text'][Index].strip().split(self.delimiter)]\n",
    "        for i in range(len(all_words)):\n",
    "            for rule in self.rules:\n",
    "                all_words[i] = re.sub(*rule, all_words[i])\n",
    "        Label = [float(word.strip()) for word in self.Label_DF['text'][Index].strip().split(self.delimiter)]\n",
    "        mask = [1.0]*len(all_words)\n",
    "        POS = [pos.strip() for pos in self.pos_DF['text'][Index].strip().split(self.delimiter)]\n",
    "        tmp_length = len(all_words)\n",
    "        if len(all_words) < self.Len_word_vec:\n",
    "            Label = Label + [3.0]*(self.Len_word_vec - len(all_words))\n",
    "            mask = mask + [0.0]*(self.Len_word_vec - len(all_words))\n",
    "            POS = POS + ['<pad>']*(self.Len_word_vec - len(all_words))\n",
    "            all_words = all_words + ['<pad>']*(self.Len_word_vec - len(all_words))\n",
    "        char_embed = self.char_embedder.embed(all_words)\n",
    "        word_embed = self.word_embedder.embed(all_words)\n",
    "        pos_embed = self.pos_embedder.embed(POS)\n",
    "        # print(len(all_words))\n",
    "        # print(len(Label))\n",
    "        # print(len(mask))\n",
    "        # print('----------')\n",
    "        return (char_embed.to(self.device), word_embed.to(self.device), \\\n",
    "                torch.tensor(Label).to(self.device), torch.tensor(mask).to(self.device), \\\n",
    "                tmp_length, pos_embed.float().to(self.device))\n",
    "    \n",
    "\n",
    "class CharEmbedding():\n",
    "    def __init__(self,\\\n",
    "    dir_char_dictionary: '(str) .txt',\\\n",
    "    max_len_char: '(int) max size of char representation, for example: given max_len_char=3 and word= \"abcde\" => only \"abc\" is used'):\n",
    "    #Example: given embed_capital=True and 'a' is embedded as array([1.,0.,0.,0.,0]). 'A' is then embedded as array([1.,0.,0.,0.,1.])\n",
    "        self.dictionary = {}\n",
    "        self.max_len_char = max_len_char\n",
    "        with open(dir_char_dictionary, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                tmp_data = line.strip().split()\n",
    "                self.dictionary[tmp_data[0]] = np.array([float(Char) for Char in tmp_data[1:]])\n",
    "    def embed(self, list_of_words: '(list[str]) example: [\"ฉัน\",\"กิน\",\"ข้าว\"]'):\n",
    "        #Note: 1 outer list is for 1 word.\n",
    "        output = []\n",
    "        for word in list_of_words:\n",
    "            embedded_word = []\n",
    "            tmp_word = word\n",
    "            if len(word) > self.max_len_char:\n",
    "                tmp_word = tmp_word[:self.max_len_char]\n",
    "            for Char in tmp_word:\n",
    "                if Char in self.dictionary:\n",
    "                    tmp_vector = self.dictionary[Char]\n",
    "                else:\n",
    "                    tmp_vector = np.zeros(self.dictionary['a'].shape)\n",
    "                embedded_word.append(tmp_vector)\n",
    "            if len(embedded_word) < self.max_len_char:\n",
    "                for i in range(self.max_len_char - len(embedded_word)):\n",
    "                    embedded_word.append(np.zeros(self.dictionary['a'].shape))\n",
    "            output.append(torch.tensor(embedded_word))\n",
    "        return torch.stack(output)\n",
    "\n",
    "class WordEmbedding():\n",
    "    #use fasttext embedding ==> read from a file\n",
    "    def __init__(self, fasttext_dictionary_dir: '(str) .vec extension of words and embedded_vectors',\\\n",
    "     Len_embedded_vector: '(int) size of embedded each vector (300 for fasttext) **Count only numbers not words'\\\n",
    "     ) -> None:\n",
    "        #example of format in fasttext_dictionary_dir\n",
    "        #กิน 1.0 -2.666 -3 22.5 .... \\n\n",
    "        #นอน 1.5 -5.666 3 9.5 .... \\n\n",
    "        #...\n",
    "        #...\n",
    "        self.dictionary = {}\n",
    "        self.Len_embedded_vector = Len_embedded_vector\n",
    "        with open(fasttext_dictionary_dir, 'r', encoding = 'utf8') as f:\n",
    "            for line in f:\n",
    "                tmp_line = line.strip()\n",
    "                tmp_words = tmp_line.split()\n",
    "                if tmp_line != '' and len(tmp_words) == self.Len_embedded_vector + 1:\n",
    "                    self.dictionary[tmp_words[0]] = np.array([float(element) for element in tmp_words[1:]])\n",
    "                else:\n",
    "                    continue\n",
    "    def embed(self, list_of_words: '(List[str]) for example: [\"ฉัน\",\"กิน\",\"ข้าว\"]'):\n",
    "        tmp_list = []\n",
    "        for word in list_of_words:\n",
    "            if word in self.dictionary:\n",
    "                tmp_list.append(self.dictionary[word])\n",
    "            else:\n",
    "                #in case of OOV: Zero-vector is used.\n",
    "                tmp_list.append(np.zeros(self.Len_embedded_vector))\n",
    "        return torch.tensor(tmp_list)\n",
    "\n",
    "class POSEmbedding():\n",
    "    def __init__(self, POSMapping: 'see in POSMap.py'):\n",
    "        self.dictionary = POSMapping\n",
    "        self.size = len(self.dictionary)\n",
    "    def embed(self, list_of_POSs:'(list[str]) example: [\"NOUN\",\"VERB\",\"NOUN\"]'):\n",
    "        tmp_list = []\n",
    "        for POS in list_of_POSs:\n",
    "            POS = POS.strip()\n",
    "            if POS == '<pad>':\n",
    "                tmp_list.append(np.zeros(self.size))\n",
    "            else:\n",
    "                tmp_data = np.zeros(self.size)\n",
    "                tmp_data[self.dictionary[POS]] = 1\n",
    "                tmp_list.append(tmp_data)\n",
    "        return torch.tensor(tmp_list)\n",
    "\n",
    "#new\n",
    "############### RNN encoding ######################\n",
    "# class CNN_char(nn.Module):\n",
    "#     def __init__(self, num_filter: '()'):\n",
    "\n",
    "#         class My2DConv(nn.Module):\n",
    "#     def __init__(self, num_filter: '(int) number of filters', use_BN: '(bool) if True, use 2d-batchnorm after linear conv',\\\n",
    "#                  activation_func: '(bool) if True, use RELU after BN', input_channel: '(int) number of input channels', \\\n",
    "#                  kernel_size: '(tuple): (width, height) size of the kernels', same_padding: '(bool) if True, input_w,input_h=output_w,output_h'):\n",
    "#         super().__init__()\n",
    "\n",
    "class RNN_char(nn.Module):\n",
    "    def __init__(self, num_char_vec_features, hidden_size, num_layers, dropout_gru, bidirectional, \\\n",
    "                output_size, dropout_FCN, num_word):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=num_char_vec_features, hidden_size=hidden_size, num_layers=num_layers,\\\n",
    "                          batch_first = True, dropout=dropout_gru, bidirectional=bidirectional)\n",
    "        self.linear = nn.Linear(hidden_size*2*num_layers, output_size)\n",
    "        self.BN = nn.BatchNorm1d(num_word)\n",
    "        self.dropout = nn.Dropout(dropout_FCN)\n",
    "        self.num_layers = num_layers\n",
    "    def forward(self, x):\n",
    "        batch_size, word_seq, char_seq, char_vec = x.size()\n",
    "        tmp_list = []\n",
    "        for i in range(word_seq):\n",
    "            tmp_compute , _ = self.gru(x[:,i,:,:].float())\n",
    "            tmp_list.append(tmp_compute.contiguous().view(batch_size,-1))\n",
    "        tmp_compute = torch.stack(tmp_list,1)\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_compute = self.dropout(tmp_compute)\n",
    "        tmp_compute = self.linear(tmp_compute)\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_compute = F.relu(self.BN(tmp_compute))#>>linear >> BachNorm >> relu\n",
    "        return tmp_compute\n",
    "    \n",
    "class over_all_NER2(nn.Module):\n",
    "    def __init__(self, Batch_size: '(int)',\\\n",
    "                 num_char_vec_features: '(int)',\\\n",
    "                 hidden_size: '(int)',\\\n",
    "                 max_num_char: '(int)',\\\n",
    "                 dropout_gru_char: '(double)',\\\n",
    "                 bidirectional_char: '(bool)',\\\n",
    "                 output_char_embed_size: '(int)',\\\n",
    "                 size_of_embedding: '(int) size of each word embedding vector',\\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', \\\n",
    "                 bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf', DO_FCN_GRUCRF: '(double)', DOchar_FCN: '(double)',\\\n",
    "                 pos_size: '(int) size of pos embedding'):\n",
    "        super().__init__()\n",
    "        self.gru_char = RNN_char(num_char_vec_features, hidden_size, max_num_char, dropout_gru_char, \\\n",
    "                                 bidirectional_char, output_char_embed_size, DOchar_FCN, num_words)\n",
    "        self.gru_crf_layer = gru_crf(size_of_embedding + output_char_embed_size + pos_size, \\\n",
    "                                     gru_hidden_size, num_words, dropout_gru, bidirectional, tags, DO_FCN_GRUCRF)\n",
    "    def forward(self, x):\n",
    "        tmp_compute = self.gru_char(x[0])\n",
    "        #print(tmp_compute.size())\n",
    "        #print(x[1].size())\n",
    "        tmp_compute = torch.cat([tmp_compute, x[1].float(), x[5]], 2)\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_gru_crf = self.gru_crf_layer((tmp_compute, x[4]), x[2], x[3].long())\n",
    "        return tmp_gru_crf\n",
    "    def predict(self, x):\n",
    "        tmp_compute = self.gru_char(x[0])\n",
    "        tmp_compute = torch.cat([tmp_compute, x[1].float(), x[5]], 2)\n",
    "        tmp_gru_crf = self.gru_crf_layer.predict((tmp_compute, x[4]), x[3].long())\n",
    "        return tmp_gru_crf\n",
    "\n",
    "class CNN_GRU_CRF(nn.Module):\n",
    "    def __init__(self, Batch_size: '(int)',\\\n",
    "                 max_num_char: '(int)',\\\n",
    "                 nums_filter: '(list[int] see in overall_char_embedding)',\\\n",
    "                 use_BN: '(bool) only for CNNchar',\\\n",
    "                 activation_func: '(bool) only for CNNchar',\\\n",
    "                 input_channel: '(int) see in My2DConv',\\\n",
    "                 kernel_sizes: '(list[int]) list of size of kernels used, and they will be computed concurrently',\\\n",
    "                 same_padding: '(bool) same padding for CNNchar',\\\n",
    "                 num_char_encoding_size: '(int) size of each char embedding vector',\\\n",
    "                 output_size: '(int) output dimension of CNNchar',\\\n",
    "                 size_of_embedding: '(int) size of each word embedding vector',\\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', \\\n",
    "                 bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf', DO_FCN_GRUCRF: '(double)',\\\n",
    "                 pos_size: '(int) size of pos embedding',\\\n",
    "                 FCN: '(bool) see overall_char_embedding',\\\n",
    "                 drop_weight):\n",
    "        super().__init__()\n",
    "        if not FCN:\n",
    "            output_size = num_char_encoding_size\n",
    "        #print(f'output_size: {output_size}')\n",
    "        self.overall_char_embedding = overall_char_embedding((Batch_size, output_size), max_num_char, \\\n",
    "                                                             nums_filter, use_BN, activation_func, \\\n",
    "                                                             input_channel, kernel_sizes, same_padding, \\\n",
    "                                                             num_words, num_char_encoding_size, FCN)\n",
    "\n",
    "        self.gru_crf_layer = gru_crf(size_of_embedding + output_size + pos_size, \\\n",
    "                                     gru_hidden_size, num_words, dropout_gru, bidirectional, tags, \\\n",
    "                                     DO_FCN_GRUCRF, drop_weight)\n",
    "    def forward(self, x):\n",
    "        tmp_compute = self.overall_char_embedding(x[0])\n",
    "        #print(tmp_compute.size())\n",
    "        #print(x[1].size())\n",
    "        tmp_compute = torch.cat([tmp_compute, x[1].float(), x[5]], 2)\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_gru_crf = self.gru_crf_layer((tmp_compute, x[4]), x[2], x[3].long())\n",
    "        return tmp_gru_crf\n",
    "    def predict(self, x):\n",
    "        tmp_compute = self.overall_char_embedding(x[0])\n",
    "        tmp_compute = torch.cat([tmp_compute, x[1].float(), x[5]], 2)\n",
    "        tmp_gru_crf = self.gru_crf_layer.predict((tmp_compute, x[4]), x[3].long())\n",
    "        return tmp_gru_crf\n",
    "\n",
    "class CNN_GRU_word_pos(nn.Module):\n",
    "    def __init__(self, Batch_size: '(int)',\\\n",
    "                 size_of_embedding: '(int) size of each word embedding vector',\\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', \\\n",
    "                 bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf', DO_FCN_GRUCRF: '(double)',\\\n",
    "                 pos_size: '(int) size of pos embedding',\\\n",
    "                 drop_GRU_out):\n",
    "        super().__init__()\n",
    "        #print(f'output_size: {output_size}')\n",
    "        self.gru_crf_layer = gru_crf(size_of_embedding + pos_size, \\\n",
    "                                     gru_hidden_size, num_words, dropout_gru, bidirectional, tags, \\\n",
    "                                     DO_FCN_GRUCRF, drop_GRU_out)\n",
    "    def forward(self, x):\n",
    "        tmp_compute = torch.cat([x[1].float(), x[5]], 2)\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_gru_crf = self.gru_crf_layer((tmp_compute, x[4]), x[2], x[3].long())\n",
    "        return tmp_gru_crf\n",
    "    def predict(self, x):\n",
    "        tmp_compute = torch.cat([x[1].float(), x[5]], 2)\n",
    "        tmp_gru_crf = self.gru_crf_layer.predict((tmp_compute, x[4]), x[3].long())\n",
    "        return tmp_gru_crf\n",
    "    \n",
    "class GRU_CRF_word(nn.Module):\n",
    "    def __init__(self, Batch_size: '(int)',\\\n",
    "                 size_of_embedding: '(int) size of each word embedding vector',\\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', \\\n",
    "                 bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf', DO_FCN_GRUCRF: '(double)'):\n",
    "        super().__init__()\n",
    "        self.gru_crf_layer = gru_crf(size_of_embedding , gru_hidden_size, num_words, \\\n",
    "                                     dropout_gru, bidirectional, tags, DO_FCN_GRUCRF)\n",
    "    def forward(self, x):\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_gru_crf = self.gru_crf_layer((x[1].float(), x[4]), x[2], x[3].long())\n",
    "        return tmp_gru_crf\n",
    "    def predict(self, x):\n",
    "        tmp_gru_crf = self.gru_crf_layer.predict((x[1].float(), x[4]), x[3].long())\n",
    "        return tmp_gru_crf\n",
    "\n",
    "class CNN_GRU_char(nn.Module):\n",
    "    def __init__(self, Batch_size: '(int)',\\\n",
    "                 max_num_char: '(int)',\\\n",
    "                 nums_filter: '(list[int] see in overall_char_embedding)',\n",
    "                 use_BN: '(bool) only for CNNchar',\n",
    "                 activation_func: '(bool) only for CNNchar',\n",
    "                 input_channel: '(int) see in My2DConv',\n",
    "                 kernel_sizes: '(list[int]) list of size of kernels used, and they will be computed concurrently',\n",
    "                 same_padding: '(bool) same padding for CNNchar',\n",
    "                 num_char_encoding_size: '(int) size of each char embedding vector',\\\n",
    "                 output_size: '(int) output dimension of CNNchar',\\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', \\\n",
    "                 bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf', DO_FCN_GRUCRF: '(double)',\\\n",
    "                 FCN: '(bool) see overall_char_embedding',\\\n",
    "                 DO_weight_gru: '(float) weight dropout'):\n",
    "        super().__init__()\n",
    "        if not FCN:\n",
    "            output_size = num_char_encoding_size\n",
    "        #print(f'output_size: {output_size}')\n",
    "        self.overall_char_embedding = overall_char_embedding((Batch_size, output_size), max_num_char, \\\n",
    "                                                             nums_filter, use_BN, activation_func, \\\n",
    "                                                             input_channel, kernel_sizes, same_padding, \\\n",
    "                                                             num_words, num_char_encoding_size, FCN)\n",
    "\n",
    "        self.gru_crf_layer = gru_crf(output_size, gru_hidden_size, num_words, dropout_gru, bidirectional, tags, \\\n",
    "                                     DO_FCN_GRUCRF, DO_weight_gru)\n",
    "    def forward(self, x):\n",
    "        tmp_compute = self.overall_char_embedding(x[0])\n",
    "        tmp_gru_crf = self.gru_crf_layer((tmp_compute, x[4]), x[2], x[3].long())\n",
    "        return tmp_gru_crf\n",
    "    def predict(self, x):\n",
    "        tmp_compute = self.overall_char_embedding(x[0])\n",
    "        tmp_gru_crf = self.gru_crf_layer.predict((tmp_compute, x[4]), x[3].long())\n",
    "        return tmp_gru_crf\n",
    "\n",
    "class CNN_GRU_char_pos(nn.Module):\n",
    "    def __init__(self, Batch_size: '(int)',\\\n",
    "                 max_num_char: '(int)',\\\n",
    "                 nums_filter: '(list[int] see in overall_char_embedding)',\\\n",
    "                 use_BN: '(bool) only for CNNchar',\\\n",
    "                 activation_func: '(bool) only for CNNchar',\\\n",
    "                 input_channel: '(int) see in My2DConv',\\\n",
    "                 kernel_sizes: '(list[int]) list of size of kernels used, and they will be computed concurrently',\\\n",
    "                 same_padding: '(bool) same padding for CNNchar',\\\n",
    "                 num_char_encoding_size: '(int) size of each char embedding vector',\\\n",
    "                 output_size: '(int) output dimension of CNNchar',\\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', \\\n",
    "                 bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf', \\\n",
    "                 DO_FCN_GRUCRF: '(double)', \\\n",
    "                 pos_size: '(int) size of pos embedding', \\\n",
    "                 FCN: '(bool) see overall_char_embedding',\\\n",
    "                 drop_weight):\n",
    "        super().__init__()\n",
    "        if not FCN:\n",
    "            output_size = num_char_encoding_size\n",
    "        #print(f'output_size: {output_size}')\n",
    "        self.overall_char_embedding = overall_char_embedding((Batch_size, output_size), max_num_char, \\\n",
    "                                                             nums_filter, use_BN, activation_func, \\\n",
    "                                                             input_channel, kernel_sizes, same_padding, \\\n",
    "                                                             num_words, num_char_encoding_size, FCN)\n",
    "\n",
    "        self.gru_crf_layer = gru_crf(output_size + pos_size, \\\n",
    "                                     gru_hidden_size, num_words, dropout_gru, bidirectional, tags, \\\n",
    "                                     DO_FCN_GRUCRF, drop_weight)\n",
    "    def forward(self, x):\n",
    "        tmp_compute = self.overall_char_embedding(x[0])\n",
    "        #print(tmp_compute.size())\n",
    "        #print(x[1].size())\n",
    "        tmp_compute = torch.cat([tmp_compute, x[5]], 2)\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_gru_crf = self.gru_crf_layer((tmp_compute, x[4]), x[2], x[3].long())\n",
    "        return tmp_gru_crf\n",
    "    def predict(self, x):\n",
    "        tmp_compute = self.overall_char_embedding(x[0])\n",
    "        tmp_compute = torch.cat([tmp_compute, x[5]], 2)\n",
    "        tmp_gru_crf = self.gru_crf_layer.predict((tmp_compute, x[4]), x[3].long())\n",
    "        return tmp_gru_crf\n",
    "\n",
    "\n",
    "#new\n",
    "def get_index(len_row, len_col)->'(iterator of all ((int)row, (int)col))':\n",
    "    for i in range(len_row):\n",
    "        for j in range(len_col):\n",
    "            yield(i,j)\n",
    "\n",
    "def get_longest_seq_len(MASK: '(torch.tensor: shape=(batch_size, num_words)) \\\n",
    "    of mask 1 for non padding, 0 for otherwise')->'(int) col index of first zero in\\\n",
    "    of the longest sequence example: x=torch.tensor([[1,1,0],[1,0,0]]) -> return 2':\n",
    "    tmp_mask = MASK.numpy()\n",
    "    if len(tmp_mask.shape) != 1:\n",
    "        tmp_mask = np.sum(tmp_mask,0)\n",
    "    col = 0\n",
    "    for i in range(tmp_mask.shape[0]):\n",
    "        if tmp_mask[i]==0:\n",
    "            col = i\n",
    "            break\n",
    "    if col == 0:\n",
    "        col = tmp_mask.shape[0]\n",
    "    return col\n",
    "\n",
    "class overall_char_embedding(nn.Module):\n",
    "    def __init__(self, output_size: '(tuple of ints): (batch_size, embedding_size_per_word)',\\\n",
    "    max_len_char: '(int) see in CharEmbedding',\\\n",
    "    nums_filter: '(list) list of number of filters according to each kernel_sizes (respectively)',\\\n",
    "    use_BN: 'see in My2DConv',\\\n",
    "    activation_func: 'see in My2DConv',\\\n",
    "    input_channel: 'see in My2DConv',\\\n",
    "    kernel_sizes: '(list[int]) list of size of kernels used, and they will be computed concurrently',\\\n",
    "    same_padding: 'see in My2DConv',\\\n",
    "    num_words: 'number of words used in 1 sample',\\\n",
    "    num_char_encoding_size: 'size of encoding for each char',\\\n",
    "    FCN: '(bool) use FCN after CNN or not'):\n",
    "        super().__init__()\n",
    "        self.batch_size, self.embedding_size_per_word = output_size\n",
    "        tmp_cnn_models = []\n",
    "        for ind_cnn, kernel_size in enumerate(kernel_sizes):\n",
    "            tmp_cnn_models.append(\\\n",
    "            My2DConvChar(nums_filter[ind_cnn], use_BN, activation_func, input_channel,\\\n",
    "            (kernel_size, 1), same_padding)\n",
    "            )\n",
    "        self.num_words = num_words\n",
    "        self.CNNs = nn.ModuleList(tmp_cnn_models)\n",
    "        self.MyMaxPool = nn.MaxPool2d((max_len_char, 1), stride= (1,1))\n",
    "        self.FCN = FCN\n",
    "        if self.FCN:\n",
    "            self.MyFCN = nn.Linear(sum(nums_filter)*num_char_encoding_size, output_size[1])\n",
    "            self.BN = nn.BatchNorm1d(output_size[1])\n",
    "    def forward(self, x):\n",
    "        batch_size, num_word, num_char, embedding_size = x.size()\n",
    "        #print(x.size())\n",
    "        tmp_compute = x.view(batch_size, num_word, 1, num_char, \\\n",
    "        embedding_size)\n",
    "        all_output_list = []\n",
    "        for num_word in range(self.num_words):\n",
    "            tmp_output_cnn = []\n",
    "            for tmp_cnn in self.CNNs:\n",
    "                tmp_output_cnn.append(self.MyMaxPool(tmp_cnn(tmp_compute[:,\\\n",
    "                num_word,:,:,:])).view((batch_size, -1)))\n",
    "            tmp = torch.cat(tmp_output_cnn, 1)\n",
    "            #print(tmp.size())\n",
    "            if self.FCN:\n",
    "                all_output_list.append(F.relu(self.BN(self.MyFCN(tmp))))\n",
    "            else:\n",
    "                all_output_list.append(tmp)\n",
    "        #print(all_output_list[0].size())\n",
    "        #print(len(all_output_list))\n",
    "        all_output_list = torch.stack(all_output_list, dim=1)\n",
    "        #print(all_output_list.size())\n",
    "        return all_output_list\n",
    "                \n",
    "class gru_crf(nn.Module):\n",
    "    def __init__(self, num_input_features: '(int) number of input features', hidden_size: '(int) number of\\\n",
    "    hidden features the outputs will also have hidden_size features', num_layers: '(int) number of \\\n",
    "    recursion', dropout_gru, bidirectional: '(bool) if True, use bidirectional GRU',\\\n",
    "    tags: \"(dict[int: str])example: {0:'I', 1:'B', 2:'O', 3:'<PAD>'}\", dropout_FCN: '(double)', drop_GRU_out):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size = num_input_features, hidden_size = hidden_size, \\\n",
    "                                  num_layers = num_layers, batch_first = True, dropout = dropout_gru, \\\n",
    "                                  bidirectional = bidirectional)\n",
    "        #self.gru = WeightDropGRU(input_size = num_input_features, hidden_size = hidden_size, \\\n",
    "        #                         num_layers = num_layers, batch_first = True, dropout = dropout_gru, \\\n",
    "        #                         bidirectional = bidirectional, weight_dropout=drop_weight)\n",
    "        all_transition=allowed_transitions('BIO', tags)\n",
    "        #self.crf = CRF(num_tags=len(tags), batch_first= True)\n",
    "        self.linear = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.BN = nn.BatchNorm1d(num_layers)\n",
    "        self.linear2 = nn.Linear(hidden_size, len(tags))\n",
    "        self.BN2 = nn.BatchNorm1d(num_layers)\n",
    "        self.crf = ConditionalRandomField(len(tags), all_transition)\n",
    "        self.dropout = dropout_FCN\n",
    "        self.drop_GRU_out = drop_GRU_out\n",
    "        \n",
    "    def forward(self, samples, target: '(torch.tensor) shape=(...............,)the target tags to be used',\\\n",
    "                mask: 'True for non-pad elements'):\n",
    "        length = samples[1]\n",
    "        samples = samples[0]\n",
    "        batch_size, words, _ = samples.size()\n",
    "        tmp_t = time()\n",
    "        #print(samples.size())\n",
    "        tmp_compute = F.dropout(self.gru(samples)[0], p=self.dropout)\n",
    "        #print('pass inference gru')\n",
    "        tmp_compute = tmp_compute.view(batch_size, words, -1)\n",
    "        #print('pass reshape gru')\n",
    "#         print(f'total GRU time: {time() - tmp_t}')\n",
    "        index_to_cut = max(length).item()#get_longest_seq_len(mask)\n",
    "        #length = torch.mean(length.float()).item()\n",
    "        ##############################################\n",
    "        ###cut padding some parts out#################\n",
    "        #print(tmp_compute.size())\n",
    "        #tmp_compute = self.dropout(tmp_compute)\n",
    "        tmp_compute = F.dropout(F.relu(self.BN(self.linear(tmp_compute))), p=self.drop_GRU_out)\n",
    "        tmp_compute = F.relu(self.BN2(self.linear2(tmp_compute)))\n",
    "        tmp_compute = F.dropout(tmp_compute[:, :index_to_cut,:],  p=self.dropout)\n",
    "        target = target[:, :index_to_cut]\n",
    "        mask = mask[:, :index_to_cut]\n",
    "        #print(tmp_compute.size())\n",
    "        nll_loss = self.crf(tmp_compute,target.long(),mask)\n",
    "#         print(f'total CRF time: {time() - tmp_t}')\n",
    "        return nll_loss#/length\n",
    "    def predict(self, samples, mask):\n",
    "        length = samples[1]\n",
    "        samples = samples[0]\n",
    "        batch_size, words, _ = samples.size()\n",
    "        tmp_t = time()\n",
    "        tmp_compute = self.gru(samples)[0].view(batch_size, words, -1)\n",
    "#         print(f'total GRU time: {time() - tmp_t}')\n",
    "        index_to_cut = max(length).item()#get_longest_seq_len(mask)\n",
    "        ##############################################\n",
    "        ###cut padding some parts out#################\n",
    "        #print(tmp_compute.size())\n",
    "        \n",
    "        tmp_compute = F.relu(self.BN(self.linear(tmp_compute)))\n",
    "        tmp_compute = F.relu(self.BN2(self.linear2(tmp_compute)))\n",
    "        tmp_compute = tmp_compute[:, :index_to_cut,:]\n",
    "        mask = mask[:, :index_to_cut]\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_t = time()\n",
    "        tmp_tags = self.crf.viterbi_tags(tmp_compute,mask)\n",
    "#         print(f'total CRF prediction time: {time() - tmp_t}')\n",
    "        return tmp_tags\n",
    "    \n",
    "class My2DConv(nn.Module):\n",
    "    def __init__(self, num_filter: '(int) number of filters', use_BN: '(bool) if True, use 2d-batchnorm after linear conv',\\\n",
    "    activation_func: '(bool) if True, use RELU after BN', input_channel: '(int) number of input channels', \\\n",
    "    kernel_size: '(tuple): (width, height) size of the kernels', same_padding: '(bool) if True, input_w,input_h=output_w,output_h'):\n",
    "        super().__init__()\n",
    "        if same_padding:\n",
    "            #assume that dialation = 1 and stride = 1\n",
    "            self.padding = (math.floor((kernel_size[0] - 1)/2), math.floor((kernel_size[1] -1)/2))\n",
    "        else:\n",
    "            self.padding = 0\n",
    "        self.Conv = nn.Conv2d(input_channel, num_filter, kernel_size, padding= self.padding)\n",
    "        self.use_BN = use_BN\n",
    "        self.activation_func = activation_func\n",
    "        if self.use_BN:\n",
    "            self.BN = nn.BatchNorm2d(num_filter)\n",
    "\n",
    "    def forward(self, input_data: '(torch.tensor) dimension= (batch_size, num_channel_in, in_height, in_width)') \\\n",
    "    -> '(torch.tensor) shape= (batch_size, num_filter, in_height, in_width)':\n",
    "        tmp_compute = self.Conv(input_data.float())\n",
    "        if self.use_BN:\n",
    "            tmp_compute = self.BN(tmp_compute)\n",
    "        if self.activation_func:\n",
    "            tmp_compute = nn.ReLU()(tmp_compute)\n",
    "        return tmp_compute\n",
    "        \n",
    "class My2DConvChar(nn.Module):\n",
    "    def __init__(self, num_filter: '(int) number of filters', use_BN: '(bool) if True, \\\n",
    "                 use 2d-batchnorm after linear conv', activation_func: '(bool) if True, use RELU\\\n",
    "                 after BN', input_channel: '(int) number of input channels', \\\n",
    "                 kernel_size: '(tuple): (width, height) size of the kernels', \\\n",
    "                 same_padding: '(bool) if True, input_w,input_h=output_w,output_h'):\n",
    "        super().__init__()\n",
    "        if same_padding:\n",
    "            #assume that dialation = 1 and stride = 1\n",
    "            self.padding = (math.floor((kernel_size[0] - 1)/2), 0)\n",
    "        else:\n",
    "            self.padding = 0\n",
    "        self.Conv = nn.Conv2d(input_channel, num_filter, kernel_size, padding= self.padding)\n",
    "        self.use_BN = use_BN\n",
    "        self.activation_func = activation_func\n",
    "        if self.use_BN:\n",
    "            self.BN = nn.BatchNorm2d(num_filter)\n",
    "\n",
    "    def forward(self, input_data: '(torch.tensor) dimension= (batch_size, num_channel_in, in_height, in_width)') \\\n",
    "    -> '(torch.tensor) shape= (batch_size, num_filter, in_height, in_width)':\n",
    "        tmp_compute = self.Conv(input_data.float())\n",
    "        if self.use_BN:\n",
    "            tmp_compute = self.BN(tmp_compute)\n",
    "        if self.activation_func:\n",
    "            tmp_compute = F.relu(tmp_compute)\n",
    "        return tmp_compute\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class AttentionBetweenWordsAndChars(nn.Module):\n",
    "    def __init__(self, hidden_size: '(int) size of key, query and value vectors',\\\n",
    "    input_vec_size: '(int) incase of fasttext input_vec_size=300'):\n",
    "        super().__init__()\n",
    "        self.K_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.Q_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.V_FCN = nn.Linear(input_vec_size, hidden_size)\n",
    "        self.AttLayer = ScaledDotProductAttention(math.sqrt(hidden_size), 0.1)\n",
    "    def forward(self, char_vectors, word_vectors):\n",
    "        batch_size, word_size, _ = word_vectors.size()\n",
    "        word_vectors = word_vectors.float()\n",
    "        char_vectors = char_vectors.float()\n",
    "#         print(word_vectors.size())\n",
    "#         print(char_vectors.size())\n",
    "        K = torch.stack([self.K_FCN(word_vectors),self.K_FCN(char_vectors)],dim = 2)\n",
    "        Q = torch.stack([self.Q_FCN(word_vectors),self.Q_FCN(char_vectors)],dim = 2)\n",
    "        V = torch.stack([self.V_FCN(word_vectors),self.V_FCN(char_vectors)],dim = 2)\n",
    "        all_output_list = []\n",
    "        for word_ind in range(word_size):\n",
    "            all_output_list.append(self.AttLayer(Q[:,word_ind,:,:], \\\n",
    "            K[:,word_ind,:,:], V[:,word_ind,:,:])[0].view(batch_size,-1))\n",
    "\n",
    "        return torch.stack(all_output_list,dim = 1)\n",
    "    \n",
    "class over_all_NER(nn.Module):\n",
    "    def __init__(self, Batch_size: '(int)',\\\n",
    "                 size_of_embedding: '(int) size of each word embedding vector',\\\n",
    "                 max_len_char: '(int) see overall_char_embedding',\\\n",
    "                 num_conv_filters: '(list[int]) see in overall_char_embedding', \\\n",
    "                 use_BN: '(bool) see in overall_char_embedding', \\\n",
    "                 use_activation: '(bool) see in overall_char_embedding', \\\n",
    "                 num_conv_input_channel: '(int) see in overall_char_embedding', \\\n",
    "                 kernel_sizes: '(list[tuple[int, int]]) see in overall_char_embedding', \\\n",
    "                 use_same_padding: '(bool) see in overall_char_embedding', \\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 num_char_encoding_size: '(int) see in overall_char_embedding', \\\n",
    "                 att_hidden_size: '(int) see in AttentionBetweenWordsAndChars', \\\n",
    "                 num_input_features: '(int) see in gru_crf', gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf'):\n",
    "        super().__init__()\n",
    "        self.char_embed = overall_char_embedding((Batch_size,size_of_embedding), max_len_char, num_conv_filters, \\\n",
    "                                                 use_BN, use_activation, num_conv_input_channel, kernel_sizes, \\\n",
    "                                                 use_same_padding, num_words, num_char_encoding_size)\n",
    "        self.my_attention = AttentionBetweenWordsAndChars(att_hidden_size, size_of_embedding)\n",
    "        self.gru_crf_layer = gru_crf(num_input_features, gru_hidden_size, num_words, dropout_gru, \\\n",
    "                                bidirectional, tags)\n",
    "        self.Batch_size = Batch_size\n",
    "    def forward(self, x):\n",
    "        tmp_compute = self.char_embed(x[0])\n",
    "        tmp_att = self.my_attention(tmp_compute, x[1])\n",
    "        tmp_gru_crf = self.gru_crf_layer(tmp_att, x[2], x[3].long())\n",
    "        return tmp_gru_crf#/self.Batch_size\n",
    "    def predict(self, x):\n",
    "        tmp_compute = self.char_embed(x[0])\n",
    "        tmp_att = self.my_attention(tmp_compute, x[1])\n",
    "        tmp_tags = self.gru_crf_layer.predict(tmp_att, x[3].long())\n",
    "        return tmp_tags\n",
    "\n",
    "def get_indices_random_train_test_split(dataset_size:'(int) number of rows', random_seed: '(int)',\\\n",
    "                                        validation_split: '(double)', shuffle_dataset: '(bool)'):\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "    return train_sampler, valid_sampler\n",
    "  \n",
    "def get_indices_random_val_test_split(dataset_size:'(int) number of rows', random_seed: '(int)',\\\n",
    "                                        validation_split: '(double)', shuffle_dataset: '(bool)'):\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    test_indices, val_indices = indices[split: 2*split], indices[:split]\n",
    "    # Creating PT data samplers and loaders:\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "    return test_sampler, valid_sampler\n",
    "\n",
    "def eval_score(tags: '(dict[int: str])', pred: '(list[(list, float)])', label: 'torch.tensor'):\n",
    "    pred = np.array([np.array(i[0]) for i in pred])\n",
    "    label = label.cpu().numpy().astype('int8')\n",
    "    label = [label[i][:len(pred[i])] for i in range(len(pred))]\n",
    "    conf_mat = np.zeros((len(tags), len(tags)))\n",
    "#     print(len(label))\n",
    "#     print(len(pred))\n",
    "#     print('---------------')\n",
    "    for i in range(len(label)):\n",
    "#         print(len(label[i]))\n",
    "#         print(len(pred[i]))\n",
    "        conf_mat += confusion_matrix(label[i],pred[i],range(len(tags)))\n",
    "    performance_mat = np.zeros((len(tags), 3))#recall, precision, f1-score\n",
    "    for i in range(len(tags)):\n",
    "        if np.sum(conf_mat[i]) == 0:\n",
    "            performance_mat[i][0] = 0\n",
    "        else:\n",
    "            performance_mat[i][0] = conf_mat[i][i]/np.sum(conf_mat[i])\n",
    "        if np.sum(conf_mat[:,i]) == 0:\n",
    "            performance_mat[i][1] = 0\n",
    "        else:\n",
    "            performance_mat[i][1] = conf_mat[i][i]/np.sum(conf_mat[:,i])\n",
    "        if performance_mat[i][1]+performance_mat[i][0] == 0:\n",
    "            performance_mat[i][2] = 0\n",
    "        else:\n",
    "            performance_mat[i][2] = (2*performance_mat[i][0]*performance_mat[i][1])/(performance_mat[i][1]+performance_mat[i][0])\n",
    "    return performance_mat, conf_mat[:,:-1]\n",
    "\n",
    "class CNN_GRU_char_pos(nn.Module):\n",
    "    def __init__(self, Batch_size: '(int)',\\\n",
    "                 max_num_char: '(int)',\\\n",
    "                 nums_filter: '(list[int] see in overall_char_embedding)',\\\n",
    "                 use_BN: '(bool) only for CNNchar',\\\n",
    "                 activation_func: '(bool) only for CNNchar',\\\n",
    "                 input_channel: '(int) see in My2DConv',\\\n",
    "                 kernel_sizes: '(list[int]) list of size of kernels used, and they will be computed concurrently',\\\n",
    "                 same_padding: '(bool) same padding for CNNchar',\\\n",
    "                 num_char_encoding_size: '(int) size of each char embedding vector',\\\n",
    "                 output_size: '(int) output dimension of CNNchar',\\\n",
    "                 num_words: '(int) see in overall_char_embedding', \\\n",
    "                 gru_hidden_size: '(int) see in gru_crf', \\\n",
    "                 dropout_gru: '(double) see in gru_crf', \\\n",
    "                 bidirectional: '(bool)', \\\n",
    "                 tags: '(dict[int: str]) see in gru_crf', \\\n",
    "                 DO_FCN_GRUCRF: '(double)', \\\n",
    "                 pos_size: '(int) size of pos embedding', \\\n",
    "                 FCN: '(bool) see overall_char_embedding', \\\n",
    "                 drop_weight\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if not FCN:\n",
    "            output_size = num_char_encoding_size\n",
    "        #print(f'output_size: {output_size}')\n",
    "        self.overall_char_embedding = overall_char_embedding((Batch_size, output_size), max_num_char, \\\n",
    "                                                             nums_filter, use_BN, activation_func, \\\n",
    "                                                             input_channel, kernel_sizes, same_padding, \\\n",
    "                                                             num_words, num_char_encoding_size, FCN)\n",
    "\n",
    "        self.gru_crf_layer = gru_crf(output_size + pos_size, \\\n",
    "                                     gru_hidden_size, num_words, dropout_gru, bidirectional, tags, \\\n",
    "                                     DO_FCN_GRUCRF, drop_weight)\n",
    "    def forward(self, x):\n",
    "        tmp_compute = self.overall_char_embedding(x[0])\n",
    "        #print(tmp_compute.size())\n",
    "        #print(x[1].size())\n",
    "        tmp_compute = torch.cat([tmp_compute, x[5]], 2)\n",
    "        #print(tmp_compute.size())\n",
    "        tmp_gru_crf = self.gru_crf_layer((tmp_compute, x[4]), x[2], x[3].long())\n",
    "        return tmp_gru_crf\n",
    "    def predict(self, x):\n",
    "        tmp_compute = self.overall_char_embedding(x[0])\n",
    "        tmp_compute = torch.cat([tmp_compute, x[5]], 2)\n",
    "        tmp_gru_crf = self.gru_crf_layer.predict((tmp_compute, x[4]), x[3].long())\n",
    "        return tmp_gru_crf\n",
    "\n",
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for ind, tmp in enumerate(named_parameters):\n",
    "        n, p= tmp\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cur_ind: 0. gru_weight_dropout: 0.20635630823301318\n",
      "grucrf_dropout: 0.24840664118118352, DO_FCN_GRUCRF: 0.3266306490218036\n",
      "grucrf_hidden_size: 128, LR: 0.0002994937337779445\n",
      "cpu\n",
      "epoch 0\n",
      "0\n",
      "0.00018978118896484375\n"
     ]
    }
   ],
   "source": [
    "BS = 4\n",
    "tags = {0:'I', 1:'B', 2:'O', 3:'<pad>'}\n",
    "scheduler_n = 2000\n",
    "word_length = 2000\n",
    "early_stop_n = 100\n",
    "max_size_char = 6\n",
    "num_search = 1000\n",
    "filename = 'aaaaaaaaa'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "data = MyDataloader('../clean84withpos.txt', '../label84withpos.txt', RULEs, \\\n",
    "                    word_length, '|', 'char_vec_dictionary.txt',max_size_char, \\\n",
    "                    '../fasttext.th.vec', 300, device, '../pos_tag84withpos.txt',POSMAP)\n",
    "\n",
    "tr, te = get_indices_random_val_test_split(len(data), 1, 0.0005, True)\n",
    "train_loader = DataLoader(data, batch_size=BS, sampler=tr)\n",
    "test_loader = DataLoader(data, batch_size=BS, sampler=te)\n",
    "\n",
    "# NER = over_all_NER(BS,300, max_size_char, num_kernels,True,True,1,kernel_sizes,\\\n",
    "#                    True,word_length,135,attention_in, attention_out, gru_hidden_size, \\\n",
    "#                    gru_dropout, True, tags)\n",
    "#####\n",
    "# Batch_size: '(int)',\\\n",
    "#                  num_char_vec_features: '(int)',\\\n",
    "#                  hidden_size: '(int)',\\\n",
    "#                  max_num_char: '(int)',\\\n",
    "#                  dropout_gru_char: '(double)',\\\n",
    "#                  bidirectional_char: '(bool)',\\\n",
    "#                  output_char_embed_size: '(int)',\\\n",
    "#                  size_of_embedding: '(int) size of each word embedding vector',\\\n",
    "#                  num_words: '(int) see in overall_char_embedding', \\\n",
    "#                  gru_hidden_size: '(int) see in gru_crf', \\\n",
    "#                  dropout_gru: '(double) see in gru_crf', \\\n",
    "#                  bidirectional: '(bool)', \\\n",
    "#                  tags: '(dict[int: str]) see in gru_crf', DO_FCN_GRUCRF: '(double)', DOchar_FCN: '(double)')\n",
    "#####\n",
    "for cur_ind in range(num_search):\n",
    "    torch.cuda.empty_cache()\n",
    "    grucrf_dropout = random.uniform(0.2,0.5)#0.5\n",
    "    DO_FCN_GRUCRF = random.uniform(0.2,0.5)#0.5\n",
    "    grucrf_hidden_size = 128#random.choice([128, 256])#5\n",
    "    gru_weight_dropout = random.uniform(0.1,0.4)\n",
    "    LR = 5*10**random.uniform(-3,-5)#0.001\n",
    "    \n",
    "    print(f'cur_ind: {cur_ind}. gru_weight_dropout: {gru_weight_dropout}')\n",
    "    print(f'grucrf_dropout: {grucrf_dropout}, DO_FCN_GRUCRF: {DO_FCN_GRUCRF}')\n",
    "    print(f'grucrf_hidden_size: {grucrf_hidden_size}, LR: {LR}')\n",
    "\n",
    "    NER = CNN_GRU_CRF(BS, max_size_char, [1], True, True, 1, \\\n",
    "                      [3], True, 135, 135,\\\n",
    "                      300, word_length, grucrf_hidden_size, grucrf_dropout, \\\n",
    "                      True, tags, DO_FCN_GRUCRF, len(POSMAP), False)\n",
    "\n",
    "    optimizer = optim.Adam(NER.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=True)\n",
    "    my_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "\n",
    "    print(device)\n",
    "    NER.to(device)\n",
    "    best_score = 0\n",
    "    best_mat = np.zeros((len(tags)-1,3))\n",
    "    cnt_idle = 0\n",
    "    for epoch in range(10):\n",
    "        print(f'epoch {epoch}')\n",
    "        all_loss = []\n",
    "        for ind, batch_x in enumerate(train_loader):\n",
    "            if ind%5 == 0:\n",
    "                print(ind)\n",
    "            t2 = time()\n",
    "            NER = NER.train()\n",
    "            print(time() - t2)\n",
    "            NER.zero_grad()\n",
    "            t1 = time()\n",
    "            loss = NER(batch_x)\n",
    "            loss = loss*(-1)\n",
    "            print(f'time per batch: {time() - t1}')\n",
    "            print(loss)\n",
    "            all_loss.append(loss)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_value_(NER.parameters(), 10)\n",
    "            optimizer.step()\n",
    "        total_loss = sum(all_loss)/(ind + 1)\n",
    "        my_scheduler.step(total_loss)\n",
    "        print(f'total loss of epoch: {total_loss.item()}')\n",
    "        print('testing')\n",
    "        per_mat = np.zeros((len(tags), 3))\n",
    "        for ind, batch_test in enumerate(test_loader):\n",
    "            NER = NER.eval()\n",
    "            output = NER.predict(batch_test)\n",
    "            per_mat += eval_score(tags, output, batch_test[2])\n",
    "        per_mat = per_mat/(ind+1)\n",
    "        per_mat = per_mat[:len(tags),:]\n",
    "        print(per_mat)\n",
    "        score = sum(per_mat[:,2])/(len(tags)-1)\n",
    "        if best_score < score:\n",
    "            best_mat=per_mat\n",
    "            best_score = score\n",
    "            cnt_idle = 0\n",
    "        else:\n",
    "            cnt_idle += 1\n",
    "        print(f'overall score: {score}')\n",
    "        print('--------------------')\n",
    "        if early_stop_n == cnt_idle:\n",
    "            break\n",
    "    break\n",
    "\n",
    "    print(f'best_score: {best_score}\\n')\n",
    "    #print(f'best_mat\\n')\n",
    "    print(f'I => recall: {best_mat[0,0]}, precision: {best_mat[0,1]}, , f1: {best_mat[0,2]}\\n')\n",
    "    print(f'B => recall: {best_mat[1,0]}, precision: {best_mat[1,1]}, , f1: {best_mat[1,2]}\\n')\n",
    "    print(f'O => recall: {best_mat[2,0]}, precision: {best_mat[2,1]}, , f1: {best_mat[2,2]}\\n')\n",
    "    #print(f'best_mat\\n')\n",
    "    print(f'----------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "LE: 0.0019529859275205712\n",
      "dropouti: 0, DO_FCN_LSTMCRF: 0.41411433791047636\n",
      "lstmcrf_hidden_size: 128, dropouti: 0\n",
      "dropouto: 0, dropoutw: 0.3811934675043389\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CNN_LSTM_CRF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3bdc1bebf111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     NER = CNN_LSTM_CRF(BS, max_size_char, nums_filter, use_BN, activation_func, input_channel, \\\n\u001b[0m\u001b[1;32m     67\u001b[0m                        \u001b[0mkernel_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msame_padding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_char_encoding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                        \u001b[0msize_of_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstmcrf_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CNN_LSTM_CRF' is not defined"
     ]
    }
   ],
   "source": [
    "BS = 2\n",
    "tags = {0:'I', 1:'B', 2:'O', 3:'<pad>'}\n",
    "scheduler_n = 50\n",
    "word_length = 84\n",
    "early_stop_n = 5\n",
    "max_size_char = 20\n",
    "num_search = 100\n",
    "nums_filter = [1]\n",
    "use_BN = True\n",
    "activation_func = True\n",
    "input_channel = 1\n",
    "kernel_sizes = [3]\n",
    "same_padding = True\n",
    "num_char_encoding_size = 135\n",
    "output_size = 64\n",
    "size_of_embedding = 300\n",
    "pos_size = len(POSMAP)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "data = MyDataloader('../clean84withpos.txt', '../label84withpos.txt', RULEs, \\\n",
    "                    word_length, '|', 'char_vec_dictionary.txt',max_size_char, \\\n",
    "                    '../fasttext.th.vec', 300, device, '../pos_tag84withpos.txt',POSMAP)\n",
    "\n",
    "\n",
    "tr, te = get_indices_random_val_test_split(len(data), 1, 0.00005, True)\n",
    "train_loader = DataLoader(data, batch_size=BS, sampler=tr)\n",
    "test_loader = DataLoader(data, batch_size=BS, sampler=te)\n",
    "\n",
    "# NER = over_all_NER(BS,300, max_size_char, num_kernels,True,True,1,kernel_sizes,\\\n",
    "#                    True,word_length,135,attention_in, attention_out, gru_hidden_size, \\\n",
    "#                    gru_dropout, True, tags)\n",
    "#####\n",
    "# Batch_size: '(int)',\\\n",
    "#                  num_char_vec_features: '(int)',\\\n",
    "#                  hidden_size: '(int)',\\\n",
    "#                  max_num_char: '(int)',\\\n",
    "#                  dropout_gru_char: '(double)',\\\n",
    "#                  bidirectional_char: '(bool)',\\\n",
    "#                  output_char_embed_size: '(int)',\\\n",
    "#                  size_of_embedding: '(int) size of each word embedding vector',\\\n",
    "#                  num_words: '(int) see in overall_char_embedding', \\\n",
    "#                  gru_hidden_size: '(int) see in gru_crf', \\\n",
    "#                  dropout_gru: '(double) see in gru_crf', \\\n",
    "#                  bidirectional: '(bool)', \\\n",
    "#                  tags: '(dict[int: str]) see in gru_crf', DO_FCN_GRUCRF: '(double)', DOchar_FCN: '(double)')\n",
    "#####\n",
    "for cur_ind in range(num_search):\n",
    "    torch.cuda.empty_cache()\n",
    "    #grucrf_dropout = random.uniform(0.3,0.7)#0.5\n",
    "    DO_FCN_LSTMCRF = random.uniform(0.3,0.7)#0.5\n",
    "    \n",
    "    dropouti = 0#random.uniform(0.1,0.7)\n",
    "    dropouto = 0#random.uniform(0.1,0.7)\n",
    "    dropoutw = random.uniform(0.1,0.7)\n",
    "    \n",
    "    lstmcrf_hidden_size = random.choice([128])#5\n",
    "    LR = 5*10**random.uniform(-3,-5)#0.001\n",
    "    print(f'LE: {LR}')\n",
    "    print(f'dropouti: {dropouti}, DO_FCN_LSTMCRF: {DO_FCN_LSTMCRF}')\n",
    "    print(f'lstmcrf_hidden_size: {lstmcrf_hidden_size}, dropouti: {dropouti}')\n",
    "    print(f'dropouto: {dropouto}, dropoutw: {dropoutw}')\n",
    "\n",
    "    \n",
    "    NER = CNN_LSTM_CRF(BS, max_size_char, nums_filter, use_BN, activation_func, input_channel, \\\n",
    "                       kernel_sizes, same_padding, num_char_encoding_size, output_size,\\\n",
    "                       size_of_embedding, word_length, lstmcrf_hidden_size, \\\n",
    "                       True, tags, DO_FCN_LSTMCRF, pos_size, False, dropouti=dropouti, \\\n",
    "                       dropouto=dropouto, dropoutw=dropoutw)\n",
    "#self, Batch_size, max_num_char, nums_filter, use_BN, activation_func, input_channel, \n",
    "#kernel_sizes, same_padding, num_char_encoding_size, output_size, size_of_embedding, \n",
    "#num_words, gru_hidden_size, dropout_gru, bidirectional, tags, DO_FCN_GRUCRF, pos_size, FCN, dropouti, \n",
    "#dropoutw, dropouto\n",
    "\n",
    "    optimizer = optim.Adam(NER.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=True)\n",
    "    my_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "\n",
    "    print(device)\n",
    "    NER.to(device)\n",
    "    best_score = 0\n",
    "    best_mat = np.zeros((len(tags)-1,3))\n",
    "    cnt_idle = 0\n",
    "    for epoch in range(6):\n",
    "        print(f'epoch {epoch}')\n",
    "        all_loss = []\n",
    "        for ind, batch_x in enumerate(train_loader):\n",
    "            if ind%5 == 0:\n",
    "                print(ind)\n",
    "            t2 = time()\n",
    "            print('------------train--------------------')\n",
    "            NER = NER.train()\n",
    "            print(time() - t2)\n",
    "            NER.zero_grad()\n",
    "            t1 = time()\n",
    "            loss = NER(batch_x)\n",
    "            output = NER.predict(batch_x)\n",
    "#             for i in range(len(output)):\n",
    "#                 print(batch_x[2][i])\n",
    "#                 print(output[i])\n",
    "            loss = loss*(-1)\n",
    "            print(f'time per batch: {time() - t1}')\n",
    "            print(loss)\n",
    "            all_loss.append(loss)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_value_(NER.parameters(), 1)\n",
    "            plot_grad_flow(NER.named_parameters())\n",
    "            optimizer.step()\n",
    "        total_loss = sum(all_loss)/(ind + 1)\n",
    "        my_scheduler.step(total_loss)\n",
    "        print(f'total loss of epoch: {total_loss.item()}')\n",
    "        print('testing')\n",
    "        per_mat = np.zeros((len(tags), 3))\n",
    "        for ind, batch_test in enumerate(test_loader):\n",
    "            NER = NER.eval()\n",
    "            output = NER.predict(batch_test)\n",
    "            for i in range(len(output)):\n",
    "                print(batch_test[2][i])\n",
    "                print(output[i])\n",
    "            per_mat += eval_score(tags, output, batch_test[2])\n",
    "        per_mat = per_mat/(ind+1)\n",
    "        per_mat = per_mat[:len(tags),:]\n",
    "        print(per_mat)\n",
    "        score = sum(per_mat[:,2])/(len(tags)-1)\n",
    "        if best_score < score:\n",
    "            best_mat=per_mat\n",
    "            best_score = score\n",
    "            cnt_idle = 0\n",
    "        else:\n",
    "            cnt_idle += 1\n",
    "        print(f'overall score: {score}')\n",
    "        print('--------------------')\n",
    "        if early_stop_n == cnt_idle:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment1 word with POS only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "lstmcrf_dropout = DO_FCN_LSTMCRF: 0\n",
      "lstmcrf_hidden_size: 128, LR: 0.0001\n",
      "cpu\n",
      "epoch 0\n",
      "progress: 10.0\n",
      "0\n",
      "8.606910705566406e-05\n",
      "time per batch: 4.677277088165283\n",
      "tensor(232.3398, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.200241088867188e-05\n",
      "time per batch: 2.9562060832977295\n",
      "tensor(114.2868, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "9.489059448242188e-05\n",
      "time per batch: 5.48121190071106\n",
      "tensor(98.3271, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.176399230957031e-05\n",
      "time per batch: 3.318082094192505\n",
      "tensor(220.4478, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.033348083496094e-05\n",
      "time per batch: 3.408078193664551\n",
      "tensor(133.0703, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "5\n",
      "0.00011396408081054688\n",
      "time per batch: 3.599493980407715\n",
      "tensor(109.1365, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "0.00012493133544921875\n",
      "time per batch: 3.8385379314422607\n",
      "tensor(139.8772, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.486343383789062e-05\n",
      "time per batch: 3.985848903656006\n",
      "tensor(138.5245, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.987022399902344e-05\n",
      "time per batch: 4.679396867752075\n",
      "tensor(165.7661, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.700920104980469e-05\n",
      "time per batch: 2.6822638511657715\n",
      "tensor(82.7229, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "10\n",
      "8.0108642578125e-05\n",
      "time per batch: 4.505958080291748\n",
      "tensor(95.9642, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.104873657226562e-05\n",
      "time per batch: 5.238005876541138\n",
      "tensor(82.5719, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "0.00013303756713867188\n",
      "time per batch: 2.7869207859039307\n",
      "tensor(89.8979, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.081031799316406e-05\n",
      "time per batch: 6.1256492137908936\n",
      "tensor(132.2179, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "8.296966552734375e-05\n",
      "time per batch: 2.779116153717041\n",
      "tensor(84.7692, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "15\n",
      "7.605552673339844e-05\n",
      "time per batch: 2.7542598247528076\n",
      "tensor(132.9417, grad_fn=<MulBackward>)\n",
      "progress: 10.0\n",
      "7.700920104980469e-05\n",
      "time per batch: 3.1005301475524902\n",
      "tensor(108.3036, grad_fn=<MulBackward>)\n"
     ]
    }
   ],
   "source": [
    "stop_sign = False\n",
    "time_list1 = []\n",
    "for IND in range(2):\n",
    "    BS = 4\n",
    "    tags = {0:'I', 1:'B', 2:'O', 3:'<pad>'}\n",
    "    scheduler_n = 1000\n",
    "    word_length = 84\n",
    "    early_stop_n = 1000\n",
    "    max_size_char = 6\n",
    "    num_search = 1000\n",
    "    num_epoch = 30\n",
    "    pos_size = len(POSMAP)\n",
    "    \n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    data_tr = MyDataloader('../Data/clean84withpos_ne_tr'+ str(IND) +'.txt', '../Data/label84withpos_ne_tr'+ str(IND) +'.txt',\\\n",
    "                           RULEs, word_length, '|', 'char_vec_dictionary.txt',max_size_char, \\\n",
    "                           '../fasttext.th.vec', 300, device, '../Data/pos_tag84withpos_ne_tr'+ str(IND) +'.txt',POSMAP)\n",
    "    data_te = MyDataloader('../Data/clean84withpos_ne_te'+ str(IND) +'.txt', '../Data/label84withpos_ne_te'+ str(IND) +'.txt', \\\n",
    "                           RULEs, word_length, '|', 'char_vec_dictionary.txt',max_size_char, \\\n",
    "                           '../fasttext.th.vec', 300, device, '../Data/pos_tag84withpos_ne_te'+ str(IND) +'.txt',POSMAP)\n",
    "\n",
    "    train_loader = DataLoader(data_tr, batch_size=BS, shuffle= True)\n",
    "    test_loader = DataLoader(data_te, batch_size=BS, shuffle= True)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    grucrf_dropout = [0, 0.15, 0.30, 0.45, 0.60]#random.uniform(0.2,0.5)#0.5#random.uniform(0.2,0.5)#0.5\n",
    "    total_search = len(grucrf_dropout)*2\n",
    "    for i in grucrf_dropout:\n",
    "        grucrf_hidden_size = 128#random.choice([128])#5\n",
    "        LR = 10**(-4)#**random.uniform(-4,-5)#0.001\n",
    "        print(f'lstmcrf_dropout = DO_FCN_LSTMCRF: {i}')\n",
    "        print(f'lstmcrf_hidden_size: {grucrf_hidden_size}, LR: {LR}')\n",
    "\n",
    "        NER = CNN_GRU_word_pos(BS, 300, word_length, grucrf_hidden_size, i, True, tags, \\\n",
    "                               i, pos_size, 0.5)\n",
    "\n",
    "        optimizer = optim.Adam(NER.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=True)\n",
    "        my_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "\n",
    "        print(device)\n",
    "        NER.to(device)\n",
    "        best_score = 0\n",
    "        best_mat = np.zeros((len(tags)-1,3))\n",
    "        cnt_idle = 0\n",
    "        for epoch in range(num_epoch):\n",
    "            ttt = time()\n",
    "            print(f'epoch {epoch}')\n",
    "            all_loss = []\n",
    "            for ind, batch_x in enumerate(train_loader):\n",
    "                print(f'progress: {(100*(grucrf_dropout.index(i)+1)*(IND+1))/total_search}')\n",
    "                if ind%5 == 0:\n",
    "                    print(ind)\n",
    "                t2 = time()\n",
    "                NER = NER.train()\n",
    "                print(time() - t2)\n",
    "                NER.zero_grad()\n",
    "                t1 = time()\n",
    "                loss = NER(batch_x)\n",
    "                loss = loss*(-1)\n",
    "                print(f'time per batch: {time() - t1}')\n",
    "                print(loss)\n",
    "                all_loss.append(loss)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(NER.parameters(), 5, norm_type=2)\n",
    "                optimizer.step()\n",
    "            total_loss = sum(all_loss)/(ind + 1)\n",
    "            my_scheduler.step(total_loss)\n",
    "            time_list1.append(time()-ttt)\n",
    "            print(f'total loss of epoch: {total_loss.item()}')\n",
    "            print('testing')\n",
    "            per_mat = np.zeros((len(tags), 3))\n",
    "            cnt_mat = np.zeros((len(tags), 3))\n",
    "            for ind, batch_test in enumerate(test_loader):\n",
    "                NER = NER.eval()\n",
    "                output = NER.predict(batch_test)\n",
    "                a, b= eval_score(tags, output, batch_test[2])\n",
    "                per_mat += a\n",
    "                cnt_mat += b\n",
    "            per_mat = per_mat/(ind+1)\n",
    "            per_mat = per_mat[:len(tags),:]\n",
    "            cnt_mat = cnt_mat[:len(tags),:]\n",
    "            print(cnt_mat)\n",
    "            print(per_mat)\n",
    "            score = sum(per_mat[:,2])/(len(tags)-1)\n",
    "            if best_score < score:\n",
    "                best_mat=per_mat\n",
    "                best_score = score\n",
    "                cnt_idle = 0\n",
    "            else:\n",
    "                cnt_idle += 1\n",
    "            print(f'overall score: {score}')\n",
    "            print('--------------------')\n",
    "            if early_stop_n == cnt_idle:\n",
    "                break\n",
    "            print(f'total epoch time: {ttt-time()}')\n",
    "        print(f'best_score: {best_score}\\n')\n",
    "        print(f'I => recall: {best_mat[0,0]}, precision: {best_mat[0,1]}, , f1: {best_mat[0,2]}\\n')\n",
    "        print(f'B => recall: {best_mat[1,0]}, precision: {best_mat[1,1]}, , f1: {best_mat[1,2]}\\n')\n",
    "        print(f'O => recall: {best_mat[2,0]}, precision: {best_mat[2,1]}, , f1: {best_mat[2,2]}\\n')\n",
    "        print(f'----------------------------------\\n')\n",
    "        break\n",
    "    break\n",
    "print('end!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 3. 0.]\n",
      " [0. 5. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 0.  0.  0.]\n",
      " [ 0. 11.  0.]\n",
      " [ 0. 71.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(cnt_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment2 char with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "lstmcrf_dropout = DO_FCN_LSTMCRF: 0.6\n",
      "lstmcrf_hidden_size: 128, LR: 0.001\n",
      "cpu\n",
      "epoch 0\n",
      "progress: 50.0\n",
      "0\n",
      "0.0002052783966064453\n",
      "time per batch: 5.20810604095459\n",
      "tensor(59.1326, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0007011890411376953\n",
      "time per batch: 2.9966540336608887\n",
      "tensor(53.1094, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00013184547424316406\n",
      "time per batch: 3.234434127807617\n",
      "tensor(75.2873, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0001232624053955078\n",
      "time per batch: 2.9993391036987305\n",
      "tensor(55.4365, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00022482872009277344\n",
      "time per batch: 4.342714071273804\n",
      "tensor(57.7877, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "5\n",
      "0.00013589859008789062\n",
      "time per batch: 3.8009440898895264\n",
      "tensor(49.8162, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00013589859008789062\n",
      "time per batch: 3.500293731689453\n",
      "tensor(14.8113, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 52.19728469848633\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  61.]\n",
      " [  0.   0. 474.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.88117539 0.93648788]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3121626276769655\n",
      "--------------------\n",
      "total epoch time: -142.43686079978943\n",
      "epoch 1\n",
      "progress: 50.0\n",
      "0\n",
      "0.00016188621520996094\n",
      "time per batch: 2.48603892326355\n",
      "tensor(33.1794, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0001380443572998047\n",
      "time per batch: 2.7804360389709473\n",
      "tensor(67.1847, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00014090538024902344\n",
      "time per batch: 2.759397268295288\n",
      "tensor(51.0178, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0004639625549316406\n",
      "time per batch: 4.337448835372925\n",
      "tensor(46.8856, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0001499652862548828\n",
      "time per batch: 5.653831958770752\n",
      "tensor(50.5877, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "5\n",
      "0.00013685226440429688\n",
      "time per batch: 2.9110748767852783\n",
      "tensor(55.1220, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0001289844512939453\n",
      "time per batch: 3.6093649864196777\n",
      "tensor(44.7356, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 49.81609344482422\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  61.]\n",
      " [  0.   0. 474.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.8793247  0.93529058]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3117635269315931\n",
      "--------------------\n",
      "total epoch time: -190.73345375061035\n",
      "epoch 2\n",
      "progress: 50.0\n",
      "0\n",
      "0.00021791458129882812\n",
      "time per batch: 3.2793760299682617\n",
      "tensor(46.5068, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0001239776611328125\n",
      "time per batch: 3.2710161209106445\n",
      "tensor(48.2323, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0001556873321533203\n",
      "time per batch: 3.985711097717285\n",
      "tensor(62.5986, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00012087821960449219\n",
      "time per batch: 3.13805890083313\n",
      "tensor(58.2884, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.0001227855682373047\n",
      "time per batch: 2.804314136505127\n",
      "tensor(60.5859, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "5\n",
      "0.00012111663818359375\n",
      "time per batch: 2.733598232269287\n",
      "tensor(31.2391, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00013494491577148438\n",
      "time per batch: 2.925503969192505\n",
      "tensor(34.3126, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 48.82338333129883\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  61.]\n",
      " [  0.   0. 474.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.86748648 0.92736268]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.30912089400338033\n",
      "--------------------\n",
      "total epoch time: -246.13183307647705\n",
      "epoch 3\n",
      "progress: 50.0\n",
      "0\n",
      "0.000125885009765625\n",
      "time per batch: 2.7143898010253906\n",
      "tensor(46.4696, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00013875961303710938\n",
      "time per batch: 2.5556352138519287\n",
      "tensor(55.2853, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00012087821960449219\n",
      "time per batch: 2.6401548385620117\n",
      "tensor(52.6389, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00013899803161621094\n",
      "time per batch: 2.505537986755371\n",
      "tensor(57.6012, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00018310546875\n",
      "time per batch: 4.122006893157959\n",
      "tensor(62.1025, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "5\n",
      "0.00019025802612304688\n",
      "time per batch: 5.89029598236084\n",
      "tensor(32.2638, grad_fn=<MulBackward>)\n",
      "progress: 50.0\n",
      "0.00012612342834472656\n",
      "time per batch: 2.778158187866211\n",
      "tensor(27.8071, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 47.73833465576172\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  61.]\n",
      " [  0.   0. 474.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.87005706 0.92846515]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.30948838357834185\n",
      "--------------------\n",
      "best_score: 0.3121626276769655\n",
      "\n",
      "I => recall: 0.0, precision: 0.0, , f1: 0.0\n",
      "\n",
      "B => recall: 0.0, precision: 0.0, , f1: 0.0\n",
      "\n",
      "O => recall: 1.0, precision: 0.8811753947132889, , f1: 0.9364878830308966\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "cpu\n",
      "lstmcrf_dropout = DO_FCN_LSTMCRF: 0.6\n",
      "lstmcrf_hidden_size: 128, LR: 0.001\n",
      "cpu\n",
      "epoch 0\n",
      "progress: 100.0\n",
      "0\n",
      "0.00018310546875\n",
      "time per batch: 4.770009994506836\n",
      "tensor(56.4801, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.0013527870178222656\n",
      "time per batch: 3.858466148376465\n",
      "tensor(56.5960, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.0001361370086669922\n",
      "time per batch: 2.8363828659057617\n",
      "tensor(37.8827, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00012373924255371094\n",
      "time per batch: 2.7834181785583496\n",
      "tensor(56.0355, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00014090538024902344\n",
      "time per batch: 3.492841958999634\n",
      "tensor(32.9111, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "5\n",
      "0.00013184547424316406\n",
      "time per batch: 3.238128900527954\n",
      "tensor(86.9578, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00012373924255371094\n",
      "time per batch: 3.1632280349731445\n",
      "tensor(18.1764, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 49.29137420654297\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   2.  60.]\n",
      " [  0.   0. 377.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.03956044 0.28571429 0.06802721]\n",
      " [1.         0.85354408 0.92007346]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.32936689116961426\n",
      "--------------------\n",
      "total epoch time: -137.49906516075134\n",
      "epoch 1\n",
      "progress: 100.0\n",
      "0\n",
      "0.00013518333435058594\n",
      "time per batch: 2.5132222175598145\n",
      "tensor(56.4514, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00012183189392089844\n",
      "time per batch: 2.7796738147735596\n",
      "tensor(50.5375, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00013518333435058594\n",
      "time per batch: 2.7302310466766357\n",
      "tensor(52.2519, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.0001308917999267578\n",
      "time per batch: 3.523908853530884\n",
      "tensor(62.1255, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.0002288818359375\n",
      "time per batch: 2.81697416305542\n",
      "tensor(25.3673, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "5\n",
      "0.00013399124145507812\n",
      "time per batch: 2.8015758991241455\n",
      "tensor(35.8351, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.0001468658447265625\n",
      "time per batch: 2.6864659786224365\n",
      "tensor(40.1155, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 46.09772872924805\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   2.  60.]\n",
      " [  0.   0. 377.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.03139717 0.28571429 0.05612245]\n",
      " [1.         0.84343361 0.91335   ]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3231574840374651\n",
      "--------------------\n",
      "total epoch time: -158.0967559814453\n",
      "epoch 2\n",
      "progress: 100.0\n",
      "0\n",
      "0.00019788742065429688\n",
      "time per batch: 2.62337589263916\n",
      "tensor(47.9705, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.0001468658447265625\n",
      "time per batch: 2.7564170360565186\n",
      "tensor(45.8600, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00012922286987304688\n",
      "time per batch: 2.6643431186676025\n",
      "tensor(65.9548, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00013017654418945312\n",
      "time per batch: 3.0164520740509033\n",
      "tensor(36.8948, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00013399124145507812\n",
      "time per batch: 2.7807250022888184\n",
      "tensor(48.2354, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "5\n",
      "0.00012612342834472656\n",
      "time per batch: 3.4930481910705566\n",
      "tensor(37.8559, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.0001430511474609375\n",
      "time per batch: 2.752403974533081\n",
      "tensor(36.9582, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 45.6756706237793\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   2.  60.]\n",
      " [  0.   0. 377.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.02619048 0.28571429 0.04795205]\n",
      " [1.         0.82834509 0.90335711]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3171030514998092\n",
      "--------------------\n",
      "total epoch time: -226.19259786605835\n",
      "epoch 3\n",
      "progress: 100.0\n",
      "0\n",
      "0.000125885009765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time per batch: 2.602128028869629\n",
      "tensor(29.8180, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00013494491577148438\n",
      "time per batch: 2.609605073928833\n",
      "tensor(56.0588, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00013113021850585938\n",
      "time per batch: 2.6166231632232666\n",
      "tensor(42.8358, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00013113021850585938\n",
      "time per batch: 2.6153769493103027\n",
      "tensor(57.6423, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.0008261203765869141\n",
      "time per batch: 4.7230987548828125\n",
      "tensor(52.6968, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "5\n",
      "0.0001399517059326172\n",
      "time per batch: 3.8388478755950928\n",
      "tensor(65.8388, grad_fn=<MulBackward>)\n",
      "progress: 100.0\n",
      "0.00012993812561035156\n",
      "time per batch: 3.721648931503296\n",
      "tensor(12.0223, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 45.273250579833984\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   2.  60.]\n",
      " [  0.   0. 377.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.03826531 0.28571429 0.06746032]\n",
      " [1.         0.83223605 0.90684076]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.32476702553428893\n",
      "--------------------\n",
      "best_score: 0.32936689116961426\n",
      "\n",
      "I => recall: 0.0, precision: 0.0, , f1: 0.0\n",
      "\n",
      "B => recall: 0.039560439560439566, precision: 0.2857142857142857, , f1: 0.06802721088435375\n",
      "\n",
      "O => recall: 1.0, precision: 0.8535440837175476, , f1: 0.9200734626244891\n",
      "\n",
      "----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 30\n",
    "time_list2 = []\n",
    "\n",
    "for IND in range(2):\n",
    "    \n",
    "    BS = 4\n",
    "    tags = {0:'I', 1:'B', 2:'O', 3:'<pad>'}\n",
    "    scheduler_n = 2\n",
    "    word_length = 84\n",
    "    early_stop_n = 3\n",
    "    max_size_char = 6#[5]#[5, 10, 20]\n",
    "    nums_filter = [1]\n",
    "    use_BN = True\n",
    "    activation_func = True\n",
    "    input_channel = 1\n",
    "    kernel_sizes = [3]\n",
    "    same_padding = True\n",
    "    num_char_encoding_size = 135\n",
    "    output_size = 64\n",
    "    size_of_embedding = 300\n",
    "    pos_size = len(POSMAP)\n",
    "    FCN = False\n",
    "    grucrf_dropout = [0.6]#[0, 0.15, 0.30, 0.45, 0.60]\n",
    "    total_search = len(max_size_char)*len(grucrf_dropout)*2\n",
    "    for size_char in max_size_char:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(device)\n",
    "\n",
    "        data_tr = MyDataloader('../Data/clean84withpos_ne_tr'+ str(IND) +'.txt', '../Data/label84withpos_ne_tr'+ str(IND) +'.txt',\\\n",
    "                               RULEs, word_length, '|', 'char_vec_dictionary.txt', size_char, \\\n",
    "                               '../fasttext.th.vec', 300, device, '../Data/pos_tag84withpos_ne_tr'+ str(IND) +'.txt',POSMAP)\n",
    "        data_te = MyDataloader('../Data/clean84withpos_ne_te'+ str(IND) +'.txt', '../Data/label84withpos_ne_te'+ str(IND) +'.txt', \\\n",
    "                               RULEs, word_length, '|', 'char_vec_dictionary.txt', size_char, \\\n",
    "                               '../fasttext.th.vec', 300, device, '../Data/pos_tag84withpos_ne_te'+ str(IND) +'.txt',POSMAP)\n",
    "\n",
    "#         train_loader = DataLoader(data_tr, batch_size=BS, shuffle= True)\n",
    "#         test_loader = DataLoader(data_te, batch_size=BS, shuffle= True)\n",
    "        tr, te = get_indices_random_val_test_split(len(data_tr), 1, 0.0005, True)\n",
    "        train_loader = DataLoader(data_tr, batch_size=BS, sampler=tr)\n",
    "        test_loader = DataLoader(data_tr, batch_size=BS, sampler=te)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        for i in grucrf_dropout:\n",
    "            \n",
    "            grucrf_hidden_size = 128\n",
    "            LR = 10**(-3)\n",
    "            print(f'lstmcrf_dropout = DO_FCN_LSTMCRF: {i}')\n",
    "            print(f'lstmcrf_hidden_size: {grucrf_hidden_size}, LR: {LR}')\n",
    "\n",
    "#             NER = CNN_GRU_char_pos(BS, size_char, nums_filter, use_BN, activation_func, input_channel, \\\n",
    "#                  kernel_sizes, same_padding, num_char_encoding_size, output_size, word_length, grucrf_hidden_size, \\\n",
    "#                  i, True, tags, i, pos_size, FCN, 0.5)\n",
    "\n",
    "            optimizer = optim.Adam(NER.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=True)\n",
    "            my_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "\n",
    "            print(device)\n",
    "            NER.to(device)\n",
    "            best_score = 0\n",
    "            best_mat = np.zeros((len(tags)-1,3))\n",
    "            cnt_idle = 0\n",
    "            for epoch in range(num_epoch):\n",
    "                ttt = time()\n",
    "                print(f'epoch {epoch}')\n",
    "                all_loss = []\n",
    "                for ind, batch_x in enumerate(train_loader):\n",
    "                    print(f'progress: {(100*(grucrf_dropout.index(i)+1)*(max_size_char.index(size_char)+1)*(IND+1))/total_search}')\n",
    "                    if ind%5 == 0:\n",
    "                        print(ind)\n",
    "                    t2 = time()\n",
    "                    NER = NER.train()\n",
    "                    print(time() - t2)\n",
    "                    NER.zero_grad()\n",
    "                    t1 = time()\n",
    "                    loss = NER(batch_x)\n",
    "                    loss = loss*(-1)\n",
    "                    print(f'time per batch: {time() - t1}')\n",
    "                    print(loss)\n",
    "                    all_loss.append(loss)\n",
    "                    loss.backward()\n",
    "                    #nn.utils.clip_grad_norm_(NER.parameters(), 5, norm_type=2)\n",
    "                    optimizer.step()\n",
    "                total_loss = sum(all_loss)/(ind + 1)\n",
    "                my_scheduler.step(total_loss)\n",
    "                time_list2.append(time()-ttt)\n",
    "                print(f'total loss of epoch: {total_loss.item()}')\n",
    "                print('testing')\n",
    "                per_mat = np.zeros((len(tags), 3))\n",
    "                cnt_mat = np.zeros((len(tags), 3))\n",
    "                for ind, batch_test in enumerate(test_loader):\n",
    "                    NER = NER.eval()\n",
    "                    output = NER.predict(batch_test)\n",
    "                    a, b = eval_score(tags, output, batch_test[2])\n",
    "                    per_mat += a\n",
    "                    cnt_mat += b\n",
    "                per_mat = per_mat/(ind+1)\n",
    "                per_mat = per_mat[:len(tags),:]\n",
    "                cnt_mat = cnt_mat[:len(tags),:]\n",
    "                print(cnt_mat)\n",
    "                print(per_mat)\n",
    "                score = sum(per_mat[:,2])/(len(tags)-1)\n",
    "                if best_score < score:\n",
    "                    best_mat=per_mat\n",
    "                    best_score = score\n",
    "                    cnt_idle = 0\n",
    "                else:\n",
    "                    cnt_idle += 1\n",
    "                print(f'overall score: {score}')\n",
    "                print('--------------------')\n",
    "                if early_stop_n == cnt_idle:\n",
    "                    break\n",
    "                print(f'total epoch time: {ttt-time()}')\n",
    "            break\n",
    "            print(f'best_score: {best_score}\\n')\n",
    "            print(f'I => recall: {best_mat[0,0]}, precision: {best_mat[0,1]}, , f1: {best_mat[0,2]}\\n')\n",
    "            print(f'B => recall: {best_mat[1,0]}, precision: {best_mat[1,1]}, , f1: {best_mat[1,2]}\\n')\n",
    "            print(f'O => recall: {best_mat[2,0]}, precision: {best_mat[2,1]}, , f1: {best_mat[2,2]}\\n')\n",
    "            print(f'----------------------------------\\n')\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment3 char, word, POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "progress: 3.3333333333333335\n",
      "lstmcrf_dropout = DO_FCN_LSTMCRF: 0\n",
      "lstmcrf_hidden_size: 128, LR: 0.0001\n",
      "cpu\n",
      "epoch 0\n",
      "0\n",
      "0.0010480880737304688\n",
      "time per batch: 3.1795029640197754\n",
      "tensor(75.7545, grad_fn=<MulBackward>)\n",
      "0.0001239776611328125\n",
      "time per batch: 2.4459939002990723\n",
      "tensor(102.2093, grad_fn=<MulBackward>)\n",
      "0.00012421607971191406\n",
      "time per batch: 3.7459418773651123\n",
      "tensor(117.1745, grad_fn=<MulBackward>)\n",
      "0.000125885009765625\n",
      "time per batch: 4.9954681396484375\n",
      "tensor(138.2445, grad_fn=<MulBackward>)\n",
      "0.00012683868408203125\n",
      "time per batch: 3.0653300285339355\n",
      "tensor(63.6602, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00012731552124023438\n",
      "time per batch: 2.627847909927368\n",
      "tensor(67.9566, grad_fn=<MulBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ac4a5102abe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mall_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epoch = 30\n",
    "time_list3 = []\n",
    "for IND in range(2):\n",
    "    BS = 4\n",
    "    tags = {0:'I', 1:'B', 2:'O', 3:'<pad>'}\n",
    "    scheduler_n = 100002\n",
    "    word_length = 84\n",
    "    early_stop_n = 100003\n",
    "    max_size_char = 6#[5, 10, 20]\n",
    "    nums_filter = [1]\n",
    "    use_BN = True\n",
    "    activation_func = True\n",
    "    input_channel = 1\n",
    "    kernel_sizes = [3]\n",
    "    same_padding = True\n",
    "    num_char_encoding_size = 135\n",
    "    output_size = 64\n",
    "    size_of_embedding = 300\n",
    "    pos_size = len(POSMAP)\n",
    "    FCN = False\n",
    "    grucrf_dropout = [0, 0.15, 0.30, 0.45, 0.60]\n",
    "    total_search = len(max_size_char)*len(grucrf_dropout)*2\n",
    "    for size_char in max_size_char:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(device)\n",
    "\n",
    "        data_tr = MyDataloader('../Data/clean84withpos_ne_tr'+ str(IND) +'.txt', '../Data/label84withpos_ne_tr'+ str(IND) +'.txt',\\\n",
    "                               RULEs, word_length, '|', 'char_vec_dictionary.txt', size_char, \\\n",
    "                               '../fasttext.th.vec', 300, device, '../Data/pos_tag84withpos_ne_tr'+ str(IND) +'.txt',POSMAP)\n",
    "        data_te = MyDataloader('../Data/clean84withpos_ne_te'+ str(IND) +'.txt', '../Data/label84withpos_ne_te'+ str(IND) +'.txt', \\\n",
    "                               RULEs, word_length, '|', 'char_vec_dictionary.txt', size_char, \\\n",
    "                               '../fasttext.th.vec', 300, device, '../Data/pos_tag84withpos_ne_te'+ str(IND) +'.txt',POSMAP)\n",
    "\n",
    "        train_loader = DataLoader(data_tr, batch_size=BS, shuffle= True)\n",
    "        test_loader = DataLoader(data_te, batch_size=BS, shuffle= True)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in grucrf_dropout:\n",
    "            grucrf_hidden_size = 128\n",
    "            LR = 10**(-4)\n",
    "            print(f'lstmcrf_dropout = DO_FCN_LSTMCRF: {i}')\n",
    "            print(f'lstmcrf_hidden_size: {grucrf_hidden_size}, LR: {LR}')\n",
    "\n",
    "            NER = CNN_GRU_CRF(BS, size_char, nums_filter, use_BN, activation_func, \\\n",
    "                              input_channel, kernel_sizes, same_padding, num_char_encoding_size, \\\n",
    "                              output_size, size_of_embedding, word_length, grucrf_hidden_size, i, \\\n",
    "                              True, tags, i, pos_size, FCN)\n",
    "\n",
    "            optimizer = optim.Adam(NER.parameters(), lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4, amsgrad=True)\n",
    "            my_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')\n",
    "\n",
    "            print(device)\n",
    "            NER.to(device)\n",
    "            best_score = 0\n",
    "            best_mat = np.zeros((len(tags)-1,3))\n",
    "            cnt_idle = 0\n",
    "            for epoch in range(num_epoch):\n",
    "                ttt = time()\n",
    "                print(f'epoch {epoch}')\n",
    "                all_loss = []\n",
    "                for ind, batch_x in enumerate(train_loader):\n",
    "                    print(f'progress: {(100*(grucrf_dropout.index(i)+1)*(max_size_char.index(size_char)+1)*(IND+1))/total_search}')\n",
    "                    if ind%5 == 0:\n",
    "                        print(ind)\n",
    "                    t2 = time()\n",
    "                    NER = NER.train()\n",
    "                    print(time() - t2)\n",
    "                    NER.zero_grad()\n",
    "                    t1 = time()\n",
    "                    loss = NER(batch_x)\n",
    "                    loss = loss*(-1)\n",
    "                    print(f'time per batch: {time() - t1}')\n",
    "                    print(loss)\n",
    "                    all_loss.append(loss)\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(NER.parameters(), 5, norm_type=2)\n",
    "                    optimizer.step()\n",
    "                total_loss = sum(all_loss)/(ind + 1)\n",
    "                my_scheduler.step(total_loss)\n",
    "                time_list3.append(time()-ttt)\n",
    "                print(f'total loss of epoch: {total_loss.item()}')\n",
    "                print('testing')\n",
    "                per_mat = np.zeros((len(tags), 3))\n",
    "                cnt_mat = np.zeros((len(tags), 3))\n",
    "                for ind, batch_test in enumerate(test_loader):\n",
    "                    NER = NER.eval()\n",
    "                    output = NER.predict(batch_test)\n",
    "                    a, b += eval_score(tags, output, batch_test[2])\n",
    "                    per_mat += a\n",
    "                    cnt_mat += b\n",
    "                per_mat = per_mat/(ind+1)\n",
    "                per_mat = per_mat[:len(tags),:]\n",
    "                cnt_mat = cnt_mat[:len(tags),:]\n",
    "                print(cnt_mat)\n",
    "                print(per_mat)\n",
    "                score = sum(per_mat[:,2])/(len(tags)-1)\n",
    "                if best_score < score:\n",
    "                    best_mat=per_mat\n",
    "                    best_score = score\n",
    "                    cnt_idle = 0\n",
    "                else:\n",
    "                    cnt_idle += 1\n",
    "                print(f'overall score: {score}')\n",
    "                print('--------------------')\n",
    "                if early_stop_n == cnt_idle:\n",
    "                    break\n",
    "                print(f'total epoch time: {ttt-time()}')\n",
    "            print(f'best_score: {best_score}\\n')\n",
    "            print(f'I => recall: {best_mat[0,0]}, precision: {best_mat[0,1]}, , f1: {best_mat[0,2]}\\n')\n",
    "            print(f'B => recall: {best_mat[1,0]}, precision: {best_mat[1,1]}, , f1: {best_mat[1,2]}\\n')\n",
    "            print(f'O => recall: {best_mat[2,0]}, precision: {best_mat[2,1]}, , f1: {best_mat[2,2]}\\n')\n",
    "            print(f'----------------------------------\\n')\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter ur logname: optuna_test\n"
     ]
    }
   ],
   "source": [
    "BS = 2\n",
    "tags = {0:'I', 1:'B', 2:'O', 3:'<pad>'}\n",
    "scheduler_n = 1003\n",
    "word_length = 84\n",
    "early_stop_n = 10005\n",
    "max_size_char = 6\n",
    "same_padding = True\n",
    "use_BN = True\n",
    "activation_func = True\n",
    "input_channel = 1\n",
    "nums_filter = [1]\n",
    "output_size = 64\n",
    "size_of_embedding = 300\n",
    "pos_size = len(POSMAP)\n",
    "num_char_encoding_size = 135\n",
    "FCN = False\n",
    "file_name = input('enter ur logname: ')\n",
    "num_epoch = 30\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = MyDataloader('../clean84withpos.txt', '../label84withpos.txt', RULEs, \\\n",
    "                           word_length, '|', 'char_vec_dictionary.txt',max_size_char, \\\n",
    "                           '../fasttext.th.vec', 300, device, '../pos_tag84withpos.txt',POSMAP)\n",
    "tr, te = get_indices_random_val_test_split(len(data), 1, 0.00015, True)\n",
    "\n",
    "def objective(trial):\n",
    "    kernel_sizes = [trial.suggest_categorical('kernel_sizes', [3,5])]\n",
    "    gru_fcn_dropout = trial.suggest_uniform('gru_fcn_dropout', 0, 0.7)\n",
    "    gru_dropout = trial.suggest_uniform('gru_dropout', 0, 0.7)\n",
    "    gru_out_dropout = trial.suggest_uniform('gru_out_dropout', 0, 0.7)\n",
    "    LR = trial.suggest_uniform('LR', 5, 10)*10**(-5)\n",
    "    grucrf_hidden_size = trial.suggest_categorical('grucrf_hidden_size', [64, 128])\n",
    "    w_decay = trial.suggest_categorical('w_decay', [-3,-4,-5])\n",
    "    \n",
    "    train_loader = DataLoader(data, batch_size=BS, sampler=tr)\n",
    "    test_loader = DataLoader(data, batch_size=BS, sampler=te)\n",
    "    \n",
    "    with open(file_name + '.txt', 'a', encoding='utf8') as f:\n",
    "        f.write(f'kernel_sizes: {kernel_sizes}, gru_fcn_dropout: {gru_fcn_dropout}\\n')\n",
    "        f.write(f'gru_dropout: {gru_dropout}, gru_out_dropout: {gru_out_dropout}\\n')\n",
    "        f.write(f'LR: {LR}, grucrf_hidden_size: {grucrf_hidden_size}\\n')\n",
    "        f.write(f'w_decay: {w_decay}\\n')\n",
    "\n",
    "    NER = CNN_GRU_char_pos(BS, max_size_char, nums_filter, use_BN, activation_func, input_channel, \\\n",
    "                           kernel_sizes, same_padding, num_char_encoding_size, output_size, word_length, \\\n",
    "                           grucrf_hidden_size, gru_dropout, True, tags, gru_fcn_dropout, pos_size, FCN, \\\n",
    "                           gru_out_dropout)\n",
    "    optimizer = optim.Adam(NER.parameters(), lr=LR, eps=1e-08, weight_decay=10**w_decay,amsgrad=True)\n",
    "    my_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', verbose=True)\n",
    "    \n",
    "    best_score = 0\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        ttt = time()\n",
    "        print(f'epoch {epoch}')\n",
    "        all_loss = []\n",
    "        for ind, batch_x in enumerate(train_loader):\n",
    "            if ind%5 == 0:\n",
    "                print(ind)\n",
    "            t2 = time()\n",
    "            NER = NER.train()\n",
    "            print(time() - t2)\n",
    "            NER.zero_grad()\n",
    "            t1 = time()\n",
    "            loss = NER(batch_x)\n",
    "            loss = loss*(-1)\n",
    "            print(f'time per batch: {time() - t1}')\n",
    "            print(loss)\n",
    "            all_loss.append(loss)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(NER.parameters(), 5, norm_type=2)\n",
    "            optimizer.step()\n",
    "        total_loss = sum(all_loss)/(ind + 1)\n",
    "        my_scheduler.step(total_loss)\n",
    "        print(f'total loss of epoch: {total_loss.item()}')\n",
    "        print('testing')\n",
    "        per_mat = np.zeros((len(tags), 3))\n",
    "        cnt_mat = np.zeros((len(tags), 3))\n",
    "        for ind, batch_test in enumerate(test_loader):\n",
    "            NER = NER.eval()\n",
    "            output = NER.predict(batch_test)\n",
    "            a, b = eval_score(tags, output, batch_test[2])\n",
    "            per_mat += a\n",
    "            cnt_mat += b\n",
    "        per_mat = per_mat/(ind+1)\n",
    "        per_mat = per_mat[:len(tags),:]\n",
    "        cnt_mat = cnt_mat[:len(tags),:]\n",
    "        print(cnt_mat)\n",
    "        print(per_mat)\n",
    "        score = sum(per_mat[:,2])/(len(tags)-1)\n",
    "\n",
    "        with open(file_name + '.txt', 'a', encoding='utf8') as f:\n",
    "            f.write(f'epoch: {epoch}, score: {score}\\n')\n",
    "        \n",
    "        if score - best_score >= 0.005:\n",
    "            best_mat=per_mat\n",
    "            best_score = score\n",
    "            cnt_idle = 0\n",
    "        else:\n",
    "            cnt_idle += 1\n",
    "        print(f'overall score: {score}')\n",
    "        print('--------------------')\n",
    "        if early_stop_n == cnt_idle:\n",
    "            break\n",
    "        print(f'total epoch time: {ttt-time()}')\n",
    "    with open(file_name + '.txt', 'a', encoding='utf8') as f:\n",
    "        f.write(f'cnt_mat\\n')\n",
    "        f.write(f'I => : {cnt_mat[0,0]}, : {cnt_mat[0,1]}, : {cnt_mat[0,2]}\\n')\n",
    "        f.write(f'B => : {cnt_mat[1,0]}, : {cnt_mat[1,1]}, : {cnt_mat[1,2]}\\n')\n",
    "        f.write(f'O => : {cnt_mat[2,0]}, : {cnt_mat[2,1]}, : {cnt_mat[2,2]}\\n')\n",
    "        f.write(f'best_score: {best_score}\\n')\n",
    "        f.write(f'I => recall: {best_mat[0,0]}, precision: {best_mat[0,1]}, , f1: {best_mat[0,2]}\\n')\n",
    "        f.write(f'B => recall: {best_mat[1,0]}, precision: {best_mat[1,1]}, , f1: {best_mat[1,2]}\\n')\n",
    "        f.write(f'O => recall: {best_mat[2,0]}, precision: {best_mat[2,1]}, , f1: {best_mat[2,2]}\\n')\n",
    "        f.write(f'----------------------------------\\n')\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "0\n",
      "0.00011491775512695312\n",
      "time per batch: 2.3418049812316895\n",
      "tensor(34.8441, grad_fn=<MulBackward>)\n",
      "0.00012922286987304688\n",
      "time per batch: 2.1915690898895264\n",
      "tensor(80.6080, grad_fn=<MulBackward>)\n",
      "0.0001728534698486328\n",
      "time per batch: 2.0867130756378174\n",
      "tensor(33.8390, grad_fn=<MulBackward>)\n",
      "0.00016689300537109375\n",
      "time per batch: 2.2237839698791504\n",
      "tensor(50.1646, grad_fn=<MulBackward>)\n",
      "0.00018095970153808594\n",
      "time per batch: 2.2291951179504395\n",
      "tensor(26.9115, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.000164031982421875\n",
      "time per batch: 2.351447820663452\n",
      "tensor(11.8826, grad_fn=<MulBackward>)\n",
      "0.00019788742065429688\n",
      "time per batch: 2.503523826599121\n",
      "tensor(17.7553, grad_fn=<MulBackward>)\n",
      "0.00016617774963378906\n",
      "time per batch: 2.5306408405303955\n",
      "tensor(24.1425, grad_fn=<MulBackward>)\n",
      "0.0001227855682373047\n",
      "time per batch: 4.957162857055664\n",
      "tensor(20.1968, grad_fn=<MulBackward>)\n",
      "0.000164031982421875\n",
      "time per batch: 2.9302730560302734\n",
      "tensor(18.5895, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00016498565673828125\n",
      "time per batch: 2.7285568714141846\n",
      "tensor(15.4828, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 30.401521682739258\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [ 20.   7.   1.]\n",
      " [222.  34.   7.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.29646465 0.17424242 0.18519814]\n",
      " [0.06759907 0.34545455 0.09640582]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.09386798386798385\n",
      "--------------------\n",
      "total epoch time: -111.95125222206116\n",
      "epoch 1\n",
      "0\n",
      "0.00020194053649902344\n",
      "time per batch: 2.2625882625579834\n",
      "tensor(46.0103, grad_fn=<MulBackward>)\n",
      "0.00026106834411621094\n",
      "time per batch: 2.1433489322662354\n",
      "tensor(25.0910, grad_fn=<MulBackward>)\n",
      "0.0001659393310546875\n",
      "time per batch: 2.178135871887207\n",
      "tensor(25.9550, grad_fn=<MulBackward>)\n",
      "0.00016117095947265625\n",
      "time per batch: 2.201129913330078\n",
      "tensor(14.0956, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 2.1401519775390625\n",
      "tensor(18.9372, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.000171661376953125\n",
      "time per batch: 2.3378710746765137\n",
      "tensor(35.7406, grad_fn=<MulBackward>)\n",
      "0.00013208389282226562\n",
      "time per batch: 2.0017247200012207\n",
      "tensor(29.7711, grad_fn=<MulBackward>)\n",
      "0.00016689300537109375\n",
      "time per batch: 2.184168815612793\n",
      "tensor(60.1810, grad_fn=<MulBackward>)\n",
      "0.00016689300537109375\n",
      "time per batch: 2.337913751602173\n",
      "tensor(11.6748, grad_fn=<MulBackward>)\n",
      "0.00019097328186035156\n",
      "time per batch: 2.1117868423461914\n",
      "tensor(26.8473, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00017213821411132812\n",
      "time per batch: 1.9966328144073486\n",
      "tensor(9.0990, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 27.582075119018555\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [ 20.   7.   1.]\n",
      " [222.  34.   7.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.27954545 0.17424242 0.19747475]\n",
      " [0.06591557 0.34545455 0.09360639]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.09702704702704702\n",
      "--------------------\n",
      "total epoch time: -112.9610550403595\n",
      "epoch 2\n",
      "0\n",
      "0.0002779960632324219\n",
      "time per batch: 2.343740940093994\n",
      "tensor(23.8479, grad_fn=<MulBackward>)\n",
      "0.00011420249938964844\n",
      "time per batch: 2.163196086883545\n",
      "tensor(50.5220, grad_fn=<MulBackward>)\n",
      "0.00011277198791503906\n",
      "time per batch: 1.8329505920410156\n",
      "tensor(62.4946, grad_fn=<MulBackward>)\n",
      "0.0001709461212158203\n",
      "time per batch: 1.8018162250518799\n",
      "tensor(29.3386, grad_fn=<MulBackward>)\n",
      "0.0001671314239501953\n",
      "time per batch: 1.822556972503662\n",
      "tensor(23.5536, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001690387725830078\n",
      "time per batch: 1.8279271125793457\n",
      "tensor(12.6930, grad_fn=<MulBackward>)\n",
      "0.00011301040649414062\n",
      "time per batch: 1.8441338539123535\n",
      "tensor(33.6461, grad_fn=<MulBackward>)\n",
      "0.0001621246337890625\n",
      "time per batch: 1.9025299549102783\n",
      "tensor(22.2344, grad_fn=<MulBackward>)\n",
      "0.0001678466796875\n",
      "time per batch: 1.8807032108306885\n",
      "tensor(13.9530, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 1.860199213027954\n",
      "tensor(13.5434, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00016164779663085938\n",
      "time per batch: 2.0921928882598877\n",
      "tensor(13.2676, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 27.190399169921875\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [ 12.   6.  10.]\n",
      " [190.  25.  48.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.33402204 0.22727273 0.22783883]\n",
      " [0.30139975 0.85519481 0.37992341]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.20258741258741256\n",
      "--------------------\n",
      "total epoch time: -103.94558000564575\n",
      "epoch 3\n",
      "0\n",
      "0.0001709461212158203\n",
      "time per batch: 2.5831098556518555\n",
      "tensor(16.5752, grad_fn=<MulBackward>)\n",
      "0.0001621246337890625\n",
      "time per batch: 2.958146095275879\n",
      "tensor(24.8443, grad_fn=<MulBackward>)\n",
      "0.00016999244689941406\n",
      "time per batch: 2.1773977279663086\n",
      "tensor(16.5811, grad_fn=<MulBackward>)\n",
      "0.00011301040649414062\n",
      "time per batch: 2.5468506813049316\n",
      "tensor(34.7361, grad_fn=<MulBackward>)\n",
      "0.0001888275146484375\n",
      "time per batch: 2.406836986541748\n",
      "tensor(54.9932, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00017118453979492188\n",
      "time per batch: 1.7938427925109863\n",
      "tensor(20.8996, grad_fn=<MulBackward>)\n",
      "0.0001709461212158203\n",
      "time per batch: 1.7889330387115479\n",
      "tensor(30.4279, grad_fn=<MulBackward>)\n",
      "0.00018525123596191406\n",
      "time per batch: 1.9602317810058594\n",
      "tensor(13.5376, grad_fn=<MulBackward>)\n",
      "0.00011301040649414062\n",
      "time per batch: 2.216715097427368\n",
      "tensor(21.8905, grad_fn=<MulBackward>)\n",
      "0.00011420249938964844\n",
      "time per batch: 2.0527172088623047\n",
      "tensor(23.5671, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00015807151794433594\n",
      "time per batch: 1.8578391075134277\n",
      "tensor(35.3546, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 26.67339324951172\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  22. 241.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.20606061 0.18181818 0.17424242]\n",
      " [0.86968395 0.91839953 0.88783452]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.35402564911567197\n",
      "--------------------\n",
      "total epoch time: -112.19757509231567\n",
      "epoch 4\n",
      "0\n",
      "0.00021505355834960938\n",
      "time per batch: 2.2920618057250977\n",
      "tensor(13.6452, grad_fn=<MulBackward>)\n",
      "0.00015616416931152344\n",
      "time per batch: 2.7984719276428223\n",
      "tensor(18.4324, grad_fn=<MulBackward>)\n",
      "0.00016808509826660156\n",
      "time per batch: 2.1192867755889893\n",
      "tensor(50.8677, grad_fn=<MulBackward>)\n",
      "0.00020813941955566406\n",
      "time per batch: 2.7438597679138184\n",
      "tensor(16.9936, grad_fn=<MulBackward>)\n",
      "0.0002760887145996094\n",
      "time per batch: 3.2684011459350586\n",
      "tensor(34.2499, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00016117095947265625\n",
      "time per batch: 2.065519094467163\n",
      "tensor(33.9361, grad_fn=<MulBackward>)\n",
      "0.00016999244689941406\n",
      "time per batch: 1.9978761672973633\n",
      "tensor(26.9073, grad_fn=<MulBackward>)\n",
      "0.00011420249938964844\n",
      "time per batch: 2.9082260131835938\n",
      "tensor(19.0221, grad_fn=<MulBackward>)\n",
      "0.00016927719116210938\n",
      "time per batch: 2.647486925125122\n",
      "tensor(12.1798, grad_fn=<MulBackward>)\n",
      "0.00018525123596191406\n",
      "time per batch: 2.180769205093384\n",
      "tensor(59.2363, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0001709461212158203\n",
      "time per batch: 1.8039178848266602\n",
      "tensor(4.8328, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 26.391202926635742\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  20. 243.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.19242424 0.21212121 0.20021645]\n",
      " [0.89768684 0.89556644 0.89513857]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3651183410336675\n",
      "--------------------\n",
      "total epoch time: -120.26070189476013\n",
      "epoch 5\n",
      "0\n",
      "0.00019598007202148438\n",
      "time per batch: 2.317373037338257\n",
      "tensor(19.1985, grad_fn=<MulBackward>)\n",
      "0.00011706352233886719\n",
      "time per batch: 2.548859119415283\n",
      "tensor(17.9738, grad_fn=<MulBackward>)\n",
      "0.00018715858459472656\n",
      "time per batch: 1.8446531295776367\n",
      "tensor(22.2090, grad_fn=<MulBackward>)\n",
      "0.00016999244689941406\n",
      "time per batch: 1.8168230056762695\n",
      "tensor(13.3547, grad_fn=<MulBackward>)\n",
      "0.00011420249938964844\n",
      "time per batch: 2.890026092529297\n",
      "tensor(26.4240, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00015878677368164062\n",
      "time per batch: 2.0476338863372803\n",
      "tensor(59.4476, grad_fn=<MulBackward>)\n",
      "0.00016379356384277344\n",
      "time per batch: 2.0149500370025635\n",
      "tensor(49.2576, grad_fn=<MulBackward>)\n",
      "0.00011610984802246094\n",
      "time per batch: 2.024674654006958\n",
      "tensor(19.2857, grad_fn=<MulBackward>)\n",
      "0.00011992454528808594\n",
      "time per batch: 1.9355080127716064\n",
      "tensor(21.9928, grad_fn=<MulBackward>)\n",
      "0.00016379356384277344\n",
      "time per batch: 2.372882843017578\n",
      "tensor(27.6827, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00016999244689941406\n",
      "time per batch: 2.1955089569091797\n",
      "tensor(12.7666, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 26.326627731323242\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  18. 245.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.18344156 0.24242424 0.19393939]\n",
      " [0.89671245 0.91437867 0.9028286 ]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3655893303134703\n",
      "--------------------\n",
      "total epoch time: -117.04032588005066\n",
      "epoch 6\n",
      "0\n",
      "0.00012302398681640625\n",
      "time per batch: 2.091367721557617\n",
      "tensor(17.8485, grad_fn=<MulBackward>)\n",
      "0.00011610984802246094\n",
      "time per batch: 2.870063066482544\n",
      "tensor(19.7974, grad_fn=<MulBackward>)\n",
      "0.00011873245239257812\n",
      "time per batch: 2.728040933609009\n",
      "tensor(20.3193, grad_fn=<MulBackward>)\n",
      "0.00012111663818359375\n",
      "time per batch: 2.1087701320648193\n",
      "tensor(12.2146, grad_fn=<MulBackward>)\n",
      "0.00014901161193847656\n",
      "time per batch: 2.345688819885254\n",
      "tensor(20.5372, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001671314239501953\n",
      "time per batch: 2.053234100341797\n",
      "tensor(23.5011, grad_fn=<MulBackward>)\n",
      "0.00018906593322753906\n",
      "time per batch: 2.489654064178467\n",
      "tensor(18.0393, grad_fn=<MulBackward>)\n",
      "0.00011110305786132812\n",
      "time per batch: 1.9253439903259277\n",
      "tensor(46.9522, grad_fn=<MulBackward>)\n",
      "0.00011610984802246094\n",
      "time per batch: 3.149541139602661\n",
      "tensor(61.3384, grad_fn=<MulBackward>)\n",
      "0.00012803077697753906\n",
      "time per batch: 2.8026299476623535\n",
      "tensor(26.0382, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0001537799835205078\n",
      "time per batch: 2.6750168800354004\n",
      "tensor(20.9073, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 26.135780334472656\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  18. 245.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.20833333 0.25       0.20454545]\n",
      " [0.86835505 0.86763595 0.86591644]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3568206315708688\n",
      "--------------------\n",
      "total epoch time: -125.21857118606567\n",
      "epoch 7\n",
      "0\n",
      "0.00016999244689941406\n",
      "time per batch: 1.9438049793243408\n",
      "tensor(25.1358, grad_fn=<MulBackward>)\n",
      "0.00017595291137695312\n",
      "time per batch: 1.8292453289031982\n",
      "tensor(21.6969, grad_fn=<MulBackward>)\n",
      "0.00012183189392089844\n",
      "time per batch: 1.9973959922790527\n",
      "tensor(42.1981, grad_fn=<MulBackward>)\n",
      "0.00011801719665527344\n",
      "time per batch: 1.8681368827819824\n",
      "tensor(16.6855, grad_fn=<MulBackward>)\n",
      "0.00016307830810546875\n",
      "time per batch: 1.8730852603912354\n",
      "tensor(23.1503, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0002460479736328125\n",
      "time per batch: 1.9665398597717285\n",
      "tensor(13.3008, grad_fn=<MulBackward>)\n",
      "0.0001380443572998047\n",
      "time per batch: 2.855175256729126\n",
      "tensor(26.5808, grad_fn=<MulBackward>)\n",
      "0.00011396408081054688\n",
      "time per batch: 2.2933881282806396\n",
      "tensor(68.3181, grad_fn=<MulBackward>)\n",
      "0.00017189979553222656\n",
      "time per batch: 2.222656011581421\n",
      "tensor(18.9979, grad_fn=<MulBackward>)\n",
      "0.00011396408081054688\n",
      "time per batch: 2.208818197250366\n",
      "tensor(21.0075, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00015616416931152344\n",
      "time per batch: 1.747661828994751\n",
      "tensor(8.8478, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.992687225341797\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  18. 245.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.2478355  0.25757576 0.23405483]\n",
      " [0.88721731 0.91334918 0.89609846]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3767177645093474\n",
      "--------------------\n",
      "total epoch time: -108.79737687110901\n",
      "epoch 8\n",
      "0\n",
      "0.0002422332763671875\n",
      "time per batch: 2.3495938777923584\n",
      "tensor(15.6150, grad_fn=<MulBackward>)\n",
      "0.00013113021850585938\n",
      "time per batch: 2.0452842712402344\n",
      "tensor(18.2544, grad_fn=<MulBackward>)\n",
      "0.00016736984252929688\n",
      "time per batch: 1.8679049015045166\n",
      "tensor(17.9581, grad_fn=<MulBackward>)\n",
      "0.00017690658569335938\n",
      "time per batch: 1.8742542266845703\n",
      "tensor(16.1162, grad_fn=<MulBackward>)\n",
      "0.00011396408081054688\n",
      "time per batch: 2.7699460983276367\n",
      "tensor(56.3766, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00016117095947265625\n",
      "time per batch: 1.8614740371704102\n",
      "tensor(17.6909, grad_fn=<MulBackward>)\n",
      "0.00011277198791503906\n",
      "time per batch: 2.2264420986175537\n",
      "tensor(29.8789, grad_fn=<MulBackward>)\n",
      "0.00011897087097167969\n",
      "time per batch: 1.9166259765625\n",
      "tensor(26.0808, grad_fn=<MulBackward>)\n",
      "0.00011897087097167969\n",
      "time per batch: 2.177495002746582\n",
      "tensor(41.3134, grad_fn=<MulBackward>)\n",
      "0.00016880035400390625\n",
      "time per batch: 2.1132640838623047\n",
      "tensor(34.0481, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00016689300537109375\n",
      "time per batch: 2.3330917358398438\n",
      "tensor(12.6511, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.998493194580078\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  18. 245.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.27121212 0.24242424 0.23333333]\n",
      " [0.82500799 0.85726109 0.82833985]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3538910600492921\n",
      "--------------------\n",
      "total epoch time: -116.54733490943909\n",
      "epoch 9\n",
      "0\n",
      "0.00015687942504882812\n",
      "time per batch: 2.007650136947632\n",
      "tensor(55.2036, grad_fn=<MulBackward>)\n",
      "0.00023293495178222656\n",
      "time per batch: 2.2219841480255127\n",
      "tensor(33.1758, grad_fn=<MulBackward>)\n",
      "0.00016307830810546875\n",
      "time per batch: 2.587847948074341\n",
      "tensor(20.4704, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 2.2919540405273438\n",
      "tensor(17.7307, grad_fn=<MulBackward>)\n",
      "0.00011873245239257812\n",
      "time per batch: 2.4054131507873535\n",
      "tensor(26.9647, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001709461212158203\n",
      "time per batch: 2.596266984939575\n",
      "tensor(14.3594, grad_fn=<MulBackward>)\n",
      "0.00011992454528808594\n",
      "time per batch: 2.2901389598846436\n",
      "tensor(32.5058, grad_fn=<MulBackward>)\n",
      "0.0001251697540283203\n",
      "time per batch: 2.2996859550476074\n",
      "tensor(20.0323, grad_fn=<MulBackward>)\n",
      "0.00011301040649414062\n",
      "time per batch: 2.3172221183776855\n",
      "tensor(11.2071, grad_fn=<MulBackward>)\n",
      "0.00026917457580566406\n",
      "time per batch: 2.356051206588745\n",
      "tensor(18.8602, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00010991096496582031\n",
      "time per batch: 2.433833122253418\n",
      "tensor(34.4997, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.90996551513672\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  18. 245.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.28636364 0.25757576 0.24458874]\n",
      " [0.86038363 0.87875714 0.8664449 ]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3703445479783815\n",
      "--------------------\n",
      "total epoch time: -142.3913140296936\n",
      "epoch 10\n",
      "0\n",
      "0.0002598762512207031\n",
      "time per batch: 1.871377944946289\n",
      "tensor(32.0448, grad_fn=<MulBackward>)\n",
      "0.0001227855682373047\n",
      "time per batch: 1.8431870937347412\n",
      "tensor(21.1295, grad_fn=<MulBackward>)\n",
      "0.0001220703125\n",
      "time per batch: 1.8453559875488281\n",
      "tensor(14.4795, grad_fn=<MulBackward>)\n",
      "0.00012111663818359375\n",
      "time per batch: 2.0543460845947266\n",
      "tensor(29.6818, grad_fn=<MulBackward>)\n",
      "0.00021886825561523438\n",
      "time per batch: 2.2312002182006836\n",
      "tensor(61.9354, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001800060272216797\n",
      "time per batch: 2.1228270530700684\n",
      "tensor(13.5257, grad_fn=<MulBackward>)\n",
      "0.00011372566223144531\n",
      "time per batch: 2.0411579608917236\n",
      "tensor(18.2465, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 1.962265968322754\n",
      "tensor(20.5276, grad_fn=<MulBackward>)\n",
      "0.00016307830810546875\n",
      "time per batch: 2.140740156173706\n",
      "tensor(25.2230, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 2.2282729148864746\n",
      "tensor(39.9510, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00015282630920410156\n",
      "time per batch: 1.9155871868133545\n",
      "tensor(7.7834, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.8662052154541\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  18. 245.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.23268398 0.27272727 0.22929293]\n",
      " [0.8314748  0.86253104 0.8322327 ]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.35384187626598856\n",
      "--------------------\n",
      "total epoch time: -112.26399111747742\n",
      "epoch 11\n",
      "0\n",
      "0.00020694732666015625\n",
      "time per batch: 2.841010093688965\n",
      "tensor(23.9131, grad_fn=<MulBackward>)\n",
      "0.00018286705017089844\n",
      "time per batch: 2.2410051822662354\n",
      "tensor(29.7381, grad_fn=<MulBackward>)\n",
      "0.0001571178436279297\n",
      "time per batch: 2.5738439559936523\n",
      "tensor(24.3436, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 1.9061710834503174\n",
      "tensor(13.2006, grad_fn=<MulBackward>)\n",
      "0.00016808509826660156\n",
      "time per batch: 2.1986610889434814\n",
      "tensor(59.8987, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001678466796875\n",
      "time per batch: 2.3275580406188965\n",
      "tensor(13.6305, grad_fn=<MulBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011992454528808594\n",
      "time per batch: 1.9130549430847168\n",
      "tensor(20.9848, grad_fn=<MulBackward>)\n",
      "0.00017595291137695312\n",
      "time per batch: 2.2734529972076416\n",
      "tensor(14.5507, grad_fn=<MulBackward>)\n",
      "0.0001709461212158203\n",
      "time per batch: 2.3001480102539062\n",
      "tensor(46.7139, grad_fn=<MulBackward>)\n",
      "0.00016999244689941406\n",
      "time per batch: 2.0149738788604736\n",
      "tensor(32.0082, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0001690387725830078\n",
      "time per batch: 1.7409262657165527\n",
      "tensor(4.3721, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.759477615356445\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  18. 245.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.23863636 0.24242424 0.22164502]\n",
      " [0.90598753 0.90408587 0.90344813]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.37503105010651\n",
      "--------------------\n",
      "total epoch time: -114.85900282859802\n",
      "epoch 12\n",
      "0\n",
      "0.0003199577331542969\n",
      "time per batch: 1.8766100406646729\n",
      "tensor(13.1292, grad_fn=<MulBackward>)\n",
      "0.00018095970153808594\n",
      "time per batch: 1.9215056896209717\n",
      "tensor(19.1912, grad_fn=<MulBackward>)\n",
      "0.0002238750457763672\n",
      "time per batch: 2.4046430587768555\n",
      "tensor(18.8208, grad_fn=<MulBackward>)\n",
      "0.00016379356384277344\n",
      "time per batch: 4.265607118606567\n",
      "tensor(15.4766, grad_fn=<MulBackward>)\n",
      "0.00012183189392089844\n",
      "time per batch: 1.873499870300293\n",
      "tensor(55.0106, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00017690658569335938\n",
      "time per batch: 2.358811855316162\n",
      "tensor(25.5627, grad_fn=<MulBackward>)\n",
      "0.00011992454528808594\n",
      "time per batch: 1.9140605926513672\n",
      "tensor(33.0826, grad_fn=<MulBackward>)\n",
      "0.00017380714416503906\n",
      "time per batch: 1.9620389938354492\n",
      "tensor(39.3478, grad_fn=<MulBackward>)\n",
      "0.0001709461212158203\n",
      "time per batch: 1.914504051208496\n",
      "tensor(25.0107, grad_fn=<MulBackward>)\n",
      "0.00016999244689941406\n",
      "time per batch: 862.014671087265\n",
      "tensor(28.5636, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0002231597900390625\n",
      "time per batch: 2.801568031311035\n",
      "tensor(9.7605, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.72331428527832\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  18. 245.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.16818182 0.22727273 0.18354978]\n",
      " [0.86693165 0.89446008 0.87490195]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.35281724323306474\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-06 22:39:52,879] Finished trial#106 resulted in value: 0.3767177645093474. Current best value is 0.3767177645093474 with parameters: {'LR': 5.826992948964408, 'gru_dropout': 0.6023800703615053, 'gru_fcn_dropout': 0.026658703460356214, 'gru_out_dropout': 0.25442982964747174, 'grucrf_hidden_size': 64, 'kernel_sizes': 5, 'w_decay': -3}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "0\n",
      "0.00019407272338867188\n",
      "time per batch: 6.435258865356445\n",
      "tensor(10.3699, grad_fn=<MulBackward>)\n",
      "0.000125885009765625\n",
      "time per batch: 3.1884660720825195\n",
      "tensor(20.6080, grad_fn=<MulBackward>)\n",
      "0.00011801719665527344\n",
      "time per batch: 4.82292103767395\n",
      "tensor(16.3300, grad_fn=<MulBackward>)\n",
      "0.00011968612670898438\n",
      "time per batch: 3.0032050609588623\n",
      "tensor(17.8857, grad_fn=<MulBackward>)\n",
      "0.00012183189392089844\n",
      "time per batch: 3.218485116958618\n",
      "tensor(64.2073, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001678466796875\n",
      "time per batch: 3.7899110317230225\n",
      "tensor(25.4161, grad_fn=<MulBackward>)\n",
      "0.00012421607971191406\n",
      "time per batch: 4.840949058532715\n",
      "tensor(59.6643, grad_fn=<MulBackward>)\n",
      "0.0001201629638671875\n",
      "time per batch: 4.619460105895996\n",
      "tensor(18.3695, grad_fn=<MulBackward>)\n",
      "0.00011897087097167969\n",
      "time per batch: 5.243571043014526\n",
      "tensor(19.8214, grad_fn=<MulBackward>)\n",
      "0.0004420280456542969\n",
      "time per batch: 6.41439414024353\n",
      "tensor(19.2011, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00015807151794433594\n",
      "time per batch: 5.9020280838012695\n",
      "tensor(14.1987, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 26.006547927856445\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [ 13.   2.  13.]\n",
      " [201.   7.  55.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.03636364 0.09090909 0.05194805]\n",
      " [0.44133847 0.78838384 0.44008721]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.16401175364023662\n",
      "--------------------\n",
      "total epoch time: -236.79990577697754\n",
      "epoch 1\n",
      "0\n",
      "0.0002090930938720703\n",
      "time per batch: 3.5337631702423096\n",
      "tensor(9.5474, grad_fn=<MulBackward>)\n",
      "0.00011873245239257812\n",
      "time per batch: 5.198749780654907\n",
      "tensor(7.5931, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 4.285388231277466\n",
      "tensor(14.4532, grad_fn=<MulBackward>)\n",
      "0.0001270771026611328\n",
      "time per batch: 4.257362127304077\n",
      "tensor(20.5022, grad_fn=<MulBackward>)\n",
      "0.00013303756713867188\n",
      "time per batch: 2.775218963623047\n",
      "tensor(36.4471, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001590251922607422\n",
      "time per batch: 2.752642869949341\n",
      "tensor(58.8259, grad_fn=<MulBackward>)\n",
      "0.00012087821960449219\n",
      "time per batch: 2.6002233028411865\n",
      "tensor(68.9915, grad_fn=<MulBackward>)\n",
      "0.0002319812774658203\n",
      "time per batch: 3.264130115509033\n",
      "tensor(8.6960, grad_fn=<MulBackward>)\n",
      "0.00033473968505859375\n",
      "time per batch: 4.264814853668213\n",
      "tensor(20.0463, grad_fn=<MulBackward>)\n",
      "0.0001163482666015625\n",
      "time per batch: 3.3093347549438477\n",
      "tensor(22.1029, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00016069412231445312\n",
      "time per batch: 3.75252103805542\n",
      "tensor(12.2825, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.40801239013672\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  28.]\n",
      " [  0.   0. 263.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.8711793  0.92563594]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3085453124923813\n",
      "--------------------\n",
      "total epoch time: -244.24800992012024\n",
      "epoch 2\n",
      "0\n",
      "0.00018405914306640625\n",
      "time per batch: 3.303091049194336\n",
      "tensor(20.9793, grad_fn=<MulBackward>)\n",
      "0.00016689300537109375\n",
      "time per batch: 3.8686130046844482\n",
      "tensor(23.3794, grad_fn=<MulBackward>)\n",
      "0.00011181831359863281\n",
      "time per batch: 5.055170774459839\n",
      "tensor(60.7486, grad_fn=<MulBackward>)\n",
      "0.00011610984802246094\n",
      "time per batch: 9.400724172592163\n",
      "tensor(26.1313, grad_fn=<MulBackward>)\n",
      "0.0002338886260986328\n",
      "time per batch: 10.50766897201538\n",
      "tensor(52.7077, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001800060272216797\n",
      "time per batch: 8.440577030181885\n",
      "tensor(13.4327, grad_fn=<MulBackward>)\n",
      "0.0002307891845703125\n",
      "time per batch: 6.7211010456085205\n",
      "tensor(11.2374, grad_fn=<MulBackward>)\n",
      "0.00022077560424804688\n",
      "time per batch: 7.695574998855591\n",
      "tensor(8.5237, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 8.24535083770752\n",
      "tensor(20.0794, grad_fn=<MulBackward>)\n",
      "0.0001308917999267578\n",
      "time per batch: 4.422145128250122\n",
      "tensor(19.0986, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0001742839813232422\n",
      "time per batch: 3.5683159828186035\n",
      "tensor(22.4603, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.343503952026367\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  28.]\n",
      " [  0.   0. 263.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.90848849 0.95097563]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.31699187810287116\n",
      "--------------------\n",
      "total epoch time: -310.37673711776733\n",
      "epoch 3\n",
      "0\n",
      "0.00013518333435058594\n",
      "time per batch: 3.466884136199951\n",
      "tensor(36.1058, grad_fn=<MulBackward>)\n",
      "0.0001270771026611328\n",
      "time per batch: 3.347856044769287\n",
      "tensor(52.9377, grad_fn=<MulBackward>)\n",
      "0.00013494491577148438\n",
      "time per batch: 4.092397689819336\n",
      "tensor(7.5897, grad_fn=<MulBackward>)\n",
      "0.00016617774963378906\n",
      "time per batch: 2.982361078262329\n",
      "tensor(12.7089, grad_fn=<MulBackward>)\n",
      "0.0001881122589111328\n",
      "time per batch: 3.6583080291748047\n",
      "tensor(9.5161, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00019669532775878906\n",
      "time per batch: 3.8105249404907227\n",
      "tensor(11.2097, grad_fn=<MulBackward>)\n",
      "0.00011515617370605469\n",
      "time per batch: 3.2234833240509033\n",
      "tensor(18.0929, grad_fn=<MulBackward>)\n",
      "0.0001220703125\n",
      "time per batch: 4.826373815536499\n",
      "tensor(17.2632, grad_fn=<MulBackward>)\n",
      "0.0001800060272216797\n",
      "time per batch: 3.3795106410980225\n",
      "tensor(24.4530, grad_fn=<MulBackward>)\n",
      "0.0001227855682373047\n",
      "time per batch: 4.50267219543457\n",
      "tensor(70.7504, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00014019012451171875\n",
      "time per batch: 3.172400951385498\n",
      "tensor(18.1411, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.34259033203125\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  28.]\n",
      " [  0.   0. 263.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.89839507 0.94525868]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.31508622637714734\n",
      "--------------------\n",
      "total epoch time: -189.12939286231995\n",
      "epoch 4\n",
      "0\n",
      "0.0001800060272216797\n",
      "time per batch: 2.7858898639678955\n",
      "tensor(52.8364, grad_fn=<MulBackward>)\n",
      "0.00012063980102539062\n",
      "time per batch: 3.3692119121551514\n",
      "tensor(16.4090, grad_fn=<MulBackward>)\n",
      "0.00023293495178222656\n",
      "time per batch: 3.060664176940918\n",
      "tensor(12.7584, grad_fn=<MulBackward>)\n",
      "0.00011515617370605469\n",
      "time per batch: 4.123517751693726\n",
      "tensor(25.9538, grad_fn=<MulBackward>)\n",
      "0.00011587142944335938\n",
      "time per batch: 7.875448226928711\n",
      "tensor(11.7722, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00020194053649902344\n",
      "time per batch: 3.366075038909912\n",
      "tensor(19.7848, grad_fn=<MulBackward>)\n",
      "0.00013494491577148438\n",
      "time per batch: 2.9253580570220947\n",
      "tensor(12.6879, grad_fn=<MulBackward>)\n",
      "0.00018906593322753906\n",
      "time per batch: 3.519602060317993\n",
      "tensor(66.1630, grad_fn=<MulBackward>)\n",
      "0.00012302398681640625\n",
      "time per batch: 3.3296778202056885\n",
      "tensor(25.0112, grad_fn=<MulBackward>)\n",
      "0.00012373924255371094\n",
      "time per batch: 2.768544912338257\n",
      "tensor(27.6029, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00020003318786621094\n",
      "time per batch: 3.2416460514068604\n",
      "tensor(6.7753, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.250457763671875\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  28.]\n",
      " [  0.   0. 263.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.88010868 0.93060609]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.310202030574935\n",
      "--------------------\n",
      "total epoch time: -299.1418831348419\n",
      "epoch 5\n",
      "0\n",
      "0.0001270771026611328\n",
      "time per batch: 2.6451621055603027\n",
      "tensor(9.4163, grad_fn=<MulBackward>)\n",
      "0.0001239776611328125\n",
      "time per batch: 3.951555013656616\n",
      "tensor(11.7511, grad_fn=<MulBackward>)\n",
      "0.00020575523376464844\n",
      "time per batch: 3.446021795272827\n",
      "tensor(68.3224, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 3.289094924926758\n",
      "tensor(17.4634, grad_fn=<MulBackward>)\n",
      "0.0001220703125\n",
      "time per batch: 2.9865219593048096\n",
      "tensor(26.5857, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0001609325408935547\n",
      "time per batch: 3.7924091815948486\n",
      "tensor(19.5906, grad_fn=<MulBackward>)\n",
      "0.0001327991485595703\n",
      "time per batch: 4.296236038208008\n",
      "tensor(15.4896, grad_fn=<MulBackward>)\n",
      "0.0001270771026611328\n",
      "time per batch: 3.486403226852417\n",
      "tensor(25.7249, grad_fn=<MulBackward>)\n",
      "0.0001239776611328125\n",
      "time per batch: 3.11382794380188\n",
      "tensor(47.3672, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 3.5341529846191406\n",
      "tensor(17.1972, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0001709461212158203\n",
      "time per batch: 3.2341830730438232\n",
      "tensor(18.1006, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.182636260986328\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  28.]\n",
      " [  0.   0. 263.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.91358844 0.95389516]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3179650544342368\n",
      "--------------------\n",
      "total epoch time: -204.14462900161743\n",
      "epoch 6\n",
      "0\n",
      "0.0002741813659667969\n",
      "time per batch: 3.483484983444214\n",
      "tensor(30.2040, grad_fn=<MulBackward>)\n",
      "0.00012111663818359375\n",
      "time per batch: 2.499074935913086\n",
      "tensor(70.2251, grad_fn=<MulBackward>)\n",
      "0.00011515617370605469\n",
      "time per batch: 2.2515032291412354\n",
      "tensor(19.6572, grad_fn=<MulBackward>)\n",
      "0.0001201629638671875\n",
      "time per batch: 3.2130091190338135\n",
      "tensor(10.9784, grad_fn=<MulBackward>)\n",
      "0.00011396408081054688\n",
      "time per batch: 3.340139865875244\n",
      "tensor(15.5994, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.0002269744873046875\n",
      "time per batch: 3.174293041229248\n",
      "tensor(6.9760, grad_fn=<MulBackward>)\n",
      "0.00011801719665527344\n",
      "time per batch: 3.396857976913452\n",
      "tensor(24.3186, grad_fn=<MulBackward>)\n",
      "0.00017189979553222656\n",
      "time per batch: 4.165647983551025\n",
      "tensor(35.7579, grad_fn=<MulBackward>)\n",
      "0.00013017654418945312\n",
      "time per batch: 4.004518985748291\n",
      "tensor(12.6235, grad_fn=<MulBackward>)\n",
      "0.00011897087097167969\n",
      "time per batch: 8.278184175491333\n",
      "tensor(43.2385, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0003008842468261719\n",
      "time per batch: 5.223528861999512\n",
      "tensor(7.4418, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.183677673339844\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  28.]\n",
      " [  0.   0. 263.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.83068409 0.89129254]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.29709751398421264\n",
      "--------------------\n",
      "total epoch time: -206.89271998405457\n",
      "epoch 7\n",
      "0\n",
      "0.00011801719665527344\n",
      "time per batch: 2.8479411602020264\n",
      "tensor(13.4015, grad_fn=<MulBackward>)\n",
      "0.00011587142944335938\n",
      "time per batch: 3.7143540382385254\n",
      "tensor(53.7739, grad_fn=<MulBackward>)\n",
      "0.00011491775512695312\n",
      "time per batch: 4.274854898452759\n",
      "tensor(69.9286, grad_fn=<MulBackward>)\n",
      "0.0001900196075439453\n",
      "time per batch: 3.270581007003784\n",
      "tensor(26.1539, grad_fn=<MulBackward>)\n",
      "0.00012111663818359375\n",
      "time per batch: 3.20361590385437\n",
      "tensor(28.1353, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00015687942504882812\n",
      "time per batch: 3.0993008613586426\n",
      "tensor(13.7747, grad_fn=<MulBackward>)\n",
      "0.00011682510375976562\n",
      "time per batch: 3.426536798477173\n",
      "tensor(11.9872, grad_fn=<MulBackward>)\n",
      "0.00011277198791503906\n",
      "time per batch: 3.4169509410858154\n",
      "tensor(21.8054, grad_fn=<MulBackward>)\n",
      "0.00011420249938964844\n",
      "time per batch: 4.733046054840088\n",
      "tensor(23.9380, grad_fn=<MulBackward>)\n",
      "0.00017976760864257812\n",
      "time per batch: 2.8229880332946777\n",
      "tensor(9.3437, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00010919570922851562\n",
      "time per batch: 3.5005850791931152\n",
      "tensor(4.1400, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 25.125640869140625\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   0.  28.]\n",
      " [  0.   0. 263.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [1.         0.89377811 0.94215592]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.31405197273205004\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2019-11-06 23:14:50,977] Finished trial#107 resulted in value: 0.31699187810287116. Current best value is 0.3767177645093474 with parameters: {'LR': 5.826992948964408, 'gru_dropout': 0.6023800703615053, 'gru_fcn_dropout': 0.026658703460356214, 'gru_out_dropout': 0.25442982964747174, 'grucrf_hidden_size': 64, 'kernel_sizes': 5, 'w_decay': -3}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "0\n",
      "0.00019097328186035156\n",
      "time per batch: 6.1610801219940186\n",
      "tensor(26.5541, grad_fn=<MulBackward>)\n",
      "0.0001900196075439453\n",
      "time per batch: 4.142956018447876\n",
      "tensor(17.5993, grad_fn=<MulBackward>)\n",
      "0.0001201629638671875\n",
      "time per batch: 3.2134928703308105\n",
      "tensor(24.0803, grad_fn=<MulBackward>)\n",
      "0.0001850128173828125\n",
      "time per batch: 2.6568210124969482\n",
      "tensor(106.2484, grad_fn=<MulBackward>)\n",
      "0.0001506805419921875\n",
      "time per batch: 2.6093101501464844\n",
      "tensor(24.5067, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00017404556274414062\n",
      "time per batch: 2.4154341220855713\n",
      "tensor(31.5961, grad_fn=<MulBackward>)\n",
      "0.00012111663818359375\n",
      "time per batch: 2.852065086364746\n",
      "tensor(15.0602, grad_fn=<MulBackward>)\n",
      "0.00011801719665527344\n",
      "time per batch: 2.4248390197753906\n",
      "tensor(20.3710, grad_fn=<MulBackward>)\n",
      "0.0004951953887939453\n",
      "time per batch: 2.7142980098724365\n",
      "tensor(16.4489, grad_fn=<MulBackward>)\n",
      "0.00018906593322753906\n",
      "time per batch: 2.513058662414551\n",
      "tensor(46.1006, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.00018405914306640625\n",
      "time per batch: 3.179694890975952\n",
      "tensor(4.6618, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 30.29339599609375\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.  22.   6.]\n",
      " [  0. 230.  33.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.55681818 0.08874507 0.14982423]\n",
      " [0.12604928 0.7711039  0.21540287]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.12174236643561598\n",
      "--------------------\n",
      "total epoch time: -135.6154112815857\n",
      "epoch 1\n",
      "0\n",
      "0.00011086463928222656\n",
      "time per batch: 2.2214510440826416\n",
      "tensor(16.7636, grad_fn=<MulBackward>)\n",
      "0.00017881393432617188\n",
      "time per batch: 2.3040459156036377\n",
      "tensor(21.1540, grad_fn=<MulBackward>)\n",
      "0.00011682510375976562\n",
      "time per batch: 2.453544855117798\n",
      "tensor(37.9710, grad_fn=<MulBackward>)\n",
      "0.00016880035400390625\n",
      "time per batch: 2.3691349029541016\n",
      "tensor(57.1030, grad_fn=<MulBackward>)\n",
      "0.00011992454528808594\n",
      "time per batch: 2.6181130409240723\n",
      "tensor(11.3462, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00028705596923828125\n",
      "time per batch: 2.605570077896118\n",
      "tensor(10.0041, grad_fn=<MulBackward>)\n",
      "0.00012731552124023438\n",
      "time per batch: 2.266045093536377\n",
      "tensor(25.2175, grad_fn=<MulBackward>)\n",
      "0.00011801719665527344\n",
      "time per batch: 2.708430767059326\n",
      "tensor(10.1664, grad_fn=<MulBackward>)\n",
      "0.00017690658569335938\n",
      "time per batch: 2.6585021018981934\n",
      "tensor(10.5984, grad_fn=<MulBackward>)\n",
      "0.0001289844512939453\n",
      "time per batch: 2.162233829498291\n",
      "tensor(19.5145, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0001850128173828125\n",
      "time per batch: 2.962510824203491\n",
      "tensor(5.3978, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 20.476045608520508\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.  12.  16.]\n",
      " [  0.  92. 171.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.32727273 0.1360894  0.17710721]\n",
      " [0.56991049 0.8350704  0.67199268]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.28303329430513796\n",
      "--------------------\n",
      "total epoch time: -132.02174305915833\n",
      "epoch 2\n",
      "0\n",
      "0.00017499923706054688\n",
      "time per batch: 3.377817153930664\n",
      "tensor(6.9072, grad_fn=<MulBackward>)\n",
      "0.00019502639770507812\n",
      "time per batch: 2.0912561416625977\n",
      "tensor(8.6745, grad_fn=<MulBackward>)\n",
      "0.00013518333435058594\n",
      "time per batch: 1.98759126663208\n",
      "tensor(8.5756, grad_fn=<MulBackward>)\n",
      "0.00012683868408203125\n",
      "time per batch: 2.076616048812866\n",
      "tensor(19.9089, grad_fn=<MulBackward>)\n",
      "0.00011801719665527344\n",
      "time per batch: 2.0478198528289795\n",
      "tensor(17.1812, grad_fn=<MulBackward>)\n",
      "5\n",
      "0.00016999244689941406\n",
      "time per batch: 2.7127883434295654\n",
      "tensor(21.5095, grad_fn=<MulBackward>)\n",
      "0.00012731552124023438\n",
      "time per batch: 2.1747560501098633\n",
      "tensor(17.1377, grad_fn=<MulBackward>)\n",
      "0.00018930435180664062\n",
      "time per batch: 2.1337201595306396\n",
      "tensor(14.5193, grad_fn=<MulBackward>)\n",
      "0.00021576881408691406\n",
      "time per batch: 2.4171462059020996\n",
      "tensor(73.5173, grad_fn=<MulBackward>)\n",
      "0.00014281272888183594\n",
      "time per batch: 1.8868680000305176\n",
      "tensor(19.1004, grad_fn=<MulBackward>)\n",
      "10\n",
      "0.0001900196075439453\n",
      "time per batch: 3.165539026260376\n",
      "tensor(9.0365, grad_fn=<MulBackward>)\n",
      "total loss of epoch: 19.642555236816406\n",
      "testing\n",
      "[[  0.   0.   0.]\n",
      " [  0.   6.  22.]\n",
      " [  0.  37. 226.]\n",
      " [  0.   0.   0.]]\n",
      "[[0.         0.         0.        ]\n",
      " [0.15707071 0.15584416 0.11944444]\n",
      " [0.82121099 0.83712159 0.81469387]\n",
      " [0.         0.         0.        ]]\n",
      "overall score: 0.3113794383972396\n",
      "--------------------\n",
      "total epoch time: -134.84521794319153\n",
      "epoch 3\n",
      "0\n",
      "0.00021696090698242188\n",
      "time per batch: 2.269212007522583\n",
      "tensor(10.3335, grad_fn=<MulBackward>)\n",
      "0.00012302398681640625\n",
      "time per batch: 2.860908031463623\n",
      "tensor(11.5392, grad_fn=<MulBackward>)\n",
      "0.00018024444580078125\n",
      "time per batch: 2.5476815700531006\n",
      "tensor(43.7410, grad_fn=<MulBackward>)\n",
      "0.00020313262939453125\n",
      "time per batch: 2.640000820159912\n",
      "tensor(11.6618, grad_fn=<MulBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-852ad59538f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sqlite:///test_optuna.db'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test optuna'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimize_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks)\u001b[0m\n\u001b[1;32m    414\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     def _optimize_parallel(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             message = 'Setting status of trial#{} as {}. {}'.format(trial_number,\n",
      "\u001b[0;32m<ipython-input-43-a0c1460a0df5>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mall_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.study.load_study(storage='sqlite:///test_optuna.db', study_name='test optuna')\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT name FROM * WHERE type='table';': near \"*\": syntax error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: near \"*\": syntax error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-40d7d8ea3ec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_optuna.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT name FROM * WHERE type='table';\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     )\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1608\u001b[0m                 \u001b[0;34m\"Execution failed on sql '{sql}': {exc}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m             )\n\u001b[0;32m-> 1610\u001b[0;31m             \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/pandas/compat/__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[0;34m(exc, traceback)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/allennlp2/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT name FROM * WHERE type='table';': near \"*\": syntax error"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "cnx = sqlite3.connect('test_optuna.db')\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT name FROM * WHERE type='table';\", cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.load_study(storage='sqlite:///test_optuna.db', study_name='test optuna')\n",
    "df = study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>state</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th colspan=\"7\" halign=\"left\">params</th>\n",
       "      <th colspan=\"2\" halign=\"left\">system_attrs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>gru_dropout</th>\n",
       "      <th>gru_fcn_dropout</th>\n",
       "      <th>gru_out_dropout</th>\n",
       "      <th>grucrf_hidden_size</th>\n",
       "      <th>kernel_sizes</th>\n",
       "      <th>w_decay</th>\n",
       "      <th>_number</th>\n",
       "      <th>fail_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>TrialState.RUNNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-06 21:40:01.802894</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8.998950</td>\n",
       "      <td>0.075830</td>\n",
       "      <td>0.044007</td>\n",
       "      <td>0.506712</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>104</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>TrialState.RUNNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-06 21:41:37.515459</td>\n",
       "      <td>NaT</td>\n",
       "      <td>7.578753</td>\n",
       "      <td>0.270617</td>\n",
       "      <td>0.432835</td>\n",
       "      <td>0.123188</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>TrialState.COMPLETE</td>\n",
       "      <td>0.376718</td>\n",
       "      <td>2019-11-06 21:42:43.610794</td>\n",
       "      <td>2019-11-06 22:39:52.725988</td>\n",
       "      <td>5.826993</td>\n",
       "      <td>0.602380</td>\n",
       "      <td>0.026659</td>\n",
       "      <td>0.254430</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>106</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>TrialState.COMPLETE</td>\n",
       "      <td>0.316992</td>\n",
       "      <td>2019-11-06 22:39:52.884477</td>\n",
       "      <td>2019-11-06 23:14:50.689517</td>\n",
       "      <td>6.558108</td>\n",
       "      <td>0.163159</td>\n",
       "      <td>0.485613</td>\n",
       "      <td>0.526041</td>\n",
       "      <td>128.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>107</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>TrialState.RUNNING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11-06 23:14:51.032208</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8.818262</td>\n",
       "      <td>0.182233</td>\n",
       "      <td>0.396082</td>\n",
       "      <td>0.471763</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>108</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number                state     value             datetime_start  \\\n",
       "                                                                       \n",
       "104    104   TrialState.RUNNING       NaN 2019-11-06 21:40:01.802894   \n",
       "105    105   TrialState.RUNNING       NaN 2019-11-06 21:41:37.515459   \n",
       "106    106  TrialState.COMPLETE  0.376718 2019-11-06 21:42:43.610794   \n",
       "107    107  TrialState.COMPLETE  0.316992 2019-11-06 22:39:52.884477   \n",
       "108    108   TrialState.RUNNING       NaN 2019-11-06 23:14:51.032208   \n",
       "\n",
       "             datetime_complete    params                              \\\n",
       "                                      LR gru_dropout gru_fcn_dropout   \n",
       "104                        NaT  8.998950    0.075830        0.044007   \n",
       "105                        NaT  7.578753    0.270617        0.432835   \n",
       "106 2019-11-06 22:39:52.725988  5.826993    0.602380        0.026659   \n",
       "107 2019-11-06 23:14:50.689517  6.558108    0.163159        0.485613   \n",
       "108                        NaT  8.818262    0.182233        0.396082   \n",
       "\n",
       "                                                            system_attrs  \\\n",
       "    gru_out_dropout grucrf_hidden_size kernel_sizes w_decay      _number   \n",
       "104        0.506712               64.0          3.0    -5.0          104   \n",
       "105        0.123188              128.0          3.0    -5.0          105   \n",
       "106        0.254430               64.0          5.0    -3.0          106   \n",
       "107        0.526041              128.0          5.0    -5.0          107   \n",
       "108        0.471763               64.0          3.0    -4.0          108   \n",
       "\n",
       "                 \n",
       "    fail_reason  \n",
       "104         NaN  \n",
       "105         NaN  \n",
       "106         NaN  \n",
       "107         NaN  \n",
       "108         NaN  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "allennlp2",
   "language": "python",
   "name": "allennlp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
