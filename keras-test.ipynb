{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-285938f6133c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, TextDir: '.txt extension of samples', LabelDir: '.txt extension of labels',rules:\\\n\u001b[1;32m      3\u001b[0m                  \u001b[0;34m'the rules to be replaced => see in RULE.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLen_word_vec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'size of word vector'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mdelimiter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'(str) delimiter used to separate data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_char_dictionary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;34m'(str) see in CharEmbedding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len_char\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'(int) see in CharEmbedding'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class MyDataloader(Dataset):\n",
    "    def __init__(self, TextDir: '.txt extension of samples', LabelDir: '.txt extension of labels',rules:\\\n",
    "                 'the rules to be replaced => see in RULE.py', Len_word_vec: 'size of word vector', \\\n",
    "                delimiter: '(str) delimiter used to separate data', dir_char_dictionary: \\\n",
    "                '(str) see in CharEmbedding', max_len_char: '(int) see in CharEmbedding', \\\n",
    "                fasttext_dictionary_dir: '(str) see in WordEmbedding',\\\n",
    "                Len_embedded_vector: '(int) see in WordEmbedding', device, POSDir: '(str) .txt extension of POS',\\\n",
    "                POSMapping: 'see in POSMap.py', BS: '(int) batch size') -> None:\n",
    "        super().__init__()\n",
    "        self.DF = pd.read_csv(TextDir, names=['text'])\n",
    "        self.Label_DF = pd.read_csv(LabelDir, names=['text'])\n",
    "        self.pos_DF = pd.read_csv(POSDir, names=['text'])\n",
    "        self.rules = rules\n",
    "        self.Len_word_vec = Len_word_vec\n",
    "        self.delimiter = delimiter\n",
    "        self.char_embedder = CharEmbedding(dir_char_dictionary, max_len_char)\n",
    "        self.word_embedder = WordEmbedding(fasttext_dictionary_dir, Len_embedded_vector)\n",
    "        self.device = device\n",
    "        self.pos_embedder = POSEmbedding(POSMapping)\n",
    "        self.BS = BS\n",
    "    def __len__(self):\n",
    "        return len(self.DF)//self.BS\n",
    "    def __getitem__(self, Index) -> '(sample: (torch.tensor), label: (torch.tensor))':\n",
    "        all_words = [word.strip() for word in self.DF['text'][Index].strip().split(self.delimiter)]\n",
    "        for i in range(len(all_words)):\n",
    "            for rule in self.rules:\n",
    "                all_words[i] = re.sub(*rule, all_words[i])\n",
    "        Label = [float(word.strip()) for word in self.Label_DF['text'][Index].strip().split(self.delimiter)]\n",
    "        mask = [1.0]*len(all_words)\n",
    "        POS = [pos.strip() for pos in self.pos_DF['text'][Index].strip().split(self.delimiter)]\n",
    "        tmp_length = len(all_words)\n",
    "        if len(all_words) < self.Len_word_vec:\n",
    "            Label = Label + [3.0]*(self.Len_word_vec - len(all_words))\n",
    "            mask = mask + [0.0]*(self.Len_word_vec - len(all_words))\n",
    "            POS = POS + ['<pad>']*(self.Len_word_vec - len(all_words))\n",
    "            all_words = all_words + ['<pad>']*(self.Len_word_vec - len(all_words))\n",
    "        char_embed = self.char_embedder.embed(all_words)\n",
    "        word_embed = self.word_embedder.embed(all_words)\n",
    "        pos_embed = self.pos_embedder.embed(POS)\n",
    "        # print(len(all_words))\n",
    "        # print(len(Label))\n",
    "        # print(len(mask))\n",
    "        # print('----------')\n",
    "        return (char_embed.to(self.device), word_embed.to(self.device), \\\n",
    "                torch.tensor(Label).to(self.device), torch.tensor(mask).to(self.device), \\\n",
    "                tmp_length, pos_embed.float().to(self.device))\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbedding():\n",
    "    def __init__(self,\\\n",
    "    dir_char_dictionary: '(str) .txt',\\\n",
    "    max_len_char: '(int) max size of char representation, for example: given max_len_char=3 and word= \"abcde\" => only \"abc\" is used'):\n",
    "    #Example: given embed_capital=True and 'a' is embedded as array([1.,0.,0.,0.,0]). 'A' is then embedded as array([1.,0.,0.,0.,1.])\n",
    "        self.dictionary = {}\n",
    "        self.max_len_char = max_len_char\n",
    "        with open(dir_char_dictionary, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                tmp_data = line.strip().split()\n",
    "                self.dictionary[tmp_data[0]] = np.array([float(Char) for Char in tmp_data[1:]])\n",
    "    def embed(self, list_of_words: '(list[str]) example: [\"ฉัน\",\"กิน\",\"ข้าว\"]'):\n",
    "        #Note: 1 outer list is for 1 word.\n",
    "        output = []\n",
    "        for word in list_of_words:\n",
    "            embedded_word = []\n",
    "            tmp_word = word\n",
    "            if len(word) > self.max_len_char:\n",
    "                tmp_word = tmp_word[:self.max_len_char]\n",
    "            for Char in tmp_word:\n",
    "                if Char in self.dictionary:\n",
    "                    tmp_vector = self.dictionary[Char]\n",
    "                else:\n",
    "                    tmp_vector = np.zeros(self.dictionary['a'].shape)\n",
    "                embedded_word.append(tmp_vector)\n",
    "            if len(embedded_word) < self.max_len_char:\n",
    "                for i in range(self.max_len_char - len(embedded_word)):\n",
    "                    embedded_word.append(np.zeros(self.dictionary['a'].shape))\n",
    "            output.append(torch.tensor(embedded_word))\n",
    "        return torch.stack(output)\n",
    "\n",
    "class WordEmbedding():\n",
    "    #use fasttext embedding ==> read from a file\n",
    "    def __init__(self, fasttext_dictionary_dir: '(str) .vec extension of words and embedded_vectors',\\\n",
    "     Len_embedded_vector: '(int) size of embedded each vector (300 for fasttext) **Count only numbers not words'\\\n",
    "     ) -> None:\n",
    "        #example of format in fasttext_dictionary_dir\n",
    "        #กิน 1.0 -2.666 -3 22.5 .... \\n\n",
    "        #นอน 1.5 -5.666 3 9.5 .... \\n\n",
    "        #...\n",
    "        #...\n",
    "        self.dictionary = {}\n",
    "        self.Len_embedded_vector = Len_embedded_vector\n",
    "        with open(fasttext_dictionary_dir, 'r', encoding = 'utf8') as f:\n",
    "            for line in f:\n",
    "                tmp_line = line.strip()\n",
    "                tmp_words = tmp_line.split()\n",
    "                if tmp_line != '' and len(tmp_words) == self.Len_embedded_vector + 1:\n",
    "                    self.dictionary[tmp_words[0]] = np.array([float(element) for element in tmp_words[1:]])\n",
    "                else:\n",
    "                    continue\n",
    "    def embed(self, list_of_words: '(List[str]) for example: [\"ฉัน\",\"กิน\",\"ข้าว\"]'):\n",
    "        tmp_list = []\n",
    "        for word in list_of_words:\n",
    "            if word in self.dictionary:\n",
    "                tmp_list.append(self.dictionary[word])\n",
    "            else:\n",
    "                #in case of OOV: Zero-vector is used.\n",
    "                tmp_list.append(np.zeros(self.Len_embedded_vector))\n",
    "        return np.array(tmp_list)\n",
    "\n",
    "class POSEmbedding():\n",
    "    def __init__(self, POSMapping: 'see in POSMap.py'):\n",
    "        self.dictionary = POSMapping\n",
    "        self.size = len(self.dictionary)\n",
    "    def embed(self, list_of_POSs:'(list[str]) example: [\"NOUN\",\"VERB\",\"NOUN\"]'):\n",
    "        tmp_list = []\n",
    "        for POS in list_of_POSs:\n",
    "            POS = POS.strip()\n",
    "            if POS == '<pad>':\n",
    "                tmp_list.append(np.zeros(self.size))\n",
    "            else:\n",
    "                tmp_data = np.zeros(self.size)\n",
    "                tmp_data[self.dictionary[POS]] = 1\n",
    "                tmp_list.append(tmp_data)\n",
    "        return np.array(tmp_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
